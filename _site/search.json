[
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Project Portfolio",
    "section": "",
    "text": "Data Science Guides [R|Python] - Guides for common data science tasks, in R & Python.\n{scwplot} - An R package for creating consistent SCW-style ggplot figures.\nCAIP App - The PCN Capacity and Access Improvement Payment (CAIP) app.\nExpected Kills - Predicting kills in volleyball using XGBoost.\nPlayer Similarities - Using clustering methods to measure player similarity within positions.\n{nice} - The stupidest R package ever created."
  },
  {
    "objectID": "projects/index.html#data-science",
    "href": "projects/index.html#data-science",
    "title": "Project Portfolio",
    "section": "",
    "text": "Data Science Guides [R|Python] - Guides for common data science tasks, in R & Python.\n{scwplot} - An R package for creating consistent SCW-style ggplot figures.\nCAIP App - The PCN Capacity and Access Improvement Payment (CAIP) app.\nExpected Kills - Predicting kills in volleyball using XGBoost.\nPlayer Similarities - Using clustering methods to measure player similarity within positions.\n{nice} - The stupidest R package ever created."
  },
  {
    "objectID": "projects/index.html#research-publications",
    "href": "projects/index.html#research-publications",
    "title": "Project Portfolio",
    "section": "Research Publications ",
    "text": "Research Publications \n\nJohnson, P., & Cantu, F. (2020) “The Nationalization of Mexican Parties”. Política y Gobierno 27 (2)"
  },
  {
    "objectID": "projects/index.html#working-papers",
    "href": "projects/index.html#working-papers",
    "title": "Project Portfolio",
    "section": "Working Papers ",
    "text": "Working Papers \n\nHypothesis Testing for Reliability Statistics: A Permutation Approach (with Justin Kirkland)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Paul Johnson",
    "section": "",
    "text": "I am a data scientist at NHS South, Central and West CSU. I am interested in applying my knowledge of causal inference, statistics, probabilistic thinking, and machine learning to problems of understanding human behaviour.\nAt heart, I want to understand how the world works, and I enjoy telling stories that help others do the same, leveraging data and my quantitative skill set in the process."
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact ",
    "section": "",
    "text": "Contact \nI am always open to hearing from anyone who wants to chat about data science, social science, or statistics. Any time spent talking to people on the internet about these topics is time not spent ruining evenings with friends and family going on about p-values or gradient boosting and refusing to talk about normal things like whether you’d rather have feet-shaped hands or hand-shaped feet (I don’t know what normal people talk about).\nIf you have any feedback or questions or want to say hello and chat about nerdy stuff, please use this contact form.\nHowever, for enquiries related to consultancy, collaborations, or speaking engagements, please head over to the consultancy page.\n\n\n\n\n\n\n\n\nFull Name \nEmail Address \nMessage\n\n\nSend message"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog Posts",
    "section": "",
    "text": "Developing Offensive Ratings Using Desirability Functions\n\n\nRanking football players using {desirability2} and multivariate optimisation\n\n\n\nDesirability Functions\n\n\nOptimisation\n\n\nFootball Analytics\n\n\nData Science\n\n\nR\n\n\n\n\n\n\nFeb 22, 2024\n\n\nPaul Johnson\n\n\n\n\n\n\n\n\n\n\n\n\nA Cheatsheet for Building Multilevel Models in R\n\n\nA short introduction to basic multilevel modelling syntax in R (using lme4, brms or rstanarm).\n\n\n\nStatistics\n\n\nData Science\n\n\nMultilevel Models\n\n\nR\n\n\n\n\n\n\nNov 1, 2022\n\n\nPaul Johnson\n\n\n\n\n\n\n\n\n\n\n\n\nRendering Mermaid Diagrams on a Hugo Website Using Quarto\n\n\n\n\n\n\nQuarto\n\n\nMarkdown\n\n\nHugo\n\n\n\n\n\n\nSep 17, 2022\n\n\nPaul Johnson\n\n\n\n\n\n\n\n\n\n\n\n\nSetting up a Hugo Website with Quarto\n\n\nAn extension of the Quarto documentation (that didn’t work off the shelf for me) for getting a Hugo website working with Quarto seamlessly.\n\n\n\nQuarto\n\n\nMarkdown\n\n\nHugo\n\n\nR\n\n\n\n\n\n\nSep 11, 2022\n\n\nPaul Johnson\n\n\n\n\n\n\n\n\n\n\n\n\nResources for Learning Bayesian Statistics\n\n\nLearning Bayesian statistics is hard enough already, so maybe I can help make that journey very slightly easier for someone else by pointing them to some of the best resources I’ve come across.\n\n\n\nBayesian Statistics\n\n\nStatistics\n\n\nData Science\n\n\nProbability\n\n\n\n\n\n\nMay 10, 2022\n\n\nPaul Johnson\n\n\n\n\n\n\n\n\n\n\n\n\nCombining Tidymodels & Caret for Machine Learning in R\n\n\nHow to combine the functionality from both the Tidymodels framework and Caret to build models that combine the best of both worlds.\n\n\n\nMachine Learning\n\n\nData Science\n\n\nR\n\n\nCaret\n\n\nTidymodels\n\n\n\n\n\n\nMay 7, 2022\n\n\nPaul Johnson\n\n\n\n\n\n\n\n\n\n\n\n\nMake More Packages!\n\n\nHow (and why) to learn basic package development in R, for data scientists.\n\n\n\nR\n\n\nPackages\n\n\nSoftware Development\n\n\nData Science\n\n\n\n\n\n\nFeb 19, 2022\n\n\nPaul Johnson\n\n\n\n\n\n\n\n\n\n\n\n\nMaking Myself Accountable\n\n\nSetting myself a goal to try and write one blog post a month for the whole of 2022, so that I can come back to it and assess just how much I missed my target by at the end of the year.\n\n\n\nMe Me Me\n\n\nUpdates\n\n\n\n\n\n\nJan 17, 2022\n\n\nPaul Johnson\n\n\n\n\n\n\n\n\n\n\n\n\nRecent Football Analytics Articles\n\n\nA short summary of some of the football analytics pieces I have written recently (as of 04/20)\n\n\n\nFootball Analytics\n\n\nR\n\n\nPython\n\n\n\n\n\n\nApr 14, 2020\n\n\nPaul Johnson\n\n\n\n\n\n\nNo matching items\n\n\n  \n\nReuseCC BY-SA 4.0"
  },
  {
    "objectID": "blog/2024-02-22-football-ratings-using-desirability-functions/index.html",
    "href": "blog/2024-02-22-football-ratings-using-desirability-functions/index.html",
    "title": "Developing Offensive Ratings Using Desirability Functions",
    "section": "",
    "text": "Setup Code (Click to Expand)\n# import packages\nsuppressPackageStartupMessages({\n  library(dplyr)\n  library(gt)\n  library(showtext)\n  library(ggplot2)\n  library(worldfootballR)\n  library(desirability2)\n})\n\n# setup fonts\nsysfonts::font_add_google(\"Poppins\")\nsysfonts::font_add_google(\"Lora\")\nshowtext::showtext_auto()\n\n# set plot theme\n# inspired by https://github.com/z3tt/TidyTuesday/blob/main/R/2020_31_PalmerPenguins.Rmd\ntheme_set(theme_minimal(base_size = 20, base_family = \"Poppins\")) +\n  theme_update(\n    panel.grid.major = element_line(color = \"grey90\", linewidth = .4),\n    panel.grid.minor = element_blank(),\n    axis.title.x = element_text(\n      color = \"grey30\", margin = margin(t = 5), size = rel(1.05)\n      ),\n    axis.title.y = element_text(\n      color = \"grey30\", margin = margin(r = 5), size = rel(1.05)\n      ),\n    axis.text = element_text(color = \"grey50\", size = rel(1)),\n    axis.ticks = element_line(color = \"grey90\", linewidth = .4),\n    axis.ticks.length = unit(.2, \"lines\"),\n    legend.position = \"top\",\n    legend.title = element_blank(),\n    legend.text = element_text(size = rel(1)),\n    plot.title = element_text(\n      hjust = 0, color = \"black\", family = \"Lora\",\n      size = rel(1.5), margin = margin(t = 5, b = 5)\n    ),\n    plot.subtitle = element_text(\n      hjust = 0, color = \"grey30\", family = \"Lora\",\n      lineheight = 0.5, size = rel(1.1), \n      margin = margin(5, 0, 5, 0)\n    ),\n    plot.title.position = \"plot\",\n    plot.caption = element_text(\n      color = \"grey50\", size = rel(0.8), hjust = 1,\n      margin = margin(10, 0, 0, 0)\n    ),\n    plot.caption.position = \"plot\",\n    plot.margin = margin(rep(10, 4))\n  )\n\n# function for creating tables\nformat_table &lt;-\n  function(data, cols) {\n    data |&gt;\n      gt() |&gt;\n      fmt_number(columns = cols, decimals = 2) |&gt;\n      tab_spanner(\n        label = \"Desirability Scores\",\n        columns = cols\n      ) |&gt;\n      cols_align(\n        align = \"center\",\n        columns = cols\n      ) |&gt;\n      tab_source_note(source_note = \"Source: FB Ref Via {worldfootballR}\") |&gt;\n      tab_options(\n        table.width = pct(100),\n        table.font.names = \"Poppins\"\n      )\n  }\nBuilding rating systems for complex, multidimensional contexts is difficult, and the results are often underwhelming. There are many examples of this in sports, where rating systems are common. While many sports have metrics that accurately describe team and player performance in specific contexts, ratings that summarise performance are more limited. Football has expected goals (xG). xG is a relatively accurate proxy for team performance because it describes every team’s primary objective - score more goals than the opponent. However, players help their team achieve this objective in many ways that xG does not capture.\nAn accurate player rating system would help address this. Performance has multiple components, so rating players is a balancing act. While goals may be the most important part of the game, a player who scored two goals has not necessarily played better than their teammate who contributed a goal and an assist, because player performance is a combination of factors. Player ratings are a multivariate optimisation problem.\nIn this blog post, I will use a simple but often overlooked method for multivariate optimisation - desirability functions - to develop a rating system for offensive players in football, using data from the big five leagues in the 2022/23 season. The ratings will attempt to identify the best players in Europe in the 2022/23 season because match-level ratings seem less fun (and more difficult).\nAs someone prone to seeing rabbit holes as a challenge, I have spent far too much time debating what factors should go into player ratings and how to maximise the performance of a rating system, but the focus of this post is on the demonstration of desirability functions. The result is a rating system with plenty of room for improvement, but hopefully, in the process, this post will serve as a brief introduction to desirability functions."
  },
  {
    "objectID": "blog/2024-02-22-football-ratings-using-desirability-functions/index.html#how-desirability-works",
    "href": "blog/2024-02-22-football-ratings-using-desirability-functions/index.html#how-desirability-works",
    "title": "Developing Offensive Ratings Using Desirability Functions",
    "section": "How Desirability Works",
    "text": "How Desirability Works\nDesirability functions optimise across multiple variables by mapping each variable onto a standard scale from one (maximally desirable) to zero (unacceptable) using a transformation function1, before calculating the weighted combination of these individual scores using the arithmetic or geometric mean2 and optimising this overall desirability score (Kuhn 2016, 2023; Kuhn and Johnson 2019).\nMapping each variable to a standard scale makes a direct comparison between variables easier. The transformation function depends on what desirability looks like for each variable. If higher values are more desirable, maximisation would be appropriate, or minimisation when lower values are more desirable. Other transformations are possible, such as a particular target value or a box function that specifies a desirable interval, and there are also mechanisms for making desirability easier or harder to achieve, using scaling features. Having transformed each variable onto a scale where values approaching one are more desirable, the observation that achieves the highest average value of all individual desirability scores is the observation that is the closest to being maximally desirable."
  },
  {
    "objectID": "blog/2024-02-22-football-ratings-using-desirability-functions/index.html#the-desirability2-package",
    "href": "blog/2024-02-22-football-ratings-using-desirability-functions/index.html#the-desirability2-package",
    "title": "Developing Offensive Ratings Using Desirability Functions",
    "section": "The {desirability2} Package",
    "text": "The {desirability2} Package\nThere are two main groups of functions in the {desirability2} package - the transformation functions and the overall desirability function.\nWhile the most commonly used transformation functions will be d_max() and d_min(), there are several other transformations (d_target(), d_box(), and d_category()). Where none of these functions are sufficient, d_custom() offers functionality for specifying a custom transformation. Finally, d_overall() calculates the overall desirability score.\nAll {desirability2} functions can be a part of a typical tidyverse pipeline."
  },
  {
    "objectID": "blog/2024-02-22-football-ratings-using-desirability-functions/index.html#goal-threat",
    "href": "blog/2024-02-22-football-ratings-using-desirability-functions/index.html#goal-threat",
    "title": "Developing Offensive Ratings Using Desirability Functions",
    "section": "Goal Threat",
    "text": "Goal Threat\nPerhaps it is stating the obvious to even the most “I’m only here for the stats” folks reading this blog post, but scoring goals is a very important part of football. Goals are good. Teams love them. Everyone loves players that do them. So it makes sense to start the rating system with the goal threat component.\nThe four variables that make up the goal threat desirability function are npG, npxG, shots, and touches in the opponent’s penalty area. All of these variables have relatively clear predictive value when trying to predict goals, though you could also include other variables like average shot distance if you wanted to be more precise. Each variable also has a positive association with performance, so the desirability functions are all seeking to maximise their values.\nThe two most predictive variables are, perhaps unsurprisingly, npG and npxG, while shots and touches in the opponent’s penalty area are more supplementary. As a result, goals and expected goals have been rescaled to give them greater weight in the overall desirability score, while shots and touches have been rescaled to downweight their value (discussed in greater detail in Section 2.1.1). The code for computing the goal threat desirability function is below (d_max() is the function doing all the work) and Table 1 shows the top ten players in terms of goal threat.\n\ngoal_threat &lt;- \n  big_five_stats |&gt;\n  mutate(\n    # maximise npg90 & npxg90 with higher scale\n    across(\n      .cols = c(npg90, npxg90),\n      ~ d_max(.x, use_data = TRUE, scale = 2),\n      .names = \"d_{.col}\"\n      ),\n    # maximise shots90 & pen_touches90 with lower scale\n    across(\n      .cols = c(sh90, pen_touches90),\n      ~ d_max(.x, use_data = TRUE, scale = 0.5),\n      .names = \"d_{.col}\"\n    ),\n    # overall desirability score for goal scoring\n    # tolerance set to 0.1 so that lowest values are not 0\n    d_goals = d_overall(across(starts_with(\"d_\")), tolerance = 0.1)\n  ) |&gt; \n  select(player, position, team, league, starts_with(\"d_\")) |&gt; \n  arrange(desc(d_goals)) |&gt; \n  rename_at(vars(player:league), snakecase::to_title_case) |&gt; \n  rename(\n    \"Non-Penalty Goals\" = d_npg90,\n    \"Non-Penalty xG\" = d_npxg90,\n    \"Shots\" = d_sh90,\n    \"Penalty Area Touches\" = d_pen_touches90,\n    \"Overall\" = d_goals\n  )\n\n\n\nTable Code (Click to Expand)\ngoal_threat |&gt; \n  head(10) |&gt;\n  format_table(cols = 5:9)\n\n\n\n\nTable 1: Goal Threat Ratings\n\n\n\n\n\n\n\n\n\nPlayer\nPosition\nTeam\nLeague\nDesirability Scores\n\n\nNon-Penalty Goals\nNon-Penalty xG\nShots\nPenalty Area Touches\nOverall\n\n\n\n\nErling Haaland\nFW\nManchester City\nPremier League\n1.00\n1.00\n0.90\n0.85\n0.93\n\n\nKylian Mbappé\nFW\nParis S-G\nLigue 1\n0.78\n0.90\n1.00\n1.00\n0.91\n\n\nVictor Osimhen\nFW\nNapoli\nSerie A\n0.80\n0.77\n1.00\n0.88\n0.86\n\n\nRobert Lewandowski\nFW\nBarcelona\nLa Liga\n0.60\n0.97\n0.96\n0.83\n0.83\n\n\nCallum Wilson\nFW\nNewcastle Utd\nPremier League\n0.59\n0.92\n0.85\n0.64\n0.74\n\n\nLoïs Openda\nFW\nLens\nLigue 1\n0.59\n0.71\n0.89\n0.74\n0.72\n\n\nLautaro Martínez\nFW\nInter\nSerie A\n0.55\n0.60\n0.97\n0.75\n0.70\n\n\nKarim Benzema\nFW\nReal Madrid\nLa Liga\n0.32\n0.77\n0.97\n0.76\n0.65\n\n\nAlexandre Lacazette\nFW\nLyon\nLigue 1\n0.48\n0.54\n0.84\n0.83\n0.65\n\n\nDarwin Núñez\nFW\nLiverpool\nPremier League\n0.26\n0.73\n0.98\n0.71\n0.60\n\n\n\nSource: FB Ref Via {worldfootballR}\n\n\n\n\n\n\n\n\n\n\n\nErling Haaland, Kylian Mbappé, and Victor Osimhen leading the way in Table 1 definitely inspires some confidence in this approach! Bar a few exceptions, like Alexandre Lacazette being one of the ten greatest goal threats in Europe at his big old age, the top ten feels pretty reasonable. The player that stands out the most, in part due to how high he is on the list, is Callum Wilson. However, looking back at his numbers last season, it’s more that he had a freak season than that the goal threat desirability score is doing something wonky. I don’t believe Callum Wilson poses the fifth greatest goal threat in the big five leagues, but it’s hard to argue with him being ranked fifth for his production last season.\nI was also a little surprised not to see Harry Kane making the top ten, and it turns out he is in 15th. This appears to be because his xG was lower than the players ahead of him, despite him scoring plenty of goals last season. I think Kane’s lower rating raises questions about the decision to weight goals and expected goals equally. A more detailed rating system would need to make a decision about whether the goal is to describe a player’s past performance or identify the best players, which is inherently about predicting future performance. If it is the former, goals should probably be weighted higher, but if it is the latter, then expected goals should probably be favoured.\n\nScaling Desirability Functions\nEach individual desirability function can be easily adjusted using the scaling feature, \\(s\\), which can be increased in order to make maximal desirability harder to reach, and decreased to make it easier. The default value is 1 because \\(s\\) is an exponent and a 1 effectively cancels this out. We’re having fun.\nIn the goal threat desirability function, goals and expected goals have \\(s = 2\\), while for shots and touches in the opponent’s penalty area \\(s = 0.5\\). This captures the fact that npG and npxG are very direct measures of goal threat (they are literally the actual goals and the probability of scoring actual goals!), while shots and touches are a lot less direct. I suspect we’d be better served giving shots greater weight in the function than touches, but in the interest of trying my hardest not to get bogged down in the domain context and focus on the method, let’s just give them the same weight.\nGiven that \\(s\\) is the exponent, the scaling feature transforms the desirability function in a non-linear manner. The plot below illustrates how scaling affects desirability.\n\n\nPlot Code (Click to Expand)\nbig_five_stats |&gt; \n  mutate(\n    # maximise shots90 without rescaling\n    no_scale = d_max(sh90, use_data = TRUE),\n    # maximise shots90 with lower scale\n    easier   = d_max(sh90, use_data = TRUE, scale = 0.2),\n    # maximise shots90 with higher scale\n    harder = d_max(sh90, use_data = TRUE, scale = 5)\n  ) |&gt; \n  tidyr::pivot_longer(\n    cols = c(no_scale, easier, harder), \n    names_to = \"scaling\", values_to = \"value\"\n    ) |&gt; \n  mutate(scaling = factor(scaling, levels = c(\"no_scale\", \"easier\", \"harder\"))) |&gt; \n  ggplot(aes(x = sh90, y = value, colour = scaling)) +\n  geom_point(size = 1, alpha = .5) +\n  geom_line(linewidth = 0.8, alpha = .5) +\n  scale_colour_manual(\n    values = c(\"grey20\", \"#026E99\", \"#D93649\"),\n    labels = snakecase::to_title_case\n    ) +\n  labs(\n    title = \n        \"Scaling Effects on the Desirability Score of Shots/90\",\n    subtitle = \n      stringr::str_wrap(\n        glue::glue(\n          \"The unscaled desirability function is a linear transformation of \",\n          \"shots per 90, while the rescaled functions are non-linear, with \",\n          \"scaling values &gt; 1 making higher desirability more difficult to \",\n          \"satisfy and values &lt; 1 making it easier.\"\n          ),\n        width = 95\n      ),\n    x = \"Shots/90\", y = \"Desirability\",\n    caption = \n      \"Visualisation: Paul Johnson  |  Data: FB Ref Via {worldfootballR}\"\n    )\n\n\n\n\n\n\n\n\n\nWhen scaling desirability functions I think it is important to recognise the effects that imposing a non-linear transformation has. Because I’ve used the use_data = TRUE argument in d_max(), the maximum and minimum value in each desirability function is set by the data, meaning that regardless of the scaling applied the maximum value will still be 1 and the minimum value will be 0. However, as the scale increases, this will make it much harder for values to reach higher desirability scores, meaning that lower values of the corresponding variable will be given lower scores, but the non-linearity imposed increases the rate of desirability increases at higher values of the corresponding variable.\nIt is important to realise that making maximal desirability harder in this non-linear fashion does not downweight the highest values of that variable. Instead, it actually increases the magnitude of those highest values, because so few other observations are similarly high! Therefore, if you want to downweight the value of a particular variable in the overall desirability score, it needs to be easier to reach maximal desirability, because all values of that variable will have a higher score and that will give the highest values less leverage.\nI think it is a little difficult to process the exponential effects of \\(s\\) in this context. We are primarily interested in the highest values of each desirability function, given that the focus is on building a rating system, and increasing \\(s\\) achieves our goals at the top-end of the ratings, but it obviously feels a little wonky to exponentially increase the value of a particular desirability function in order to downweight it in the overall desirability score! I think it is sensible to visualise the effects that your intended scaling will have, so as to help you think through the choices you’re making."
  },
  {
    "objectID": "blog/2024-02-22-football-ratings-using-desirability-functions/index.html#chance-creation",
    "href": "blog/2024-02-22-football-ratings-using-desirability-functions/index.html#chance-creation",
    "title": "Developing Offensive Ratings Using Desirability Functions",
    "section": "Chance Creation",
    "text": "Chance Creation\nIf doing goals is the most important part of an offense in football, then helping others do goals must be the next best thing. Which brings us to chance creation. The process for creating a rating system for chance creation is more or less the same as with goal scoring. I’ve selected three variables that will go into the chance creation ratings - assists, expected assists (xA), and shot-creating actions (SCA).\nIn this case, the SCA desirability function has been scaled to reduce SCA’s effect on the overall score but assists and expected assists are unscaled. While I think that shot-creating actions are not quite as valuable as assists, I don’t want to exponentially increase the value of assists and expected assists at the top end of the desirability score. Goalscoring is the most important part of football, so I think small increases in the per 90 rate of goalscoring should be rewarded greatly, but creating chances, while still very important, is still reliant on the player receiving the ball putting it away, and I think the decisions made here reflect that.\nJust as with the goal threat component, the choices I’ve made here are more of an art than a science, and I think I could easily be convinced that the way I’ve structured the desirability functions for chance creation are incorrect. Unfortunately for you, I’m not actually having that debate, so I went ahead and made my own choice. It’s probably wrong. Table 2 shows the results of my poor choices, in life and in this blog post.\n\nchance_creation &lt;- \n  big_five_stats |&gt;\n  mutate(\n    across(\n      .cols = c(ast90, xa90),\n      ~ d_max(.x, use_data = TRUE),\n      .names = \"d_{.col}\"\n      ),\n    d_sca90 = d_max(sca90, use_data = TRUE, scale = 0.5),\n    d_chances = d_overall(across(starts_with(\"d_\")), tolerance = 0.1)\n  ) |&gt; \n  select(player, position, team, league, starts_with(\"d_\")) |&gt; \n  rename_at(vars(player:league), snakecase::to_title_case) |&gt; \n  rename(\n    \"Assists\" = d_ast90,\n    \"xA\" = d_xa90,\n    \"SCA\" = d_sca90,\n    \"Overall\" = d_chances\n  ) |&gt; \n  arrange(desc(Overall))\n\n\n\nTable Code (Click to Expand)\nchance_creation |&gt; \n  head(10) |&gt; \n  format_table(cols = 5:8)\n\n\n\n\nTable 2: Chance Creation Ratings\n\n\n\n\n\n\n\n\n\nPlayer\nPosition\nTeam\nLeague\nDesirability Scores\n\n\nAssists\nxA\nSCA\nOverall\n\n\n\n\nKevin De Bruyne\nMF\nManchester City\nPremier League\n0.94\n1.00\n1.00\n0.98\n\n\nNeymar\nMF\nParis S-G\nLigue 1\n1.00\n0.83\n0.97\n0.93\n\n\nLionel Messi\nFW\nParis S-G\nLigue 1\n0.80\n0.98\n0.98\n0.91\n\n\nRémy Cabella\nMF\nLille\nLigue 1\n0.56\n0.83\n0.99\n0.77\n\n\nThomas Müller\nFW\nBayern Munich\nBundesliga\n0.67\n0.73\n0.84\n0.74\n\n\nAntoine Griezmann\nFW\nAtlético Madrid\nLa Liga\n0.78\n0.60\n0.88\n0.74\n\n\nOusmane Dembélé\nFW\nBarcelona\nLa Liga\n0.70\n0.62\n0.91\n0.73\n\n\nRayan Cherki\nMF\nLyon\nLigue 1\n0.45\n0.83\n0.97\n0.71\n\n\nKingsley Coman\nFW\nBayern Munich\nBundesliga\n0.50\n0.79\n0.91\n0.71\n\n\nRaphaël Guerreiro\nDF\nDortmund\nBundesliga\n0.73\n0.60\n0.81\n0.71\n\n\n\nSource: FB Ref Via {worldfootballR}\n\n\n\n\n\n\n\n\n\n\n\nAgain, the names at the top of the Table 2 ratings give me confidence, especially the three names at the very top - Neymar, Kevin De Bruyne, and Lionel Messi. I don’t think there are any huge surprises in these ratings, perhaps with the exception of Rémy Cabella, who has some solid to very good creation numbers but wouldn’t necessarily be a name you’d immediately think of when thinking about the ten best creative players in the world. This is once again a reminder that Ligue 1 is very silly but is wonderfully chaotic."
  },
  {
    "objectID": "blog/2024-02-22-football-ratings-using-desirability-functions/index.html#ball-progression",
    "href": "blog/2024-02-22-football-ratings-using-desirability-functions/index.html#ball-progression",
    "title": "Developing Offensive Ratings Using Desirability Functions",
    "section": "Ball Progression",
    "text": "Ball Progression\nThe final component, and possibly the most interesting of the three, is ball progression. The three variables included in the ball progression desirability function are progressive carries, passes, and receptions. Just as is the case with the previous two components, I’m sure you could include other variables as measures of ball progression, but I’m still too lazy to do that and I think a lot of those other variables would risk double-counting where those actions are picked up by one of the progression metrics.\nI am particularly unsure about how to weight the value of progressive actions, especially progressive passing and receptions, which are connected actions. I could see an argument for giving less weight to progressive receptions, and treating carries and passes equally, but I could also see a case that progressive passes and receptions should be treated equally because both the passer and receiver have to do their job to make the progressive action successful. Given my uncertainty about this particular category, I’ve decided it is best to weight all three variables equally.\n\nball_progression &lt;-\n  big_five_stats |&gt;\n  mutate(\n    across(\n      starts_with(\"prg\"),\n      ~ d_max(.x, use_data = TRUE),\n      .names = \"d_{.col}\"\n    ),\n    d_prg = d_overall(across(starts_with(\"d_\")), tolerance = 0.1)\n  ) |&gt;\n  select(player, position, team, league, starts_with(\"d_\")) |&gt;\n  arrange(desc(d_prg)) |&gt;\n  rename_at(vars(player:league), snakecase::to_title_case) |&gt; \n  rename(\n    \"Progressive Carrying\" = d_prg_c90,\n    \"Progressive Passing\" = d_prg_p90,\n    \"Progressive Receptions\" = d_prg_r90,\n    \"Overall\" = d_prg\n  )\n\n\n\nTable Code (Click to Expand)\nball_progression |&gt;\n  head(10) |&gt;\n  format_table(cols = 5:8)\n\n\n\n\nTable 3: Ball Progression Ratings\n\n\n\n\n\n\n\n\n\nPlayer\nPosition\nTeam\nLeague\nDesirability Scores\n\n\nProgressive Carrying\nProgressive Passing\nProgressive Receptions\nOverall\n\n\n\n\nRayan Cherki\nMF\nLyon\nLigue 1\n0.81\n0.50\n0.66\n0.64\n\n\nKingsley Coman\nFW\nBayern Munich\nBundesliga\n0.79\n0.31\n1.00\n0.62\n\n\nOusmane Dembélé\nFW\nBarcelona\nLa Liga\n1.00\n0.32\n0.71\n0.61\n\n\nJack Grealish\nFW\nManchester City\nPremier League\n0.79\n0.30\n0.72\n0.56\n\n\nKevin De Bruyne\nMF\nManchester City\nPremier League\n0.47\n0.67\n0.48\n0.53\n\n\nKylian Mbappé\nFW\nParis S-G\nLigue 1\n0.67\n0.28\n0.79\n0.53\n\n\nRémy Cabella\nMF\nLille\nLigue 1\n0.51\n0.60\n0.46\n0.52\n\n\nNeymar\nMF\nParis S-G\nLigue 1\n0.51\n0.69\n0.39\n0.52\n\n\nRaphinha\nFW\nBarcelona\nLa Liga\n0.53\n0.31\n0.78\n0.51\n\n\nVinicius Júnior\nFW\nReal Madrid\nLa Liga\n0.89\n0.17\n0.84\n0.51\n\n\n\nSource: FB Ref Via {worldfootballR}\n\n\n\n\n\n\n\n\n\n\n\nForwards dominated the first two components but Table 3 is more diverse. This is to be expected, given that ball progression is often about moving the ball up the pitch to the forwards! Table 4 aggregates the ball progression desirability scores by position.\n\n\nTable Code (Click to Expand)\nball_progression |&gt; \n  summarise(across(where(is.numeric), mean), .by = Position) |&gt; \n  arrange(desc(Overall)) |&gt; \n  format_table(cols = 2:5)\n\n\n\n\nTable 4: Ball Progression Desirability by Position\n\n\n\n\n\n\n\n\n\nPosition\nDesirability Scores\n\n\nProgressive Carrying\nProgressive Passing\nProgressive Receptions\nOverall\n\n\n\n\nFW\n0.31\n0.16\n0.39\n0.26\n\n\nMF\n0.22\n0.35\n0.17\n0.23\n\n\nDF\n0.16\n0.24\n0.12\n0.18\n\n\n\nSource: FB Ref Via {worldfootballR}\n\n\n\n\n\n\n\n\n\n\n\nThe results are not shocking, but the marginal differences in progression by position are interesting. As expected, forwards are dominant in the receptions category, but the responsibilities for progressive carries are more evenly split, and forwards are third behind midfielders and defenders in progressive passing."
  },
  {
    "objectID": "blog/2024-02-22-football-ratings-using-desirability-functions/index.html#overall-offensive-ratings",
    "href": "blog/2024-02-22-football-ratings-using-desirability-functions/index.html#overall-offensive-ratings",
    "title": "Developing Offensive Ratings Using Desirability Functions",
    "section": "Overall Offensive Ratings",
    "text": "Overall Offensive Ratings\nHaving created desirability scores for each offensive component, we could combine all three into an overall offensive rating using d_overall(). I think this would be a reasonable choice when the assumption is that each category is equally important in the overall score, but as discussed already, I don’t think that’s the case here. I think goal threat matters most, and chance creation and ball progression are, while important enough to be the two other categories in the overall ratings, a little less important than goals.\nWhether or not goals are more important than chance creation and ball progression is probably open to plenty of debate, but assuming that they are (because this is my blog and I said so), we have to weight the three categories to reflect this. Rather than overcomplicating things and trying to do this as part of the process within the desirability functions, I have downweighted chance creation and ball progression by multiplying them by 0.5.\nFinally, I have opted for the arithmetic mean instead of the geometric mean, because I think the purpose of the geometric mean is of less value here.\n\noffensive_rating &lt;-\n  goal_threat |&gt;\n  rename(\"Goal Threat\" = Overall) |&gt; \n  full_join(chance_creation |&gt; rename(\"Chance Creation\" = Overall)) |&gt;\n  full_join(ball_progression |&gt; rename(\"Ball Progression\" = Overall)) |&gt;\n  mutate(\n    # downweight chance creation and ball progression desirability scores\n    across(.cols = c(\"Chance Creation\", \"Ball Progression\"), ~ .x * 0.5),\n    Overall =\n      d_overall(\n        across(.cols = c(\"Goal Threat\", \"Chance Creation\", \"Ball Progression\")),\n        geometric = FALSE\n        )\n  ) |&gt;\n  select(\n    Player, Position, Team, League, \"Goal Threat\", \n    \"Chance Creation\", \"Ball Progression\", Overall\n    ) |&gt;\n  arrange(desc(Overall))\n\n\n\nTable Code (Click to Expand)\noffensive_rating |&gt;\n  head(10) |&gt; \n  format_table(cols = 5:8)\n\n\n\n\nTable 5: Overall Offensive Ratings\n\n\n\n\n\n\n\n\n\nPlayer\nPosition\nTeam\nLeague\nDesirability Scores\n\n\nGoal Threat\nChance Creation\nBall Progression\nOverall\n\n\n\n\nKylian Mbappé\nFW\nParis S-G\nLigue 1\n0.91\n0.25\n0.26\n0.47\n\n\nLionel Messi\nFW\nParis S-G\nLigue 1\n0.54\n0.46\n0.25\n0.41\n\n\nErling Haaland\nFW\nManchester City\nPremier League\n0.93\n0.18\n0.08\n0.40\n\n\nRobert Lewandowski\nFW\nBarcelona\nLa Liga\n0.83\n0.21\n0.13\n0.39\n\n\nNeymar\nMF\nParis S-G\nLigue 1\n0.45\n0.46\n0.26\n0.39\n\n\nVictor Osimhen\nFW\nNapoli\nSerie A\n0.86\n0.13\n0.10\n0.36\n\n\nKarim Benzema\nFW\nReal Madrid\nLa Liga\n0.65\n0.21\n0.20\n0.36\n\n\nKingsley Coman\nFW\nBayern Munich\nBundesliga\n0.38\n0.36\n0.31\n0.35\n\n\nMohamed Salah\nFW\nLiverpool\nPremier League\n0.56\n0.27\n0.21\n0.34\n\n\nKevin De Bruyne\nMF\nManchester City\nPremier League\n0.25\n0.49\n0.27\n0.34\n\n\n\nSource: FB Ref Via {worldfootballR}\n\n\n\n\n\n\n\n\n\n\n\nThe old rule was that any metric not broken by Lionel Messi being so far ahead of everyone else is wrong, but Messi isn’t quite the freak of nature he once was. Nonetheless, Messi’s name being near the top of the ratings is a good sign, as is Mbappé and Haaland flanking him at the top. At a glance, there are few surprises in Table 5.\nKingsley Coman making it into the top ten feels generous, but perhaps the biggest issue is the three Ligue 1 players in the top five. While those three players are exceptional, they seem to be rated a little too high. I think this points to the inevitable league effects compromising the ratings. The big five leagues have been treated equally and we know that isn’t really the case (Ligue 1 is probably the weakest of the five). A more precise approach would involve weighting by league strength, but that’s complicated and I don’t want to do it.\nNonetheless, I think the ratings look pretty reasonable, and when we split the top ten overall desirability scores into their component parts, visualised below, we can see how the three offensive components are balanced.\n\n\nPlot Code (Click to Expand)\noffensive_rating |&gt; \n  head(10) |&gt; \n  tidyr::pivot_longer(\n    cols = 5:7,  \n    names_to = \"component\", \n    values_to = \"value\"\n    ) |&gt; \n  mutate(\n    component = \n      factor(\n        component, \n        levels = c(\"Goal Threat\", \"Chance Creation\", \"Ball Progression\")\n        )\n    ) |&gt; \n  ggplot(aes(x = reorder(Player, Overall), y = value, fill = component)) + \n  geom_col(position = position_dodge2(reverse = TRUE), colour = \"grey20\", linewidth = 0.4) +\n  geom_hline(yintercept = 0, colour = \"grey20\", linewidth = 0.8) +\n  coord_flip() +\n  scale_x_discrete(labels = scales::label_wrap(12)) +\n  scale_fill_manual(values = c(\"#026E99\", \"#D93649\", \"#FCCE26\")) +\n  labs(\n    title = \n        \"Component Desirability Scores for the Offensive Ratings' Best Players\",\n    subtitle = \n      stringr::str_wrap(\n        glue::glue(\n          \"The component desirability scores for the top ten players in the \",\n          \"offensive ratings in the big five leagues for the 2022/23 season. \",\n          \"The overall ratings are a weighted average of the desirability scores \",\n          \"for goal threat (50%), chance creation (25%), and ball progression (25%).\"\n          ),\n        width = 95\n      ),\n    x = NULL, y = \"Desirability\",\n    caption = \n      \"Visualisation: Paul Johnson  |  Data: FB Ref Via {worldfootballR}\"\n    ) +\n  theme(\n    panel.grid.major.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    axis.text.y = element_text(\n      colour = \"grey30\", size = rel(1.15), hjust = 0.5, \n      margin = margin(r = -15, l = -8), lineheight = .4\n      )\n    )"
  },
  {
    "objectID": "blog/2024-02-22-football-ratings-using-desirability-functions/index.html#footnotes",
    "href": "blog/2024-02-22-football-ratings-using-desirability-functions/index.html#footnotes",
    "title": "Developing Offensive Ratings Using Desirability Functions",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWith \\(\\boldsymbol{R}\\) variables being simultaneously optimised, denoted \\(f_{r}(\\boldsymbol{X})\n(r = 1 ... R)\\), each must be individually optimised using a desirability function that approaches 1 when \\(f_{r}(\\boldsymbol{X})\\) is optimal and approaches 0 when \\(f_{r}(\\boldsymbol{X})\\) is unacceptable. The optimisation function for minimising \\(f_{r}(\\boldsymbol{X})\\) (Kuhn 2016) is:\n\\[\n\\begin{equation}\n  d_{r}^{min}=\\begin{cases}\n  0 & \\text{if $f_r(\\boldsymbol{X}) &gt; B$}\\\\\n  (\\frac{f_{r}(\\boldsymbol{X})-B}{A-B})^{s} & \\text{if $A \\leq f_{r}(\\boldsymbol{X}) \\leq B$}\\\\\n  1 & \\text{if $f_r(\\boldsymbol{X}) &lt; A$}\n  \\end{cases}\n\\end{equation}\n\\]\nwhere \\(A\\) and \\(B\\) specify the minimum and maximum value that \\(f_{r}(\\boldsymbol{X})\\) can reach, and \\(s\\) is a scaling feature that makes it easier (\\(s &gt; 1\\)) or harder (\\(s &lt; 1\\)) to achieve maximal desirability.\nThe optimisation function for maximising \\(f_{r}(\\boldsymbol{X})\\) (Kuhn 2016) is:\n\\[\n\\begin{equation}\n  d_{r}^{max}=\\begin{cases}\n  0 & \\text{if $f_r(\\boldsymbol{X}) &lt; A$}\\\\\n  (\\frac{f_{r}(\\boldsymbol{X})-A}{B-A})^{s} & \\text{if $A \\leq f_{r}(\\boldsymbol{X}) \\leq B$}\\\\\n  1 & \\text{if $f_r(\\boldsymbol{X}) &gt; B$}\n  \\end{cases}\n\\end{equation}\n\\]\nwhere \\(A\\), \\(B\\), and \\(s\\) are again the minimum and maximum values and the scaling feature.↩︎\nThe arithmetic mean is the typical mean average value that people are used to seeing, which sums all values and divides the sum by the sample size, \\(n\\), while the geometric mean multiplies all values, before taking the \\(n\\)th root (the \\(n\\) being the number of values multiplied) of that product. Both arithmetic and geometric mean are measures of central tendency, but geometric mean is more robust when dealing with non-independent values, less responsive to outliers, and more capable of handling skewed data.↩︎\nDefensive event data does not reflect performance and is more of a measure of team and player style.↩︎"
  },
  {
    "objectID": "blog/2022-09-17-rendering-mermaid-diagrams-using-quarto-and-hugo/index.html",
    "href": "blog/2022-09-17-rendering-mermaid-diagrams-using-quarto-and-hugo/index.html",
    "title": "Rendering Mermaid Diagrams on a Hugo Website Using Quarto",
    "section": "",
    "text": "Note: Having ported my site over to Quarto in entirety, this blog post no longer applies to the site as is. There are some inconsistencies with some of the issues it talks about and how the site looks now (for example, there is no longer an option to toggle a dark theme), but the advice in this post should hopefully still work for anyone that is using Quarto with Hugo!\nI find a lot of the methods for drawing diagrams and flow charts using code to be a bit of a nightmare. I’m not sure what it is, perhaps I’m just a bit stupid, but they always seem a little more convoluted than I can handle. It’s possible that the design is actually of some value when it comes to drawing diagrams that are much larger and more complicated than I am generally dealing with, but in my case, it always seems to be a lot of work for what little I’m trying to do.\nThe easiest, most intuitive way that I’ve come across is using a JavaScript library called Mermaid. It’s just very easy, and the diagrams look great. Even better is the fact that it works with a lot of Markdown formats, and it’s even one of the diagram formats that Quarto can handle.\nOne issue that I came across, however, is that Mermaid diagrams don’t render when you’re using Quarto to create Hugo Markdown files for a Hugo website. You do have some simple options to resolve this, such as setting the mermaid-format to PNG or SVG in the article front matter, but with just a little extra work, you can get Mermaid working perfectly on your Hugo website."
  },
  {
    "objectID": "blog/2022-09-17-rendering-mermaid-diagrams-using-quarto-and-hugo/index.html#flowcharts",
    "href": "blog/2022-09-17-rendering-mermaid-diagrams-using-quarto-and-hugo/index.html#flowcharts",
    "title": "Rendering Mermaid Diagrams on a Hugo Website Using Quarto",
    "section": "Flowcharts",
    "text": "Flowcharts\nThe diagram I will get the most use out of is flowcharts, and Mermaid makes them really easy.\n\n```{mermaid}\n%%| label: fig-mermaid-flowchart\n%%| fig-cap: Wow look how it flows.\n\ngraph LR\n    A([Oooh]) --&gt; B([Would You])\n    B --&gt; C([Look at This])\n    C --&gt; D[Very]\n    C --&gt; E[Fancy]\n    C --&gt; F[Flowchart]\n```\n\n\n\n\n\ngraph LR\n    A([Oooh]) --&gt; B([Would You])\n    B --&gt; C([Look at This])\n    C --&gt; D[Very]\n    C --&gt; E[Fancy]\n    C --&gt; F[Flowchart]\n\n\n\n\nFigure 1: Wow look how it flows."
  },
  {
    "objectID": "blog/2022-09-17-rendering-mermaid-diagrams-using-quarto-and-hugo/index.html#gantt-charts",
    "href": "blog/2022-09-17-rendering-mermaid-diagrams-using-quarto-and-hugo/index.html#gantt-charts",
    "title": "Rendering Mermaid Diagrams on a Hugo Website Using Quarto",
    "section": "Gantt Charts",
    "text": "Gantt Charts\nIf Figure 1 isn’t enough to convince you, how about some Gantt charts?\n\n```{mermaid}\n%%| label: fig-mermaid-gantt\n%%| fig-cap: Oh you real fancy huh?\n\n%%{init: {'theme': 'default', 'themeVariables': { 'textColor': '#798189'}}}%%\ngantt\n    title Big Ol' Gantts\n    dateFormat  YYYY-MM-DD\n    section Section\n    Some Normal Tasks : done, a1, 2022-09-01, 15d\n    But Wait, There's More! :after a1, 20d\n    section And ANOTHER\n    Deary Me, It's Never Ending : crit, 2022-10-01, 20d\n    I Hate This Project : 14d\n```\n\n\n\n\n\n%%{init: {'theme': 'default', 'themeVariables': { 'textColor': '#798189'}}}%%\ngantt\n    title Big Ol' Gantts\n    dateFormat  YYYY-MM-DD\n    section Section\n    Some Normal Tasks : done, a1, 2022-09-01, 15d\n    But Wait, There's More! :after a1, 20d\n    section And ANOTHER\n    Deary Me, It's Never Ending : crit, 2022-10-01, 20d\n    I Hate This Project : 14d\n\n\n\n\nFigure 2: Oh you real fancy huh?\n\n\n\n\n\nThe dark and light theme my Hugo site uses doesn’t handle the Gantt chart as well as the other Mermaid diagrams. If I use the neutral Mermaid theme, it doesn’t show as clearly when using the dark Hugo theme, and vice versa with the dark Mermaid theme and light Hugo theme. This isn’t ideal, but you can customise the appearance of your Mermaid diagrams too.\nI’ve stuck with the neutral Mermaid theme for the rest of the site because it seems to be the best fit for flowcharts with the two different Hugo themes, and flowcharts are the diagrams I’m going to be using Mermaid for most often on this site. However, for this Gantt chart I’ve customised the appearance a little, using %%init%%, using the default theme and a grey text colour that works slightly better (though it’s still not brilliant)."
  },
  {
    "objectID": "blog/2022-09-17-rendering-mermaid-diagrams-using-quarto-and-hugo/index.html#git-graphs",
    "href": "blog/2022-09-17-rendering-mermaid-diagrams-using-quarto-and-hugo/index.html#git-graphs",
    "title": "Rendering Mermaid Diagrams on a Hugo Website Using Quarto",
    "section": "Git Graphs",
    "text": "Git Graphs\nSo you’ve got Figure 1 to wow people with your idea and Figure 2 to manage the project, but now you need some git graphs to show how you broke everything.\n\n```{mermaid}\n%%| label: fig-mermaid-git\n%%| fig-cap: Git a load of this!\n\ngitGraph\n  commit\n  commit\n  branch develop\n  checkout develop\n  commit\n  commit\n  checkout main\n  merge develop\n  commit\n  commit\n```\n\n\n\n\n\ngitGraph\n  commit\n  commit\n  branch develop\n  checkout develop\n  commit\n  commit\n  checkout main\n  merge develop\n  commit\n  commit\n\n\n\n\nFigure 3: Git a load of this!"
  },
  {
    "objectID": "blog/2022-05-10-learning-bayesian-statistics/index.html",
    "href": "blog/2022-05-10-learning-bayesian-statistics/index.html",
    "title": "Resources for Learning Bayesian Statistics",
    "section": "",
    "text": "As a social scientist turned data scientist, my graduate school training taught me a lot of frequentist statistics that has served me well in my career since. However, there’s one thing that frequentist statistics can’t solve, and that’s my lack of a personality. For that, we’ve got Bayesian statistics, the perfect substitute.\nI’ve spent the last year or so gradually trying to become more Bayesian in my approach to statistical reasoning and analysis, and while there’s quite a few really good resources for supporting that journey, it can be a little difficult to know where to start. I wanted someone to give me a short reading list that would save me trying to work out which books to use and in what order. In an attempt to fill that gap, I’ve put together a list of resources that I’ve come across and found to be pretty solid. I’ve not worked through each and every one of these cover to cover, but I’ve interacted with them enough to think they’re useful and that someone might be able to learn a lot from them.\nWhere possible I’ve tried to find stuff that is free and open-source, but that hasn’t been possible in every single case. But in each case I have at least found a freely available resource that is made available by the authors, whether it be the book itself, a video series, or a series of articles."
  },
  {
    "objectID": "blog/2022-05-10-learning-bayesian-statistics/index.html#statistical-rethinking",
    "href": "blog/2022-05-10-learning-bayesian-statistics/index.html#statistical-rethinking",
    "title": "Resources for Learning Bayesian Statistics",
    "section": "Statistical Rethinking",
    "text": "Statistical Rethinking\nI think the starting point for everybody, and an all-round great resource for anyone interested in statistics, is Richard McElreath’s Statistical Rethinking. If you read only one of these books, make it Statistical Rethinking. It’s a great read for anybody interested in statistics and quantitative research.\nThe focus of Statistical Rethinking is utilizing Bayesian statistics for scientific reasoning/thinking and causal inference. It’s a really great read, in part because it is really accessible while also being pretty thorough too. I particularly like his consistent focus on how these tools apply to scientific models.\nNot only is it available as a book, but Richard also makes a lecture series available on YouTube which can serve as an accompaniment to the book, or can even stand on its own as a good learning resource."
  },
  {
    "objectID": "blog/2022-05-10-learning-bayesian-statistics/index.html#students-guide-to-bayesian-statistics",
    "href": "blog/2022-05-10-learning-bayesian-statistics/index.html#students-guide-to-bayesian-statistics",
    "title": "Resources for Learning Bayesian Statistics",
    "section": "Student’s Guide to Bayesian Statistics",
    "text": "Student’s Guide to Bayesian Statistics\nBen Lambert’s A Student’s Guide to Bayesian Statistics is a really good dive into the statistics that underpin Bayesian analysis, and the implementation of the statistical methods. It’s not light on notation, but it does introduce new concepts by first explaining the intuition before laying it out mathematically. I think this book pairs very well with Statistical Rethinking, as they take slightly different approaches, and between the two you cover tons of ground.\nYou can buy the book on Amazon but it isn’t freely available anywhere (that I’ve found). Ben does offer a free YouTube course and lecture notes that mirror the book’s content though. The YouTube course doesn’t offer the kind of detail that the book does, but it is still pretty good."
  },
  {
    "objectID": "blog/2022-05-10-learning-bayesian-statistics/index.html#introduction-to-empirical-bayes",
    "href": "blog/2022-05-10-learning-bayesian-statistics/index.html#introduction-to-empirical-bayes",
    "title": "Resources for Learning Bayesian Statistics",
    "section": "Introduction to Empirical Bayes",
    "text": "Introduction to Empirical Bayes\nIf you’re looking for something to help you apply Bayesian statistics to a real-world situation, or you’re the kind of learner that needs a relatable hook to help you make sense of things (this is me), then David Robinson’s Introduction to Empirical Bayes is the perfect starting point. David applies Empirical Bayes methods (Bayesian approaches where the prior is estimated from the data) in a baseball context, specifically simulating batting averages. Baseball isn’t really the focus of David’s work here, but it serves as a really good medium for couching some quite complicated concepts in terms that people can understand, and thanks to this (and David’s excellent communication of the concepts), it is a really good opening gambit for anyone interested in Bayesian statistics.\nThe Introduction to Empirical Bayes book started out as a series of blog posts on David’s website, Variance Explained, and those blog posts are all still freely available, but his e-book also includes an extra chapter, some additional materials across several other chapters, and some minor edits and changes to help simplify things. The book is also available at a “pay-what-you-want” price, which means you can choose to pay nothing, if you can’t afford to pay more, but for those that can, the suggested price is $9.95 (~£8).\nI don’t think that David’s e-book/blog posts are the most thorough of the resources listed here, but they’re also not trying to be. Instead, they’re a really good plain(ish)-English explanation of Bayesian concepts, applied to examples that should help a lot of people intuit them a little easier. The use of Empirical Bayes serves as a good first step in the direction of Bayesian approaches, and it is also a good way to learn some really useful methods in real-world data analysis. If you find that some of the other resources are a little hard to wrap your head round, I’d read all of his posts (or buy his book if you can afford to) first, because it might help lay the groundwork for your learning journey."
  },
  {
    "objectID": "blog/2022-05-10-learning-bayesian-statistics/index.html#think-bayes",
    "href": "blog/2022-05-10-learning-bayesian-statistics/index.html#think-bayes",
    "title": "Resources for Learning Bayesian Statistics",
    "section": "Think Bayes",
    "text": "Think Bayes\nThe vast majority of the resources listed here use R for implementation. I think that’s because the packages for computing Bayesian models in R are much easier to navigate than their alternatives in Python. However, if you’re coming at this as someone with limited or no knowledge of R and no interest in changing that, then Allen Downey’s Think Bayes is worth a look. Think Bayes takes a code-first approach to teaching Bayesian concepts, rather than using notation and illustrating ideas using concepts like calculus. For anyone that uses Python and would learn better through concepts being explained through code, this book is a great start."
  },
  {
    "objectID": "blog/2022-05-10-learning-bayesian-statistics/index.html#bayesian-data-analysis",
    "href": "blog/2022-05-10-learning-bayesian-statistics/index.html#bayesian-data-analysis",
    "title": "Resources for Learning Bayesian Statistics",
    "section": "Bayesian Data Analysis",
    "text": "Bayesian Data Analysis\nEverything Andrew Gelman does is good, and Bayesian Data Analysis (co-authored with John B. Carlin, Hal S. Stern, David B. Dunson, Aki Vehtari, and Donald B. Rubin) is yet more proof of that. It’s a really extensive book covering the fundamentals of Bayesian inference, analysis, and methods.\nI think Bayesian Data Analysis might be a little heavier going if you’re just starting out, but the book is also supplemented by an online course made available by Aki Vehtari, with lecture slides and videos to help bridge any gaps in understanding.\nThe book and the course are excellent resources for those looking to build on foundational knowledge (perhaps developed using the earlier resources). It’s probably the most thorough of all the resources here, so if you’re comfortable with mathematical notation and only want to read one book, this might be your best bet."
  },
  {
    "objectID": "blog/2022-05-10-learning-bayesian-statistics/index.html#stan-documentation",
    "href": "blog/2022-05-10-learning-bayesian-statistics/index.html#stan-documentation",
    "title": "Resources for Learning Bayesian Statistics",
    "section": "Stan Documentation",
    "text": "Stan Documentation\nStan is a probabilistic programming language for implementing Bayesian statistical models. It’s the language that underpins the computation of Bayesian methods, and there are interfaces that make it accessible in a number of different languages, including R, Python, and Julia (for those of you looking to intimidate your peers).\nThe Stan documentation, and the documentation for the R interface to Stan, RStan, and the Python equivalent, PyStan are good resources for learning how to implement a variety of Bayesian models."
  },
  {
    "objectID": "blog/2022-05-10-learning-bayesian-statistics/index.html#regression-and-other-stories",
    "href": "blog/2022-05-10-learning-bayesian-statistics/index.html#regression-and-other-stories",
    "title": "Resources for Learning Bayesian Statistics",
    "section": "Regression and Other Stories",
    "text": "Regression and Other Stories\nMore Gelman. I have no regrets! Regression and Other Stories is co-authored with Jennifer Hill and Aki Vehtari, and it will serve you brilliantly if you’re looking to learn about Bayesian approaches while also covering the frequentist methods too. It is a book that focuses on the application of regression to real-world problems and the challenges users will face in the process.\nWhile the focus of the book is regression, it incorporates Bayesian inference and methods across the book, and this might be useful to anyone wanting a broader education in statistical analysis that includes Bayesian approaches."
  },
  {
    "objectID": "blog/2022-05-10-learning-bayesian-statistics/index.html#probabilistic-machine-learning",
    "href": "blog/2022-05-10-learning-bayesian-statistics/index.html#probabilistic-machine-learning",
    "title": "Resources for Learning Bayesian Statistics",
    "section": "Probabilistic Machine Learning",
    "text": "Probabilistic Machine Learning\nIf you’re looking to apply your newly developed Bayesian principles to machine learning, then Probabilistic Machine Learning: An Introduction by Kevin Patrick Murphy is a great start.\nIt’s not specifically a Bayesian machine learning book, but probabilistic machine learning incorporates a lot of Bayesian principles, and there’s plenty of explicitly Bayesian tools in the book too. If you’ve cracked several of the resources in this list and are looking to take your next step, I think this would be a great direction."
  },
  {
    "objectID": "blog/2022-02-19-make-more-packages/index.html",
    "href": "blog/2022-02-19-make-more-packages/index.html",
    "title": "Make More Packages!",
    "section": "",
    "text": "Having learned R in an academic context, I wasn’t exposed to programming in a linear path that taught me the basics of programming theory and application. I learned how to carry out statistical analysis using R, and my interaction with packages was that they would help me get the job done without having to do everything manually. As a result, package development has always intimidated me a little bit. It seemed like the responsibility of much smarter people that know what they’re doing, and that can build packages that can help me figure out what I’m doing too.\nTo be fair, that was probably an accurate description of my situation for… a good few years of my time using R. But practice makes you less bad at things, and over time I’ve gone from being someone that uses R like it’s a boiling pot of oil that they’re about to chuck a fistful of frozen chips at for the first time, to someone that has full confidence that they can’t do any damage because they’ve already burned the house down anyway.\nThat is (mostly) a lie. I’m now (mostly) good at what I do. But that didn’t stop package development being a process that intimidated me.\nHowever, I recently made the jump into some basic package development as a result of working on an internal package that I am helping to develop. While it wasn’t being built entirely from scratch, this package was a good opportunity for me to play around with some cool functions and learn about package infrastructure. Having done so, I realised that the process is actually pretty painless.\nTo follow-up on what I’d learned, I decided to build my own (stupid but funny) package from scratch, and made nice.\nNow, I thought I would write this short blog post detailing the process for creating a very basic package, in the hope that it might help someone else overcome an imagined barrier to entry. I will run through the process of setting up a package, adding some simple functions, and then building out the basic infrastructure with documentation and testing, using nice as an example.\n\nInitial Setup\nIn order to build a basic package from scratch, you should only need to rely on four different packages that will aid you in setting everything up, building the correct infrastructure, and documenting and testing your functions.\nThe packages you need:\n\ndevtools\nusethis\nroxygen2\ntestthat\n\ndevtools is responsible for the majority of the functions you’ll need to build and maintain a package and usethis handles the initial setup of the project, while roxygen2 helps you document your package, and testthat simplifies unit testing.\n\n\nFunctions\nPackages are effectively just a way of housing and distributing functions. If you’re making a package, you’ll need some functions! They don’t need to be wildly complex. They just need to work, and ideally they would be reasonably useful too. However, in the absence of useful functions, we’ve got an example of a funny function instead:\n\ncheck &lt;- function(x){\n  if (x %in% c(69, 0.69)){\n    print(\"Nice!\")\n  } else {\n    print(\"Not very nice.\")\n  }\n}\n\nThis is very simple. All it is doing is checking if an object x matches either the number 69 or 0.69. If it does, then it returns “Nice!”, and if it doesn’t, it returns “Not very nice.”. If you’re just looking to build a basic package as a means of learning how to do so, this is all you need! You can build on this iteratively, as you become more confident with package development, or come up with other useful functions your package should include.\nI think it is generally best practice to dedicate a different file to each individual function, though it’s not absolutely necessary. If you choose to, you can store all your functions in one file, and this might be a sensible approach if you’re creating a package that contains all your miscellaneous functions. However, for anything a little more meaty than that, I think the best approach is to split out everything into files for each function. That makes it easier for someone (likely including yourself) to navigate the source code.\n\n\nDocumentation\nThe next step is to document your package, so anyone using it will have some guidance on what they’re doing and why. Documentation doesn’t need to be complicated, though if the function it is documenting is pretty complex, it should probably go into plenty of detail to help anyone trying to use it. In the nice example, the functions are super simple, so there’s no sense in overdoing the docs either.\nYou need to add comments at the beginning of your function script which explain what the function does, details any of the required parameters for the function to run, and give some examples of usage. In order for roxygen2 to recognise the comments that need to be turned into documentation, you have to add #’ in front of them.\nBelow is a basic example of what is needed, corresponding to the nice check function above:\n\n#' Checking whether your R output is really nice\n#'\n#' @keywords nice\n#' @export\n#' @examples\n#' check(69)\n#' \n\nHaving done this, devtools will process the documentation so that it all works as expected:\n\ndevtools::document()\n\nYou can get a look at your documentation by calling ?function like you normally would. In this case, calling ?check returns the documentation for the nice check function.\n\n\nTesting\nIn order to reduce the likelihood that your package becomes a mess of errors, it is important to test it. The best way of doing this is automating your tests, to make everything reproducible, and reduce your own opportunities to do anything stupid!\nTo set up your tests, the following creates a new script for all tests of the particular function(s) you’re working on.\n\nusethis::use_test(\"name\")\n\nAn example of a basic test:\n\ntest_that(\"check returns correct type of response\", {\n  nice &lt;- check(69)\n  expect_type(nice, \"character\")\n})\n\nThis test is, as it describes, checking that check is returning the right type of output when it runs. The test creates an object called nice that contains the output from the check function, when the input it receives is 69. The test expects the object to be of type character, and if that is so, then it will pass the test. If you want to build further tests for this particular function, you could check that the function correctly recognises whether the input is nice or not, and returns the correct response.\nThis would look as follows:\n\ntest_that(\"check returns correct positive response\", {\n  nice &lt;- check(69)\n  expect_equal(nice, \"Nice!\")\n})\n\nOnce you have set up some tests that you want to use to check your functions, you can run those tests by calling on devtools:\n\ndevtools::test()\n\nThis will run through each test and check that it returns the expected output. If it doesn’t it will show that it has failed, and show you where exactly that failure has occurred, in order to help you fix it. If it has worked, then you’re all good!\n\n\nFurther Reading\nThis was a very, very quick run-through on how to create a basic package, just to get started, but if you want to make something that is a little more useful than some dumb function to check if your outputs are nice, then there’s plenty of resources for how to take your packages from basics to CRAN and beyond.\nThe starting point, as is often the case, is Hadley Wickham. Hadley Wickham and Jenny Bryan’s R Packages is an excellent resource for learning to make some killer packages. And if you’re looking to learn how to build the kind of functions that belong in those killer packages, then Hadley Wickham’s Advanced R will get you most of the way there.\n\n\n\nAcknowledgments\n\nPreview image by Jiawei Zhao on Unsplash.\n\n\nReuseCC BY-SA 4.0CitationFor attribution, please cite this work as:\nJohnson, Paul. 2022. “Make More Packages!” February 19,\n2022. https://paulrjohnson.net/blog/2022-02-19-make-more-packages."
  },
  {
    "objectID": "blog/2020-04-14-football-analytics/index.html",
    "href": "blog/2020-04-14-football-analytics/index.html",
    "title": "Recent Football Analytics Articles",
    "section": "",
    "text": "As a side-project/hobby, I write about football. Some of my recent work (as of 04/20) can be found below:\n\nPast his peak? Assessing goalkeeper performance by age and Manuel Neuer vs Alexander Nübel\nFor Bavarian Football Works, I analyzed goalkeeper peak age, and the performance of Bayern Munich’s great but ageing Manuel Neuer. I found that Neuer continues to be a very good goalkeeper, but that his performances are gradually declining, suggesting that the effects of age are starting to wear on him, and showing that even the greatest athletes ever can’t beat Father Time.\n\n\n\nNeuer\n\n\n\n\nThe Hinrunde Review: Analyzing Dortmund’s Season So Far\nBefore the 2019/20 season was interrupted by Covid-19, I carried out an analysis of the first half of the BVB season. I looked at team and individual performances, and identified some key areas of concern going forward.\n\n\n\nnpxG+xA/90\n\n\n\n\nThe Earth is Flat and Felix Zwayer is an Agent of Bavaria: Are Referees Biased?\nUsing data taken from the German football website, Wahre Tabelle, I studied referee errors and looked for evidence of bias for or against certain teams. Every fan base likes to believe they’re hard done by, but unfortunately I found little to suggest this is really the case.\n\n\n\nCorrections\n\n\n\n\n\nAcknowledgments\n\nPreview image by Álvaro Mendoza on Unsplash.\n\n\n\nNeuer\nnpxG+xA/90\nCorrections\n\nReuseCC BY-SA 4.0CitationFor attribution, please cite this work as:\nJohnson, Paul. 2020. “Recent Football Analytics Articles.”\nApril 14, 2020. https://paulrjohnson.net/blog/2020-04-14-football-analytics."
  },
  {
    "objectID": "blog/2022-01-17-making-myself-accountable/index.html",
    "href": "blog/2022-01-17-making-myself-accountable/index.html",
    "title": "Making Myself Accountable",
    "section": "",
    "text": "One of my goals when setting up this site was to use the blog section reasonably frequently to share some of my work and share anything particularly interesting that I’ve learned in my development as a data scientist. There hasn’t been much sharing going on. I’d like to think that isn’t because I haven’t done or learned anything interesting (…), I just haven’t found the time to share it.\nI’m the ideas man. No one told me I’d also have to be the one to finish the ideas too. That seems deeply unfair.\nBut having realized that everything I’ve thought about sharing either hasn’t made it out of my head, or has been shared with friends in the form of a dumb joke, I’m looking to set myself some goals that will make the blog a little more active. So I’m using this space as an opportunity to hold myself accountable to my own fantastical intentions.\nOver the course of the next 12 months, I’m going to try and average a blog post a month, around data science generally, and anything cool that I’m either learning or doing.\nGiven that everything is posting and everyone is a poster, it’s time I put some hours in at the content factory. New Year, New Me.\n\n\nAcknowledgments\n\nPreview image by Jack Lucas Smith on Unsplash.\n\n\nReuseCC BY-SA 4.0CitationFor attribution, please cite this work as:\nJohnson, Paul. 2022. “Making Myself Accountable.” January\n17, 2022. https://paulrjohnson.net/blog/2022-01-17-making-myself-accountable."
  },
  {
    "objectID": "blog/2022-05-07-combining-tidymodels-and-caret/index.html",
    "href": "blog/2022-05-07-combining-tidymodels-and-caret/index.html",
    "title": "Combining Tidymodels & Caret for Machine Learning in R",
    "section": "",
    "text": "The two main approaches to building machine learning models in R are caret and tidymodels. Having tried both, I found that I struggled to pick my favorite. There’s elements of both that made more intuitive sense to me than the other. I think it’s a product of having become very familiar with the tidyverse, particularly dplyr, for data wrangling, but still using a lot of Base R functions for statistical modeling.\nThe process for prepping the data for a machine learning model seems to make a ton of sense to me when done in tidymodels (using recipes and rsample), but the equivalent process using caret felt a little clunky. However, specifying and training models using caret made a lot of sense to my broken brain.\nAnyway, I recently discovered something that is probably entirely unremarkable to everyone else, and that probably shouldn’t have taken me by surprise… You can just combine the two! You can split and preprocess your data using the tidymodels framework before defecting to caret for the next steps. What a time to be alive.\n\nTraining a Random Forest Model to Predict Diabetes\nBecause I’m not a savage, I won’t leave you without a simple worked example. We’ll use Gary Hutson’s really useful MLDataR package to grab a toy diabetes dataset, cleaning the variable names using janitor, and converting the target variable, diabetic_class, to a factor.\n\n# import packages\nsuppressPackageStartupMessages({\n  library(dplyr)\n  library(caret)\n  library(recipes)\n  library(randomForest)\n})\n\n# load data\ndiabetes_raw &lt;- MLDataR::diabetes_data\n\n# clean data\ndf &lt;-\n  diabetes_raw %&gt;%\n  janitor::clean_names() %&gt;%\n  mutate(diabetic_class = as.factor(diabetic_class))\n\nHaving done this, we can use rsample to split the data into a train and test set.\n\n# set random seed\nset.seed(456)\n\n# split train/test data\ntrain_test_split &lt;-\n  rsample::initial_split(df,\n    strata = diabetic_class,\n    prop = 0.7\n  )\n\n# create train/test sets\ntrain_df &lt;- rsample::training(train_test_split)\ntest_df &lt;- rsample::testing(train_test_split)\n\nThe next step is a little more involved, and is where I think tidymodels really excels. Using the recipes package, we can specify all the preprocessing steps needed for the dataset, such that the data will then be ready for training a machine learning model.\n\n# preprocessing\nmodel_recipe &lt;-\n  recipe(diabetic_class ~ ., data = train_df) %&gt;%\n  # combine low frequency factor levels\n  step_other(all_nominal(), threshold = 0.05) %&gt;%\n  # remove predictors with zero variance\n  step_nzv(all_predictors()) %&gt;%\n  # normalize numeric variables (sigma = 1, mu = 0)\n  step_normalize(all_numeric()) %&gt;%\n  # convert nominal variables to numeric binary variables\n  step_dummy(all_nominal(), -all_outcomes(), one_hot = TRUE)\n\nYou can check that all the preprocessing steps are working as expected by using prep() and juice().\n\n# check preprocessing results\nmodel_recipe %&gt;%\n  prep() %&gt;%\n  juice() %&gt;%\n  head()\n\n# A tibble: 6 × 32\n     age diabetic_class gender_Female gender_Male excess_urination_No excess_urination_Yes polydipsia_No polydipsia_Yes\n   &lt;dbl&gt; &lt;fct&gt;                  &lt;dbl&gt;       &lt;dbl&gt;               &lt;dbl&gt;                &lt;dbl&gt;         &lt;dbl&gt;          &lt;dbl&gt;\n1 -0.677 Negative                   0           1                   1                    0             0              1\n2 -1.19  Negative                   0           1                   1                    0             1              0\n3 -1.53  Negative                   0           1                   1                    0             1              0\n4  1.62  Negative                   0           1                   0                    1             1              0\n5  1.02  Negative                   0           1                   1                    0             1              0\n6  0.854 Negative                   0           1                   1                    0             1              0\n# ℹ 24 more variables: weight_loss_sudden_No &lt;dbl&gt;, weight_loss_sudden_Yes &lt;dbl&gt;, fatigue_No &lt;dbl&gt;, fatigue_Yes &lt;dbl&gt;,\n#   polyphagia_No &lt;dbl&gt;, polyphagia_Yes &lt;dbl&gt;, genital_thrush_No &lt;dbl&gt;, genital_thrush_Yes &lt;dbl&gt;,\n#   blurred_vision_No &lt;dbl&gt;, blurred_vision_Yes &lt;dbl&gt;, itching_No &lt;dbl&gt;, itching_Yes &lt;dbl&gt;, irritability_No &lt;dbl&gt;,\n#   irritability_Yes &lt;dbl&gt;, delay_healing_No &lt;dbl&gt;, delay_healing_Yes &lt;dbl&gt;, partial_psoriasis_No &lt;dbl&gt;,\n#   partial_psoriasis_Yes &lt;dbl&gt;, muscle_stiffness_No &lt;dbl&gt;, muscle_stiffness_Yes &lt;dbl&gt;, alopecia_No &lt;dbl&gt;,\n#   alopecia_Yes &lt;dbl&gt;, obesity_No &lt;dbl&gt;, obesity_Yes &lt;dbl&gt;\n\n\nIf everything looks alright, you can take the model_recipe object that you’ve created and use it as the model formula that you would otherwise have to specify in the caret train() function.\nFor the rest of the process, you can switch over to caret, first using the trainControl() function to specify the training parameters and then the train() function for the model training.\n\n# set random seed\nset.seed(456)\n\n# control parameters for model training\nctrl &lt;-\n  trainControl(\n    method = \"cv\",\n    number = 5,\n    classProbs = TRUE,\n    summaryFunction = twoClassSummary\n  )\n\n# train random forest model\nrf_mod &lt;-\n  train(\n    model_recipe,\n    data = train_df,\n    method = \"rf\",\n    tunelength = 10,\n    metric = \"ROC\",\n    trControl = ctrl,\n    importance = TRUE\n  )\n\nWarning in tmp$resample: partial match of 'resample' to 'resamples'\n\n\nHaving trained the random forest model, you can check the performance, and see what parameters were chosen in the tuning process.\n\n# check results\nprint(rf_mod)\n\nRandom Forest \n\n364 samples\n 16 predictor\n  2 classes: 'Negative', 'Positive' \n\nRecipe steps: other, nzv, normalize, dummy \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 292, 291, 291, 291, 291 \nResampling results across tuning parameters:\n\n  mtry  ROC        Sens       Spec     \n   2    0.9947186  0.9500000  0.9372727\n  16    0.9873918  0.9428571  0.9281818\n  31    0.9850108  0.9428571  0.9235354\n\nROC was used to select the optimal model using the largest value.\nThe final value used for the model was mtry = 2.\n\n\nNot bad! The best performing model has an ROC of 0.995 and both the sensitivity and specificity are ~0.95. Pretty solid for a quick and easy model.\nTo really test the model’s performance, we want to see how it copes with the test data that it hasn’t seen.\n\n# make predictions on test data\nrf_predict &lt;- predict(rf_mod, newdata = test_df, type = \"prob\")\nrf_class &lt;- predict(rf_mod, newdata = test_df, type = \"raw\")\n\npreds &lt;-\n  cbind(rf_predict, rf_class) %&gt;%\n  mutate(\n    Positive = round(Positive, digits = 2),\n    Negative = round(Negative, digits = 2)\n  )\n\nFinally, we can produce a confidence matrix for a more intuitive look at how the model is performing on the test set.\n\ncm_class &lt;- test_df[, names(test_df) %in% c(\"diabetic_class\")]\n\nconfusionMatrix(\n    rf_class,\n    as.factor(cm_class$diabetic_class),\n    positive = \"Positive\"\n  )\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction Negative Positive\n  Negative       57        1\n  Positive        3       95\n                                         \n               Accuracy : 0.9744         \n                 95% CI : (0.9357, 0.993)\n    No Information Rate : 0.6154         \n    P-Value [Acc &gt; NIR] : &lt;2e-16         \n                                         \n                  Kappa : 0.9455         \n                                         \n Mcnemar's Test P-Value : 0.6171         \n                                         \n            Sensitivity : 0.9896         \n            Specificity : 0.9500         \n         Pos Pred Value : 0.9694         \n         Neg Pred Value : 0.9828         \n             Prevalence : 0.6154         \n         Detection Rate : 0.6090         \n   Detection Prevalence : 0.6282         \n      Balanced Accuracy : 0.9698         \n                                         \n       'Positive' Class : Positive       \n                                         \n\n\nThe results are pretty good for a very quick model. How exciting. Lets pretend that it’s because I’m a brilliant data scientist rather than it being due to the very clean, balanced toy dataset we used.\n\n\nConclusion\nSo there you have it, if you’re in the same position as me and you’re struggling to pick between tidymodels and caret, because both frameworks offer something you like, you can just combine the two and make Frankenstein’s framework.\nUltimately, despite this blog post, I’m probably going to stick with tidymodels (why am I like this?). I think that I’m going to force myself to get used to the tidymodels framework end-to-end because a) it is receiving tons of development so it’s probably going to continue to get better and bigger, and will be leading the way for the foreseeable future, and b) because in reality I think the explicit way that you structure each step is probably sensible, even if it confuses me a bit.\nBut it’s nice to know that I’ve got options.\n\n\n\nAcknowledgments\n\nPreview image by Rock’n Roll Monkey on Unsplash.\n\n\nReuseCC BY-SA 4.0CitationFor attribution, please cite this work as:\nJohnson, Paul. 2022. “Combining Tidymodels & Caret for Machine\nLearning in R.” May 7, 2022. https://paulrjohnson.net/blog/2022-05-07-combining-tidymodels-and-caret."
  },
  {
    "objectID": "blog/2022-09-11-setting-up-hugo-website-with-quarto/index.html",
    "href": "blog/2022-09-11-setting-up-hugo-website-with-quarto/index.html",
    "title": "Setting up a Hugo Website with Quarto",
    "section": "",
    "text": "Like every other nerd that likes R and has used RMarkdown more than one time in their entire life, I am now in the process of transitioning to using Quarto, and I want to let the world know! I wanted to make good use of Quarto, including utilising the really nice job it does with rendering R, Python, Julia, and JavaScript (not that I’m writing a lot of Julia or JavaScript) and outputting it in a HTML document. However, I didn’t want to give up this lovely website I had already put together using Hugo. What a conundrum… Well, it turns out it’s relatively easy to use Quarto with a Hugo site. These folks really have thought of everything!\nThis isn’t a tutorial for building a Hugo website or building a website from scratch with Quarto. These are more involved processes (though neither are incredibly difficult). If that’s what you’re looking for, here are some links that might point you in the right direction.\nHugo Tutorials:\n\nHugo - Quick Start\nfreeCodeCamp - How to Create Your First Hugo Blog\n\nQuarto Tutorials:\n\nQuarto - Creating a Website\nBea Milz - Creating a Blog with Quarto in 10 Steps\nAlbert Rapp - The Ultimate Guide to Starting a Quarto Blog\n\nNo, my ambitions with this blog post are considerably less ambitious. I just want to show you how to set your existing Hugo website up so that you can use Quarto to write your blog posts (and any other pages).\n\nSetting Up Hugo for Quarto Files\nThe first step is to make some changes to your ‘config.toml’ file so that it doesn’t try and publish your Quarto documents, whether they are in qmd or ipynb format. You need to add the following code to the file:\nignoreFiles = [ \"\\\\.qmd$\", \"\\\\.ipynb$\" ]\nI added it to the opening section of the config file, with the baseUrl, title, theme etc.\nYou also need to add some configuration to the markup section of the config file, in order to make sure Hugo handles raw HTML appropriately, because R and Python will sometimes give HTML as an output rather than Markdown.\n[markup.goldmark.renderer]\n  unsafe= true\nThis is essentially everything you need to get your Hugo website set up to deal with Quarto without losing its mind.\n\n\nCreating Blog Posts\nThis is the part that I expected to at least be a little bit harder than it turned out to be. Once you have set your config file up, you can now create new posts (and any other pages) in pretty much the same way you did in the before times. In my case, all the blog posts are stored under ‘content/posts’, and it’s the same deal now in this exciting new Quarto world.\nBlog posts are written in a qmd file, with the YAML front matter looking much the same as the typical Markdown posts for Hugo sites, with tags, categories, keywords etc., and format: hugo added so that Quarto knows how to render the file. This can be also be added to a ’_quarto.yml’ file, as I have done, if you are treating the site as a Quarto project.\nOnce you’ve done all this, you can render your Quarto files and it will produce Markdown files that Hugo will publish as blog posts.\n\n\nDon’t Forget to Render\nThe biggest problem I’ve faced with this process, in the short time since making the change, is that I am a stupid person and that I sometimes do stupid things.\nWhen you’ve written your posts in a qmd file you have to remember to render them as and when you make changes that you want to deploy. I have already managed to forget to do that on a couple occasions, pushing changes that I haven’t rendered, resulting in qmd files that don’t line up with the md file, and therefore don’t show on the site. Think of all the funny things you might write that people won’t get to see, and it’s all your fault.\n\n\n\nSpend some time in the shame cube\n\n\n\n\nConclusion\nThe rendering issue isn’t ideal, because as much as I’d like to believe that I will eventually remember that I need to render before I publish posts, I happen to know that I am a buffoon, so I am certain that I will forget enough that it’s going to be a bit of a nuisance.\nThis should be something that I can resolve by setting up a GitHub Actions workflow to render files when pushed to the repository. It’s not something I’ve done just yet, mostly because it is 8pm on a Sunday and I can’t be bothered to take that extra step right now. But maybe I’ll write another post about that when I get round to it. Because everything is content and everybody is posting.\nHowever, everything else is exceptionally smooth, and very easy to set up. It takes a matter of minutes to make the change and now your Hugo site can be used with Quarto, making it so much easier to write about and incorporate R, Python, and other programming languages in your blog posts. Great success!\n\n\n\nSpend some time in the shame cube\n\n\n\nReuseCC BY-SA 4.0CitationFor attribution, please cite this work as:\nJohnson, Paul. 2022. “Setting up a Hugo Website with\nQuarto.” September 11, 2022. https://paulrjohnson.net/blog/2022-09-11-setting-up-hugo-website-with-quarto."
  },
  {
    "objectID": "blog/2022-11-01-multilevel-model-r-cheatsheet/index.html",
    "href": "blog/2022-11-01-multilevel-model-r-cheatsheet/index.html",
    "title": "A Cheatsheet for Building Multilevel Models in R",
    "section": "",
    "text": "Multilevel models (MLMs) are hard. Some part of the difficulty is due to the fact that multilevel data structures add an extra layer of complexity conceptually and mathematically, and therefore the appropriate model structure for this kind of data is also more complex. However, there’s plenty about them that just seems needlessly hard.\nOh would you like to make a multilevel model or a hierarchical model? But have you considered a generalized linear mixed effects model? What about a variance component model? WHAT DO YOU MEAN THEY’RE THE SAME THING?!1 That can’t possibly be true. Why would anyone in statistics or the sciences make anything needlessly convoluted and use different names for the same concepts? Thankfully things are much simpler when it comes to discussing fixed and random effects. Just a couple concepts with precise meaning and usage in every field… Wait, what? Oh you’ve got be fucking kidding me.\nSo I thought I’d try and make things a little easier for anyone taking their first steps into this fresh hell, and for anyone (like me) that has a mushy, liquefied brain and needs a refresher any time they haven’t looked at something every single day for the last two years.\nI’m not going to write a full introduction, or a detailed how-to for constructing multilevel models, because a) that sounds long as hell and b) there are much smarter people than I that have already done these things (resources are linked at the end of the post). This will just serve as a quick explainer of what MLMs are, when you need them, and the syntax for constructing the most basic (and most common) types of MLMs in R. The goal is to keep this relatively short and make it accessible for folks without intensive statistics backgrounds. Therefore, I will attempt to keep the notation to the minimum, explaining model structures and using examples to illustrate. I have included some notation, but I’ve kept it hidden to make this cheatsheet2 a little bit less intimidating. If you fancy your chances, you can expand the notation sections.\nI won’t go into the details about fitting, interpreting, and validating multilevel models here, but I will possibly turn this into a series, and if so I will get in to it in the next blog post. LFG!!!"
  },
  {
    "objectID": "blog/2022-11-01-multilevel-model-r-cheatsheet/index.html#its-mlms-all-the-way-down",
    "href": "blog/2022-11-01-multilevel-model-r-cheatsheet/index.html#its-mlms-all-the-way-down",
    "title": "A Cheatsheet for Building Multilevel Models in R",
    "section": "It’s MLMs All The Way Down",
    "text": "It’s MLMs All The Way Down\nThere is lots of confusing and sometimes contradictory terminology that is used when talking about multilevel models.\n\nFixed & Random Effects\nWhen discussing the ways that an effect can occur at different levels in a multilevel model, people will often refer to fixed and random effects. Fixed effects are those that are fixed across groups, or that are the same for every cluster, while the random effects are those that deviate between groups. The fixed and random effects could refer to either the intercept or the slope, as these can both vary between groups, and can in some cases both vary at the same time.\nHowever, as Andrew Gelman has argued, these terms are sometimes used to refer to slightly different things, and this can lead to some confusion. So whenever you see someone refer to fixed or random effects, be aware that there may be some variance in how these terms are being used.\nAn alternative terminology, that possibly carries less confusion, is population- and group-level effects. When the variable is constant across groups/clusters and the effect varies at a unit-level (fixed-effect), this is referred to as a population-level effect. However, when the variable’s effect varies across groups/clusters (random-effect) then it is a group-level effect.\nFor the remainder of this blog post I will use this population- and group-level terminology.\n\n\nLinear Mixed Models vs Multilevel Models\nThere are some slight differences between what a linear mixed model might refer to,4 and what a hierarchical or multilevel model5 might refer to, but on the whole I think it is safe to treat them (generally speaking) as the same thing, with an understanding that multilevel models are any models where there are grouping structures that lead to the data violating the independence assumption, and hierarchical grouping structures are just one type of such structure.\nAn example of a context where you might see a grouping structure that isn’t hierarchical/nested is data where individuals are measured repeatedly, like, for example, a dataset containing repeated polling of individual’s views on a political candidate or issue (panel data). In this case, there is no hierarchical structure, but the grouping structure is the individual."
  },
  {
    "objectID": "blog/2022-11-01-multilevel-model-r-cheatsheet/index.html#varyings-intercepts",
    "href": "blog/2022-11-01-multilevel-model-r-cheatsheet/index.html#varyings-intercepts",
    "title": "A Cheatsheet for Building Multilevel Models in R",
    "section": "Varyings Intercepts",
    "text": "Varyings Intercepts\nThe simplest kind of model you can specify with a grouping structure is one which models a grouping structure with deviance from the global intercept, but no unit-level predictors. This means that the intercept is allowed to vary between groups (for example, patient outcomes in different hospitals), around the mean intercept of all groups (the global intercept). This is often referred to as a null model, or a varying (random) intercepts model.\nThis can be specified as such:\n\ny ~ 1 + (1|g)\n\nThe 1 outside of the brackets here specifies the fixed mean global intercept, and the (1|g1) specifies that the grouping structure produces intercepts that vary for each group.\n\n\nNotation\n\n\\[y_{ij} = \\alpha_{j} +  \\epsilon_{ij}\\]\n\\(\\alpha_{j}\\) refers to the intercept (\\(\\alpha\\)) for every group (\\(j\\)), meaning that every group \\(j\\) has its own intercept. The formula for \\(\\alpha_{j}\\) can be written as follows:\n\\(\\alpha_{j} = \\gamma_{00} + \\mu_{0j}\\)\nThis formula is the group level intercept variance, explaining the \\(j\\)th group’s deviation from the mean global intercept, with \\(\\gamma_{00}\\) representing the mean global intercept, and \\(\\mu_{0j}\\) representing the deviance from the mean intercept.\n\n\nAdding Population Effects\nWhile the previous model only included a group-level effect that caused deviance from the global intercept, there was no population-level effect in the model. If we add a population-level effect \\(x\\) to the model, we get the following:\n\ny ~ x + (1|g)\n\nThis is a pretty typical multilevel model that you’ll see plenty of examples of in the real-world and find plenty of use for when implementing MLMs yourself.\n\n\nNotation\n\n\\[y_{ij} = \\alpha_{j} + \\beta_{1j}x_{ij} + \\epsilon_{ij}\\]\nThis is the level one formula for the value of \\(y\\) indexed by \\(i\\) units and \\(j\\) groups.\n\\(\\alpha_{j}\\) refers to the intercept (\\(\\alpha_{}\\)) for every group (\\(j\\)), meaning that every group \\(j\\) has its own intercept. The formula for \\(\\alpha_{j}\\) can be written as follows:\n\\(\\alpha_{j} = \\gamma_{00} + \\mu_{0j}\\)\nAnd the formula for \\(\\beta_{1j}\\) is:\n\\(\\beta_{1j} = \\gamma_{10}\\)\nThese two formulas are the second level of the model, with \\(\\alpha_j\\) explaining the \\(j\\)th group’s deviation from the global intercept, when there are varying intercepts, with \\(\\gamma_{00}\\) representing the global_intercept, and \\(\\mu_{0j}\\) representing the deviance from the mean intercept, and \\(\\beta_{1j}\\) explaining the unit-level variance of \\(x\\), with \\(\\gamma_{10}\\) representing the population-effect for the slope of \\(x\\)."
  },
  {
    "objectID": "blog/2022-11-01-multilevel-model-r-cheatsheet/index.html#varying-slopes",
    "href": "blog/2022-11-01-multilevel-model-r-cheatsheet/index.html#varying-slopes",
    "title": "A Cheatsheet for Building Multilevel Models in R",
    "section": "Varying Slopes",
    "text": "Varying Slopes\nSometimes there will be an expectation that the grouping structure has no effect on the intercept, but that the slope will vary between groups (though this kind of model is generally less common). The following model structure will do the job, in those instances:\n\ny ~ x + (0 + x|g)\n\nThe default structure in multilevel modelling is to include a varying intercept, which is represented by the 1 that sits inside the brackets in the code, (1|g). If you don’t want to include a varying intercept, then a 0 is required to override this default.\nWhile possibly less common than a varying intercepts model, I think it is also useful to understand how a varying slopes model is structured, in order to get a better understanding of how MLMs work both conceptually and technically.\n\n\nNotation\n\nIf the intercepts are fixed and the slopes vary, then the formula would be slightly different to the one presented earlier:\n\\[y_{ij} = \\alpha_j + \\beta_{1j}x_i + \\epsilon_{ij}\\]\nThe differences lie in the second level formulas:\n\\(\\alpha_j\\) would be written as:\n\\[\\alpha_j = \\gamma_{00}\\]\nWhile the formulas for \\(\\beta_{1j}\\) would be written as:\n\\[\\beta_{1j} = \\gamma_{10} + \\mu_{1j}\\]\nThe difference here is that the \\(\\alpha_j\\) no longer has a \\(\\mu_{0j}\\) term, because there is no group-level deviation around the global intercept, while \\(\\beta_{1j}\\) has a \\(\\mu_{1j}\\) term, representing the \\(j\\)th group’s deviation from the global mean of the slope."
  },
  {
    "objectID": "blog/2022-11-01-multilevel-model-r-cheatsheet/index.html#varying-intercepts-slopes",
    "href": "blog/2022-11-01-multilevel-model-r-cheatsheet/index.html#varying-intercepts-slopes",
    "title": "A Cheatsheet for Building Multilevel Models in R",
    "section": "Varying Intercepts & Slopes",
    "text": "Varying Intercepts & Slopes\nAnd once you’ve figured out how to build a model that varies either the intercepts or the slopes, you can then let it all hang out and just let the groups do whatever the hell they want, in the form of both varying intercepts AND slopes.\nThe syntax for this kind of structured madness is as follows:\n\n# correlated intercepts and slopes\ny ~ x + (1 + x|g)\n\n# uncorrelated intercepts and slopes\ny ~ x + (x||g)\n\nI think the previous section, detailing the structure and syntax of a varying slopes model, really helps figure things out when presented in conjunction with this model structure. The incremental changes that lead to different types of models should help to identify which components do what.\n\n\nNotation\n\nAs you might expect, the formula for a multilevel model with both varying intercepts and slopes will look like a combination of the previous two examples:\n\\[y_{ij} = \\alpha_{j} + \\beta_{1j}x_i + \\epsilon_{ij}\\]\nwhere \\(\\alpha_{j}\\) (the intercept) can be expanded as:\n\\[\\alpha_{j} = \\gamma_{00} + \\mu_{0j}\\]\nand where \\(\\beta_{1j}\\) (the slope) can be expanded as:\n\\[\\beta_{1j} = \\gamma_{10} + \\mu_{1j}\\]\nHopefully by now the multilevel model structure is starting to make some sense. The distinction between the previous models and this model formula, that makes it a varying intercepts and slopes model, is the fact that there is a term for the variance from the global mean in both the second-level formula for the intercept (\\(\\mu_{0j}\\)) and the slope ((\\(\\mu_{1j}\\)))."
  },
  {
    "objectID": "blog/2022-11-01-multilevel-model-r-cheatsheet/index.html#footnotes",
    "href": "blog/2022-11-01-multilevel-model-r-cheatsheet/index.html#footnotes",
    "title": "A Cheatsheet for Building Multilevel Models in R",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThere are some minor distinctions, but the terms are often used interchangeably.↩︎\nDoes a blog post get to call itself a cheatsheet?↩︎\nThere are alternative ways to handle clustered data, such as cluster-robust standard errors, but I won’t cover that here. If you’re interested in learning about these methods, I’d recommend the discussion in Chapter 3 of Introduction to Multilevel Modelling, or this overview that is linked in the discussion.↩︎\nA linear mixed model can refer to any model that contains a group/clustering structure, and it does not have to be a group that is organised hierarchically, or nested.↩︎\nHierarchical/multilevel models suggest that there might be a vertical hierarchy that explains the grouping structure in the data, for example, student test results are nested within a class or school grouping structure.↩︎\nARMM hasn’t been published (released?) yet, but it is billed as the updated and expanded second edition of Gelman & Hill’s book. It’s worth keeping an eye out for when this is released, because I’m sure it will be an excellent resource.↩︎"
  },
  {
    "objectID": "consultancy/index.html",
    "href": "consultancy/index.html",
    "title": "Consultancy ",
    "section": "",
    "text": "Consultancy \nI have experience asking scientific questions, designing research, and implementing statistical methods, with a focus on actionable meaningful insights from data. I have the technical and communication skills to work effectively at every stage of the data science life cycle, using the tools required for the project.\nIf you want to hire me, here is a short list of some of my areas of expertise:\n\nResearch design\nStatistical inference\nMachine learning\nData visualisation\nScientific communication\n\nPlease use this form for all enquiries about consultancy, collaborations, or speaking engagements.\n    \n\n\n\n\n\n\n\n\nFull Name \nEmail Address \nMessage\n\n\nSend message"
  },
  {
    "objectID": "cv/index.html",
    "href": "cv/index.html",
    "title": "Paul Johnson",
    "section": "",
    "text": "M.A. Political Science\nUniversity of Houston 2017 — 2020\n\nM.Sc. Global Politics\nUniversity of Southampton 2011 — 2012\n\nB.A. Politics\nUniversity of Sussex 2008 — 2011"
  },
  {
    "objectID": "cv/index.html#education",
    "href": "cv/index.html#education",
    "title": "Paul Johnson",
    "section": "",
    "text": "M.A. Political Science\nUniversity of Houston 2017 — 2020\n\nM.Sc. Global Politics\nUniversity of Southampton 2011 — 2012\n\nB.A. Politics\nUniversity of Sussex 2008 — 2011"
  },
  {
    "objectID": "cv/index.html#selected-work-experience",
    "href": "cv/index.html#selected-work-experience",
    "title": "Paul Johnson",
    "section": "Selected Work Experience ",
    "text": "Selected Work Experience \n\nPrincipal Data Scientist\nNHS South, Central and West CSU Nov 2021 — Present\n\nProviding technical expertise in analytics and research-focused projects across the organization, including experimental and observational analysis of healthcare interventions.\nBuilding and deploying machine learning models that forecast the daily event counts in A&E and other Urgent and Emergency Care Services.\nLeading the development of organisational expertise in data science,including demonstrating and training colleagues in statistics,machine learning, and programming.\nSupporting the development of the organisation’s infrastructure for developing and deploying data science.\n\n\n\nAssociate BI Consultant\nNHS South, Central and West CSU April 2021 — Nov 2021\n\nLed research on the wider determinants of health inequalities and the role of deprivation in health outcomes.\nSupported the Population Health Management team with research design, statistical modelling, and communication and visualization of findings.\n\n\n\nGraduate Research/Teaching Assistant\nUniversity of Houston July 2017 — Dec 2020\n\nBuilt natural language processing models for research into political language in legal texts.\nSupported the Political Party Database Project in the administering of the expert survey and maintenance of the project website.\nCourses: Intro to Comparative Politics, Mexican Politics, Comparative Elections, Media & Politics.\n\n\n\nFundraising Manager: Product Innovation\nMarie Curie Aug 2016 — July 2017\n\nManaged Marie Curie’s biggest regular fundraising campaign (Fundraising My Way).\nAssisted the development of new fundraising products and the innovation of existing campaigns.\n\n\n\nEvents & Corporate Fundraiser\nWorld Child Cancer July 2015 — April 2016\n\nDeveloped, recruited, and managed the charity’s largest charity event to-date (Kilimanjaro trek) and various major events (Ride 100, Great North Run).\nSupported the Head of Corporate Fundraising in the successful corporate partnership with Deutsche Bank."
  },
  {
    "objectID": "cv/index.html#further-work-experience",
    "href": "cv/index.html#further-work-experience",
    "title": "Paul Johnson",
    "section": "Further Work Experience ",
    "text": "Further Work Experience \n\nVarious Administrative Roles\nCharity Sector & NHS\n\nFamily for Every Child – Events Coordinator\nUniversity Hospital Southampton – Oncology Outpatient Clerk\nUniversity Hospital Southampton – Income Management Assistant"
  },
  {
    "objectID": "license.html",
    "href": "license.html",
    "title": "License",
    "section": "",
    "text": "The blog posts on this website are all released under a Creative Commons Attribution-ShareAlike 4.0 International License. Please feel free to use and share anything you find valuable in these posts, but please cite me too!"
  }
]