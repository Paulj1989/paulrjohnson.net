{
  "hash": "db139c0a718a31a6dcead4a4fd3c4a2d",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Combining Tidymodels & Caret for Machine Learning in R\nsubtitle: |\n  Can you use **tidymodels** and **caret** together? Yes. But should you? Unclear.\ndescription-meta: |\n  Investigating the different ways {tidymodels} and {caret} can be combined when\n  building machine learning models in R and whether bringing the two frameworks\n  together is worthwhile.\ndate: 2022-05-07\nimage: robot.webp\nimage-alt: A light blue toy robot.\ncategories: [Machine Learning, Caret, Tidymodels, R]\n---\n\n\n\nThe two main approaches to building machine learning models in R are [**caret**](https://github.com/topepo/caret/) and [**tidymodels**](https://www.tidymodels.org/). Having tried both, I found that I struggled to pick my favorite. There's elements of both that made more intuitive sense to me than the other. I think it's a product of having become very familiar with the **tidyverse**, particularly **dplyr**, for data wrangling, but still using a lot of Base R functions for statistical modeling.\n\nThe process for prepping the data for a machine learning model seems to make a ton of sense to me when done in **tidymodels** (using [**recipes**](https://recipes.tidymodels.org/) and [**rsample**](https://rsample.tidymodels.org/)), but the equivalent process using **caret** felt a little clunky. However, specifying and training models using **caret** made a lot of sense to my broken brain.\n\nAnyway, I recently discovered something that is probably entirely unremarkable to everyone else, and that probably shouldn't have taken me by surprise... You can just combine the two! You can split and preprocess your data using the **tidymodels** framework before defecting to **caret** for the next steps. What a time to be alive.\n\n## Predicting Diabetes Using Random Forest\n\nBecause I'm not a savage, I won't leave you without a simple worked example. We'll use [Gary Hutson](https://hutsons-hacks.info/)'s really useful [**MLDataR**](https://cran.r-project.org/web/packages/MLDataR/vignettes/MLDataR.html) package to grab a toy diabetes dataset, cleaning the variable names using [**janitor**](http://sfirke.github.io/janitor/), and converting the target variable, *diabetic_class*, to a factor.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# import packages\nsuppressPackageStartupMessages({\n  library(dplyr)\n  library(caret)\n  library(recipes)\n  library(randomForest)\n})\n\n# load data\ndiabetes_raw <- MLDataR::diabetes_data\n\n# clean data\ndf <-\n  diabetes_raw |>\n  janitor::clean_names() |>\n  mutate(diabetic_class = as.factor(diabetic_class))\n```\n:::\n\n\n\n### Train/Test Splits\n\nHaving done this, we can use **rsample** to split the data into a train and test set.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# set random seed\nset.seed(456)\n\n# split train/test data\ntrain_test_split <-\n  rsample::initial_split(df, strata = diabetic_class, prop = 0.7)\n\n# create train/test sets\ntrain_df <- rsample::training(train_test_split)\ntest_df <- rsample::testing(train_test_split)\n```\n:::\n\n\n\n### Data Preprocessing\n\nThe next step is a little more involved, and is where I think **tidymodels** really excels. Using the **recipes** package, we can specify all the preprocessing steps needed for the dataset, such that the data will then be ready for training a machine learning model.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# preprocessing\nmodel_recipe <-\n  recipe(diabetic_class ~ ., data = train_df) |>\n  # combine low frequency factor levels\n  step_other(all_nominal(), threshold = 0.05) |>\n  # remove predictors with zero variance\n  step_nzv(all_predictors()) |>\n  # normalize numeric variables (sigma = 1, mu = 0)\n  step_normalize(all_numeric()) |>\n  # convert nominal variables to numeric binary variables\n  step_dummy(all_nominal(), -all_outcomes(), one_hot = TRUE)\n```\n:::\n\n\n\nYou can check that all the preprocessing steps are working as expected by using *prep()* and *juice()*.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# check preprocessing results\nmodel_recipe |>\n  prep() |>\n  juice() |>\n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 364\nColumns: 32\n$ age                    <dbl> -0.67696788, -1.18732268, -1.52755921, 1.619628…\n$ diabetic_class         <fct> Negative, Negative, Negative, Negative, Negativ…\n$ gender_Female          <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ gender_Male            <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ excess_urination_No    <dbl> 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ excess_urination_Yes   <dbl> 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ polydipsia_No          <dbl> 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,…\n$ polydipsia_Yes         <dbl> 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,…\n$ weight_loss_sudden_No  <dbl> 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ weight_loss_sudden_Yes <dbl> 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ fatigue_No             <dbl> 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,…\n$ fatigue_Yes            <dbl> 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,…\n$ polyphagia_No          <dbl> 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,…\n$ polyphagia_Yes         <dbl> 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,…\n$ genital_thrush_No      <dbl> 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,…\n$ genital_thrush_Yes     <dbl> 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,…\n$ blurred_vision_No      <dbl> 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,…\n$ blurred_vision_Yes     <dbl> 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,…\n$ itching_No             <dbl> 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,…\n$ itching_Yes            <dbl> 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,…\n$ irritability_No        <dbl> 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,…\n$ irritability_Yes       <dbl> 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,…\n$ delay_healing_No       <dbl> 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,…\n$ delay_healing_Yes      <dbl> 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,…\n$ partial_psoriasis_No   <dbl> 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,…\n$ partial_psoriasis_Yes  <dbl> 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,…\n$ muscle_stiffness_No    <dbl> 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,…\n$ muscle_stiffness_Yes   <dbl> 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,…\n$ alopecia_No            <dbl> 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,…\n$ alopecia_Yes           <dbl> 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,…\n$ obesity_No             <dbl> 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,…\n$ obesity_Yes            <dbl> 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,…\n```\n\n\n:::\n:::\n\n\n\nIf everything looks alright, you can take the *model_recipe* object that you've created and use it as the model formula that you would otherwise have to specify in the **caret** *train()* function.\n\n### Model Training\n\nFor the rest of the process, you can switch over to **caret**, first using the *trainControl()* function to specify the training parameters and then the *train()* function for the model training.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# set random seed\nset.seed(456)\n\n# control parameters for model training\nctrl <-\n  trainControl(\n    method = \"cv\",\n    number = 5,\n    classProbs = TRUE,\n    summaryFunction = twoClassSummary\n  )\n\n# train random forest model\nrf_mod <-\n  train(\n    model_recipe,\n    data = train_df,\n    method = \"rf\",\n    tunelength = 10,\n    metric = \"ROC\",\n    trControl = ctrl,\n    importance = TRUE\n  )\n```\n:::\n\n\n\nHaving trained the random forest model, you can check the performance, and see what parameters were chosen in the tuning process.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# check results\nprint(rf_mod)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRandom Forest \n\n364 samples\n 16 predictor\n  2 classes: 'Negative', 'Positive' \n\nRecipe steps: other, nzv, normalize, dummy \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 292, 291, 291, 291, 291 \nResampling results across tuning parameters:\n\n  mtry  ROC        Sens       Spec     \n   2    0.9947186  0.9500000  0.9372727\n  16    0.9865188  0.9428571  0.9326263\n  31    0.9839791  0.9428571  0.9235354\n\nROC was used to select the optimal model using the largest value.\nThe final value used for the model was mtry = 2.\n```\n\n\n:::\n:::\n\n\n\nNot bad! The best performing model has an ROC of 0.995 and both the sensitivity and specificity are ~0.95. Pretty solid for a quick and easy model.\n\n### Test Predictions\n\nTo really test the model's performance, we want to see how it copes with the test data that it hasn't seen.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# make predictions on test data\nrf_predict <- predict(rf_mod, newdata = test_df, type = \"prob\")\nrf_class <- predict(rf_mod, newdata = test_df, type = \"raw\")\n\npreds <-\n  cbind(rf_predict, rf_class) |>\n  mutate(\n    Positive = round(Positive, digits = 2),\n    Negative = round(Negative, digits = 2)\n  )\n```\n:::\n\n\n\nFinally, we can produce a confidence matrix for a more intuitive look at how the model is performing on the test set.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncm_class <- test_df[, names(test_df) %in% c(\"diabetic_class\")]\n\nconfusionMatrix(\n  rf_class,\n  as.factor(cm_class$diabetic_class),\n  positive = \"Positive\"\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nConfusion Matrix and Statistics\n\n          Reference\nPrediction Negative Positive\n  Negative       57        1\n  Positive        3       95\n                                         \n               Accuracy : 0.9744         \n                 95% CI : (0.9357, 0.993)\n    No Information Rate : 0.6154         \n    P-Value [Acc > NIR] : <2e-16         \n                                         \n                  Kappa : 0.9455         \n                                         \n Mcnemar's Test P-Value : 0.6171         \n                                         \n            Sensitivity : 0.9896         \n            Specificity : 0.9500         \n         Pos Pred Value : 0.9694         \n         Neg Pred Value : 0.9828         \n             Prevalence : 0.6154         \n         Detection Rate : 0.6090         \n   Detection Prevalence : 0.6282         \n      Balanced Accuracy : 0.9698         \n                                         \n       'Positive' Class : Positive       \n                                         \n```\n\n\n:::\n:::\n\n\n\nThe results are pretty good for a very quick model. How exciting. Lets pretend that it's because I'm a brilliant data scientist rather than it being due to the very clean, balanced toy dataset we used.\n\n## Wrapping Up\n\nSo there you have it, if you're in the same position as me and you're struggling to pick between **tidymodels** and **caret**, because both frameworks offer something you like, you can just combine the two and make Frankenstein's framework.\n\nUltimately, despite this blog post, I'm probably going to stick with **tidymodels** (why am I like this?). I think that I'm going to force myself to get used to the **tidymodels** framework end-to-end because a) it is receiving tons of development so it's probably going to continue to get better and bigger, and will be leading the way for the foreseeable future, and b) because in reality I think the explicit way that you structure each step is probably sensible, even if it confuses me a bit.\n\nBut it's nice to know that I've got options.\n\n## Acknowledgments {.appendix}\n\nPreview image by [Rock'n Roll Monkey](https://unsplash.com/@rocknrollmonkey) on [Unsplash](https://unsplash.com/photos/blue-plastic-robot-toy-R4WCbazrD1g).\n\n## Support {.appendix}\n\nIf you enjoyed this blog post and would like to support my work, you can [buy me a coffee or a beer or give me a tip](https://www.buymeacoffee.com/paulj1989) as a thank you.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}