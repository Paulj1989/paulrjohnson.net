{
  "hash": "e2b53cc11a6e692e0646d9fa8f05d43a",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Missing Data Matters (Part II) - Solutions\nsubtitle: |\n  Dealing with your missing data is like eating your greens. You only do it so \n  you can judge others that don't.\ndescription-meta: |\n  Exploring the available methods for handling different missing data \n  mechanisms, including listwise deletion, mean imputation, regression \n  imputation, and multiple imputation.\nauthor: Paul Johnson\ndate: 2024-03-26\nimage: missing_data.webp\nimage-alt: |\n  An AI generated image of a small boy's silhouette, stood in a field facing a \n  large sign that appears to say Missing Data (but is distorted as is typically \n  the case with text produced by generative models).\ncategories: [Missing Data, Imputation, Statistics, R]\nbibliography: references.bib\n---\n\n\n::: {.callout-note}\n\nThis is the second post in the Missing Data Matters series. The first post, about missing data mechanisms, can be found [here](/blog/missing-data-mechanisms).\n\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Setup Code (Click to Expand)\"}\n# import packages\nsuppressPackageStartupMessages({\n  library(dplyr)\n  library(ggplot2)\n})\n\n# helper function for transforming factor variables to title case\nstr_titlecase_fct <-\n  function(x, ...) {\n    levels(x) <-\n      stringr::str_to_title(stringr::str_replace(levels(x), ...))\n    x\n  }\n\n# import data\nattrition <-\n  modeldata::attrition |>\n  as_tibble() |>\n  janitor::clean_names() |>\n  select(\n    starts_with(\"job_\"), ends_with(\"_satisfaction\"), \n    age, attrition, education, monthly_income, \n    total_working_years, num_companies_worked\n    ) |> \n  mutate(\n    across(\n      where(is.factor), \n      ~ str_titlecase_fct(.x, \"_\", \" \"))\n  )\n\n# setup fonts\nsysfonts::font_add_google(\"Poppins\")\nsysfonts::font_add_google(\"Lora\")\nshowtext::showtext_auto()\n\n# set plot theme\n# inspired by https://github.com/z3tt/TidyTuesday/blob/main/R/2020_31_PalmerPenguins.Rmd\ntheme_set(theme_minimal(base_size = 20, base_family = \"Poppins\")) +\n  theme_update(\n    panel.grid.major = element_line(color = \"grey90\", linewidth = .4),\n    panel.grid.minor = element_blank(),\n    axis.title.x = element_text(\n      color = \"grey30\", margin = margin(t = 5), size = rel(1.05)\n      ),\n    axis.title.y = element_text(\n      color = \"grey30\", margin = margin(r = 5), size = rel(1.05)\n      ),\n    axis.text = element_text(color = \"grey50\", size = rel(1)),\n    axis.ticks = element_line(color = \"grey90\", linewidth = .4),\n    axis.ticks.length = unit(.2, \"lines\"),\n    legend.position = \"top\",\n    legend.title = element_blank(),\n    legend.text = element_text(size = rel(1)),\n    legend.box.margin = margin(0, 0, -15, 0),\n    plot.title = element_text(\n      hjust = 0, color = \"black\", family = \"Lora\",\n      size = rel(1.5), margin = margin(t = 5, b = 5)\n    ),\n    plot.subtitle = element_text(\n      hjust = 0, color = \"grey30\", family = \"Lora\",\n      lineheight = 0.5, size = rel(1.1), \n      margin = margin(5, 0, 5, 0)\n    ),\n    plot.title.position = \"plot\",\n    plot.caption = element_text(\n      color = \"grey50\", size = rel(0.8), hjust = 1,\n      margin = margin(10, 0, 0, 0)\n    ),\n    plot.caption.position = \"plot\",\n    plot.margin = margin(rep(10, 4)),\n    strip.text = element_text(size = rel(1), margin = margin(0, 0, 5, 0))\n  )\n```\n:::\n\n\nIn the opening post in the Missing Data Matters series^[I keep calling it a series because I couldn't think of another way to refer to it, but calling two posts a series feels grandiose. Am I forgetting a better word for this?], I looked at the mechanisms that cause missing data - missing completely at random (MCAR), missing at random (MAR), and missing not at random (MNAR) - and demonstrated some of the options available for diagnosing missing data mechanisms. This post will cover the next step in the missing data workflow - what the hell are you supposed to do now, having realised that your data is complete trash?\n\nI will use the same [employee attrition](https://modeldata.tidymodels.org/reference/attrition.html) dataset taken from the [**modeldata**](https://modeldata.tidymodels.org/) package and created by the IBM Watson Analytics Lab and will continue to add missingness to this data with reckless abandon. I will talk through the range of options available to you when faced with missing data, including both deletion and imputation methods, and I will also discuss how the missing data mechanism you are dealing with impacts the appropriateness of the available solutions. Finally, I will demonstrate how some of these methods work and compare their performance using a series of logistic regressions.\n\n## Have You Considered Not Having Missing Data?\n\nThe maximally optimal approach to missing data is to not have any, so if you have missing data, the best thing you can do is stop having it. The only way to do that is to go and find it! This may seem glib, but it's important to highlight this point before looking at the other options. All of the strategies that are detailed below have drawbacks. If you can find your missing values, this is a much better choice than trying to model some solution. This may involve further data collection, identifying bugs/issues in your data processing pipeline, or theory-driven inference[^inference].\n\nHowever, when this is not possible, there are two broad approaches to dealing with missing values. The first approach is to delete missing values, and the second is to replace them. The right approach is highly dependent on the nature of the missing data. Generally speaking, data that is MCAR is the most flexible, and simple methods like deletion can work without problems, but more complex methods like imputation are necessary when data is MAR. Finally, when dealing with MNAR data, it is required to model the missingness mechanism explicitly [@gelman2006]. \n\n[^inference]:\n    \n    If you know the data well and can identify precisely why the data is missing, you may also be able \n    to plug in the missing values. However, you should be pretty sure when doing this because otherwise, \n    you have personally taken the time to add some error to your data. This is bad for all but the most \n    graft-thirsty High Performance Podcast listening maniacs. If you would take the [$500k over dinner \n    with Jay-Z](https://i.kym-cdn.com/entries/icons/original/000/038/686/jayz.jpg), you should make sure \n    your theory is sound!\n\n### A Quick Note on MNAR Data\n\nAlthough we may touch on MNAR data in passing in this second post, the focus will be on MCAR and MAR data. The process for modelling MNAR data is complicated, and I am a massive coward. Cramming the suite of potential MNAR solutions into an already complex post would not do them justice. \n\nHowever, as a quick summary, there are two main approaches to modelling MNAR data - selection modelling and pattern mixture modelling. Selection models treat missingness as the outcome in a regression, similar to a more precise version of the logistic regression fit in the previous post, while pattern mixture models treat the missingness as an explanatory variable.\n\nFor more information about MNAR modelling processes, I'd recommend @enders2022's _Applied Missing Data Analysis_. \n\n### And Another on Likelihood Methods for Good Measure\n\nOne group of methods I haven't discussed here is likelihood methods, such as full information maximum likelihood (FIML). In simple terms, likelihood methods borrow information from complete cases to improve the estimation of parameters with missing data [@peters2002]. Maximum likelihood uses the available information to estimate a log-likelihood function most likely to have produced the observed data [@enders2022]. In particular, FIML considers only complete cases when calculating the log-likelihood function and maximises it using an expectation-maximisation algorithm [@xu2022]. \n\nHowever, implementing methods like FIML in R is a little tricky^[Or is at least sufficiently distinct from the implementation of the methods discussed below that it would be significant work to include likelihood methods in this post.], so these methods are out of scope for my rubbish little blog post. If you want to learn more about the implementation of FIML in R, I'd recommend the [\"Full Information Maximum Likelihood\"](https://www.statistical-thinking.com/fiml) section of William Murrah's _Statistical Thinking_ site and there the [**lavaan** package documentation](https://lavaan.ugent.be/) can also help figure out how to compute FIML to deal with missing data. If I am feeling like a total mad lad later on, I may write another post about implementing FIML.\n\n## Deletion Methods\n\nThe most straightforward and most common approach to handling missing data is to delete it. There are two different approaches to deleting missing values: listwise and pairwise deletion^[As I type this, it occurs to me that there are probably more than two methods for deleting missing values. Maybe someone out there advocates deleting the whole dataset and smashing your laptop with a hammer whenever you encounter NAs.].\n\nListwise deletion, sometimes called complete case analysis, is the most common. It removes any rows containing missing values for relevant variables. This means that the analysis is carried out on all complete cases. This is the default approach that almost every statistical tool in any language uses. It is the mechanism by which the plots in [Part I](/blog/missing-data-mechanisms) of the series are visualising the different types of missing data^[**ggplot2** removes NAs by default, so the plots in the previous post are working by creating missing values and letting **ggplot2** do the rest.]. When you choose not to resolve the missing data in your dataset, you are passing the buck and letting whatever statistical tool you are using do it for you, and they are usually carrying out listwise deletion. \n\nPairwise deletion, or available case analysis, is less common. It involves removing any missing values but not the entire row. Instead, the available observations for each variable are used to calculate their means and covariances, which can be used to build statistical models [@vanbuuren2018]. This process is a little more involved and cannot be applied to as many use cases, so you should see pairwise deletion far less often in the wild.\n\nDeletion-based methods can be robust under the right conditions. When data is MCAR and volumes of missing data are not vast, deletion methods may be suitable. However, things can start to unravel when data is MAR, and deletion methods will likely cause many problems when data is MNAR. Deleting values when data is MNAR (and to a slightly lesser extent MAR) creates selection bias. The sample that remains after deletion is a function of the missing data mechanism, which cannot be accounted for in a model if the data has been deleted. Nonetheless, deletion methods prevail because they are the default approach.\n\n## Imputation Methods\n\nIf we are not deleting missing values, we have to replace them. Imputation involves using a statistical procedure to replace missing values with values estimated from the rest of the data.\n\nImputation methods can be divided into single imputation and multiple imputation. Single imputation replaces missing values with a single value, while multiple imputation creates $m$ datasets, each replacing missing values with plausible estimated values and pooling estimates from analyses carried out on each dataset. \n\nImputation is appropriate under the starting assumption that the data is MAR. It is also suitable when dealing with MCAR data, but deletion methods are usually most appropriate unless dealing with large volumes of missingness. \n\nIn addition to **mice**'s exploratory capabilities, it is also the go-to option for carrying out imputation in R^[For anyone that prefers to use Python, [**statsmodels**](https://www.statsmodels.org/stable/imputation.html) also has the functionality for multiple imputation by chained equations, while [**fancyimpute**](https://github.com/iskandr/fancyimpute) and [**sklearn.impute**](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.impute) offer further imputation functionality.]. While I will briefly overview these methods, there is much more to imputation than could be covered here. To learn just about all there is to know about imputation, @vanbuuren2018's [_Flexible Imputation of Missing Data_](https://stefvanbuuren.name/fimd/) will have you covered.\n\n### Single Imputation\n\nThere are a few different forms of single imputation, the most common being simple imputation methods that replace all missing values with the variable's average value (usually the mean, but median and mode can be used too) or replace missing values using randomly drawn values from the same variable, otherwise known as simple hot-deck imputation [@xu2022]. Simple imputation methods are almost always harmful, especially methods like mean imputation. They distort the variable's distribution, underestimating its variance [@vanbuuren2018], disrupting the relationship between the variable with imputed values and all other variables [@nguyen2020], and biasing model estimates [@vanbuuren2018]. It is important to know that these simple imputation methods exist, but the circumstances where using them is valid are very few!\n\nAnother approach is to estimate a regression model that predicts missing values. Regression imputation could take the form of any regression model, depending on the data type and complexity, with linear and logistic regression being the most common. However, @vanbuuren2018 argues that regression imputation is the \"most dangerous\" method because it gives false confidence in outputs.\n\nWe can look at the effects of imputation by visualising imputation methods. I have transformed the total working years value for 75% of employees with high or very high relationship satisfaction to NA. Obviously, this is a very high proportion of missing values. If 75% of your data is missing, you should be deeply alarmed, stop whatever you are doing, and bin that dataset. However, in this case, it is meant to illustrate how imputation methods work, and having a disproportionately large number of missing values makes it easier to see the effects visually.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Data Code (Click to Expand)\"}\nset.seed(123)\n\nmissing_years <-\n  attrition |> \n  mutate(\n    total_working_years = replace(\n      total_working_years, runif(n()) < 0.75 & \n        relationship_satisfaction %in% c(\"High\", \"Very High\") , NA\n    ),\n    missingness = ifelse(is.na(total_working_years), \"Imputed\", \"Observed\")\n  )\n```\n:::\n\n\n::: {.panel-tabset}\n\n#### Original Data\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Plot Code (Click to Expand)\"}\nplot_regression <-\n  function(data) {\n    if (\"missingness\" %in% names(data)) {\n      data |>\n        ggplot(aes(x = total_working_years, y = monthly_income)) +\n        geom_point(aes(colour = missingness), size = 1.5, alpha = .6) +\n        geom_smooth(method = lm, colour = \"#026E99\", fill = \"#026E99\", alpha = .6) +\n        scale_colour_manual(values = c(\"#D93649\", \"#343A40\")) + \n        scale_y_continuous(labels = scales::label_currency()) +\n        guides(colour = guide_legend(override.aes = list(size = 3))) +\n        labs(\n          x = \"Total Working Years\", y = \"Monthly Income\",\n          caption = \"Visualisation: Paul Johnson  |  Data: IBM Watson Analytics Lab\"\n        )\n    } else {\n      data |>\n        ggplot(aes(x = total_working_years, y = monthly_income)) +\n        geom_point(colour = \"#343A40\", size = 1.5, alpha = .6) +\n        geom_smooth(\n          method = lm, colour = \"#026E99\",\n          fill = \"#026E99\", alpha = .6\n        ) +\n        scale_y_continuous(labels = scales::label_currency()) +\n        labs(\n          x = \"Total Working Years\", y = \"Monthly Income\",\n          caption = \"Visualisation: Paul Johnson  |  Data: IBM Watson Analytics Lab\"\n        )\n    }\n  }\n\nattrition |>\n  plot_regression() +\n  labs(\n    title = \"No Missing Data & No Imputation\",\n    subtitle = \n      stringr::str_wrap(\n        glue::glue(\n          \"Employee's monthly income by total length of their career, without \",\n          \"missing values and without imputation.\"\n          ),\n        width = 90\n      )\n    )\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/original-data-1.png){fig-alt='A scatterplot visualising the association between total working years and\nmonthly income, without any missing data and therefore no imputation\ncarried out. The plot shows a strong positive association between career\nlength and monthly income.\n' width=672}\n:::\n:::\n\n\n#### Simple Imputation\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Plot Code (Click to Expand)\"}\nmissing_years |>\n  mice::mice(\n    method = \"mean\", m = 1,\n    maxit = 1, print = FALSE\n  ) |>\n  mice::complete() |>\n  plot_regression() +\n  labs(\n    title = \"Mean Imputation\",\n    subtitle = \n      stringr::str_wrap(\n        glue::glue(\n          \"Employee's monthly income by total length of their career, with \",\n          \"missing values replaced using mean imputation.\"\n        ),\n        width = 90\n      )\n    )\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/mean-imputation-1.png){fig-alt='A scatterplot visualising the association between total working years and\nmonthly income, with missing data resolved using mean imputation. The plot\nshows a strong positive association between career length and monthly\nincome, with a large number of points imputed at the mean value of career\nlength, just over 10 years.\n' width=672}\n:::\n:::\n\n\n#### Regression Imputation\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Plot Code (Click to Expand)\"}\nmissing_years |> \n  mice::mice(\n    method = \"norm.predict\", m = 1,\n    maxit = 1, print = FALSE\n  ) |>\n  mice::complete() |>\n  plot_regression() +\n  labs(\n    title = \"Linear Regression Imputation\",\n    subtitle = \n      stringr::str_wrap(\n        glue::glue(\n          \"Employee's monthly income by total length of their career, with \",\n          \"missing values replaced using regression imputation with linear \",\n          \"regression.\"\n        ),\n        width = 90\n      )\n    )\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/regression-imputation-1.png){fig-alt='A scatterplot visualising the association between total working years and\nmonthly income, with missing data resolved using linear regression\nimputation. The plot shows a strong positive association between career\nlength and monthly income, but imputed values appear to follow a less\nstructured pattern than the observed values.\n' width=672}\n:::\n:::\n\n\n:::\n\nImputing missing values using simple mean imputation is quick and easy, but the results clearly leave much to be desired. However, estimates can sometimes be useful even if they are imprecise. That's not the case here either because mean imputation narrows the distribution of total working years. The confidence intervals around the regression line are narrower around the mean but significantly wider at the higher end of total working years. \n\nThe regression imputation is okay at first inspection, but it imputes values of total working years to six decimal points while the actual values are all integers. The imputation also thinks that some people have worked negative years! These issues can be fixed by giving **mice** the necessary details to impute more realistic values. However, the razor-thin error bars, particularly at the lower values of total working years, cannot be fixed using single imputation.\n\nThis points to the inherent flaw with single imputation that none of these methods can escape. In @rubin1987's words:\n\n> Imputing one value for a missing datum cannot be correct in general, because we don't know what value to impute with certainty (if we did, it wouldn't be missing).\n\nWe cannot account for the uncertainty in imputing missing values using single imputation because we are plugging in a single value in place of the missing value and treating it with the same confidence as the observed values. There are ways to add more uncertainty to single imputation methods, such as stochastic regression imputation, but ultimately, these are papering over the cracks. Single imputation always offers a false sense of certainty because it imputes a single value in place of each missing value.\n\n### Multiple Imputation {#sec-multiple-imputation}\n\nMultiple imputation attempts to address the fundamental flaw in single imputation methods, acknowledging the uncertainty in imputed values and baking that uncertainty into the process. Multiple imputation involves generating multiple datasets, performing analysis on each, and pooling the results. The multiple imputation process yields parameter estimates that \"average over a number of plausible replacement values\" [@enders2022, p.189] instead of the one that single imputation does. Multiple imputation can be appropriate for missing data that is not MAR, as long as the response mechanism has been explicitly modelled or other identifying assumptions have been made [@murray2018].\n\n@enders2022 splits the multiple imputation process into three phases - imputation, analysis, and pooling.\n\n1. Imputation Phase - generates multiple, $m$, complete datasets, estimating the missing values plus a random component to capture the uncertainty in the estimate,\n2. Analysis Phase - computes parameter estimates using typical statistical methods, but computed $m$ times and applied to the $m$ completed datasets, \n3. Pooling Phase - combines these estimates[^pooling] and calculates the pooled estimates and their standard errors.\n\nWhile there are multiple ways to approach each phase, the three phases are typical across multiple imputation.\n\nHere, we will use multiple imputation by chained equations (MICE). MICE imputes one variable at a time, using a separate model for each incomplete variable, a process called Fully Conditional Specification. Plausible values are generated using the rest of the data columns (using various methods depending on the variable) in an iterative process that uses Gibbs sampling [@vanbuuren2011].\n\nEffective multiple imputation requires as much information as possible, including variables associated with the missingness itself and with the observed values of that variable (which are, therefore, likely to be associated with the missing values). Thus, @murray2018 argues that imputation models should include as many variables as possible.\n\nThe plot below shows the results of the imputation phase of the multiple imputation process, with the pooling carried out manually, but it is important to note that the results are not a perfect implementation of multiple imputation. Instead, each of the five completed datasets has been stacked together, and the mean value of each row^[The mean value has been rounded to the integer value so as to not mask what predictive mean matching is doing.] across those five datasets has been calculated. The results plotted below do not perfectly demonstrate what multiple imputation looks like, but they are at least an approximation.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Plot Code (Click to Expand)\"}\nmissing_years |> \n  mice::mice(method = \"pmm\", print = FALSE, seed = 123) |>\n  mice::complete(action = \"long\") |> \n  summarise(\n    across(c(total_working_years, monthly_income), ~ round(mean(.x), 0)), \n    .by = c(.id, missingness)\n    ) |> \n  plot_regression() +\n  labs(\n    title = \"Multiple Imputation with Predictive Mean Matching\",\n    subtitle =\n      stringr::str_wrap(\n        glue::glue(\n          \"Employee's monthly income by total length of their career, with missing \",\n          \"values replaced using multiple imputation with predictive mean matching.\"\n        ),\n        width = 90\n      )\n    )\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/multiple-imputation-1.png){fig-alt='A scatterplot visualising the association between total working years and\nmonthly income, with missing data resolved using multiple imputation with\npredictive mean matching. The plot shows a strong positive association\nbetween career length and monthly income. Imputed values appear to follow\na very similar pattern to observed values, and the multiple imputation plot\nlooks very similar to the plot of the original data.\n' width=672}\n:::\n:::\n\n\nThe confidence intervals don't capture the baked-in uncertainty, a significant advantage of multiple imputation. The plot only shows the final mean values calculated from the five imputed datasets. However, the multiple imputation process results follow a similar distribution to the original data, which should reduce the risk of imputations producing biased parameter estimates. Further, the imputed values appear to be structured similarly to the observed data, making the multiple imputation plot look very similar to the original data plot. This is generally a good sign because the closer you are to replicating the data-generating process, the better your chance of making accurate inferences. The structural similarities are primarily due to the method used to carry out the multiple imputation, predictive mean matching.\n\n[^pooling]:\n  \n    @rubin1987 devised formulas for pooling parameter estimates and standard errors, where the pooled \n    parameter estimate is the arithmetic mean of the individual estimates, while pooled standard errors \n    follow a similar logic but a slightly more complex process [@enders2022]. There are also methods for \n    calculating statistical significance, but things are getting much more complicated now, and this is \n    not the time or the place. I'd recommend reading \"The Analysis and Pooling Phases of Multiple \n    Imputation\" in @enders2022's _Applied Missing Data Analysis_ if you want to get in the weeds.\n\n#### Predictive Mean Matching\n\nThe method used for modelling plausible values is predictive mean matching (PMM). PMM estimates missing values by identifying candidates from the observed data with predicted values close to the missing observation's predicted value, from which a candidate is randomly selected and the observed value is plugged in the missing observation's place [@vanbuuren2018]. Predictions are typically generated using a linear regression model. PMM effectively identifies $n$ observations from the observed data closest to the predicted value of the missing data and picks one at random.\n\nThere are several advantages to using PMM. Implementation is simple; it relies on imputations that are observed elsewhere in the data (which ensures it is realistic and won't produce nonsensical imputations), and because the imputation process is implicit, it doesn't require explicit modelling of the missingness distribution and, therefore reduces the risk of misspecification [@vanbuuren2018]. A more detailed discussion of PMM can be found in the [\"Predictive Mean Matching\"](https://stefvanbuuren.name/fimd/sec-pmm.html) section of @vanbuuren2018's _Flexible Imputation of Missing Data_.\n\nWhile I've used PMM here, **mice** offers [functionality for various methods](https://amices.org/mice/reference/mice.html) for multiple imputation. In this case, the missing values are all numeric, for which **mice** uses PMM as the default imputation method. However, for binary factor variables, logistic regression is used by default, and where a factor variable has more than two levels, proportional odds regression is used when it is ordered, and polytomous logistic regression is used when it is unordered. \n\n## Comparing Model Estimates\n\nFinally, we can test these missing data methods by fitting regression models using them. We can compare the model estimates with each other and against a regression fit on the original data.\n\nI have fit a series of logistic regressions that measure the association between the outcome, job attrition, and the explanatory variables monthly income, total working years, and number of companies worked. All explanatory variables have been transformed to include some missingness. The missing data mechanism in each case is MAR. Monthly income values have been converted to NA for 75% of research scientists or sales executives educated to a Bachelor's degree or higher. For total working years, 75% of employees between 50 and 60 have been converted to NA. Finally, the number of companies an employee has worked at has been converted to NA for 25% of employees with low job satisfaction.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Data Code (Click to Expand)\"}\nset.seed(123)\n\nmissing_income <-\n  attrition |> \n  mutate(\n    monthly_income = arm::rescale(\n      replace(\n        monthly_income, runif(n()) < 0.75 & \n          (education %in% c(\"Bachelor\", \"Master\", \"Doctor\") &\n             job_role %in% c(\"Research Scientist\", \"Sales Executive\")), NA\n        )\n      ),\n    total_working_years = replace(\n      total_working_years, runif(n()) < 0.75 & \n        between(age, 50, 60), NA\n    ),\n    num_companies_worked = replace(\n      num_companies_worked, runif(n()) < 0.25 & \n        job_satisfaction %in% c(\"Low\"), NA\n    )\n  )\n```\n:::\n\n\nI have used the defaults for most imputation processes, but I have increased the number of iterations that the Gibbs sampler uses from 5 to 10 and have increased the number of complete datasets generated by the multiple imputation process from 5 to 30. While earlier literature on the subject argues that a small number of imputations (typically 3-5 total) is better, @vanbuuren2018 lays out the case for increasing the number of imputations, where computational cost is not a problem, in the [\"How Many Imputations?\"](https://stefvanbuuren.name/fimd/sec-howmany.html) section of _Flexible Imputation of Missing Data_.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Regression Code (Click to Expand)\"}\nget_pooled_estimates <-\n  function(data, method, m, maxit) {\n    if(m > 1) {\n      data |>\n        mice::mice(\n          method = method, m = m, maxit = maxit, \n          print = FALSE, seed = 123\n          ) |>\n        with(\n          glm(attrition ~ monthly_income + total_working_years +\n                num_companies_worked, family = \"binomial\")\n             ) |>\n        mice::pool()\n    } else {\n        data |>\n        mice::mice(method = method, m = m, maxit = maxit, print = FALSE) |>\n        mice::complete() |>\n        glm(attrition ~ monthly_income + total_working_years +\n                num_companies_worked, family = \"binomial\", data = _)\n    } \n  }\n\noriginal_glm <- \n  attrition |> \n  mutate(monthly_income = arm::rescale(monthly_income)) |>\n  glm(attrition ~ monthly_income + total_working_years + \n        num_companies_worked, family = \"binomial\", data = _)\n\nlistwise_deletion <-\n  missing_income |> \n  glm(attrition ~ monthly_income + total_working_years + \n        num_companies_worked, family = \"binomial\", data = _)\n\nmean_imputation <-\n  missing_income |> \n  get_pooled_estimates(method = \"mean\", m = 1, maxit = 1)\n\nregression_imputation <- \n  missing_income |>\n  get_pooled_estimates(method = \"norm.predict\", m = 1, maxit = 1)\n\nmultiple_imputation <-\n  missing_income |>\n  get_pooled_estimates(method = \"pmm\", m = 50, maxit = 10)\n```\n:::\n\n\nMore can be done to tailor and tighten up **mice**'s imputations, but the intention here is only to demonstrate how different missing data methods perform out of the box. \n\nWhile the imputations visualised in @sec-multiple-imputation are only an approximation of the process, the outputs displayed in @tbl-regression below fully demonstrate the multiple imputation method.\n\n\n::: {#tbl-regression .cell tbl-cap='Logistic Regressions Using Different Missing Data Strategies\n'}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Table Code (Click to Expand)\"}\nmodelsummary::modelsummary(\n  models = list(\n    \"Original Data\" = original_glm,\n    \"Listwise Deletion\" = listwise_deletion,\n    \"Mean\" = mean_imputation,\n    \"Regression\" = regression_imputation,\n    \"Multiple\" = multiple_imputation\n  ),\n  coef_rename = modelsummary::coef_rename,\n  exponentiate = TRUE, gof_omit = \"IC|Log|F|RMSE\", \n  output = \"gt\"\n  ) |>\n  gt::tab_spanner(label = \"Imputation\", columns = 4:6) |>\n  gt::tab_spanner(label = \"Outcome: Job Attrition\", columns = 2:6) |> \n  gt::tab_source_note(source_note = \"Source: IBM Watson Analytics Lab\") |>\n  gt::tab_options(table.width = gt::pct(100), table.font.names = \"Poppins\")\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<div id=\"osncjrvket\" style=\"padding-left:0px;padding-right:0px;padding-top:10px;padding-bottom:10px;overflow-x:auto;overflow-y:auto;width:auto;height:auto;\">\n<style>#osncjrvket table {\n  font-family: Poppins;\n  -webkit-font-smoothing: antialiased;\n  -moz-osx-font-smoothing: grayscale;\n}\n\n#osncjrvket thead, #osncjrvket tbody, #osncjrvket tfoot, #osncjrvket tr, #osncjrvket td, #osncjrvket th {\n  border-style: none;\n}\n\n#osncjrvket p {\n  margin: 0;\n  padding: 0;\n}\n\n#osncjrvket .gt_table {\n  display: table;\n  border-collapse: collapse;\n  line-height: normal;\n  margin-left: auto;\n  margin-right: auto;\n  color: #333333;\n  font-size: 16px;\n  font-weight: normal;\n  font-style: normal;\n  background-color: #FFFFFF;\n  width: 100%;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #A8A8A8;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #A8A8A8;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n}\n\n#osncjrvket .gt_caption {\n  padding-top: 4px;\n  padding-bottom: 4px;\n}\n\n#osncjrvket .gt_title {\n  color: #333333;\n  font-size: 125%;\n  font-weight: initial;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-bottom-color: #FFFFFF;\n  border-bottom-width: 0;\n}\n\n#osncjrvket .gt_subtitle {\n  color: #333333;\n  font-size: 85%;\n  font-weight: initial;\n  padding-top: 3px;\n  padding-bottom: 5px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-color: #FFFFFF;\n  border-top-width: 0;\n}\n\n#osncjrvket .gt_heading {\n  background-color: #FFFFFF;\n  text-align: center;\n  border-bottom-color: #FFFFFF;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#osncjrvket .gt_bottom_border {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#osncjrvket .gt_col_headings {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#osncjrvket .gt_col_heading {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 6px;\n  padding-left: 5px;\n  padding-right: 5px;\n  overflow-x: hidden;\n}\n\n#osncjrvket .gt_column_spanner_outer {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  padding-top: 0;\n  padding-bottom: 0;\n  padding-left: 4px;\n  padding-right: 4px;\n}\n\n#osncjrvket .gt_column_spanner_outer:first-child {\n  padding-left: 0;\n}\n\n#osncjrvket .gt_column_spanner_outer:last-child {\n  padding-right: 0;\n}\n\n#osncjrvket .gt_column_spanner {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 5px;\n  overflow-x: hidden;\n  display: inline-block;\n  width: 100%;\n}\n\n#osncjrvket .gt_spanner_row {\n  border-bottom-style: hidden;\n}\n\n#osncjrvket .gt_group_heading {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n  text-align: left;\n}\n\n#osncjrvket .gt_empty_group_heading {\n  padding: 0.5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#osncjrvket .gt_from_md > :first-child {\n  margin-top: 0;\n}\n\n#osncjrvket .gt_from_md > :last-child {\n  margin-bottom: 0;\n}\n\n#osncjrvket .gt_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  margin: 10px;\n  border-top-style: solid;\n  border-top-width: 1px;\n  border-top-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n  overflow-x: hidden;\n}\n\n#osncjrvket .gt_stub {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#osncjrvket .gt_stub_row_group {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 5px;\n  padding-right: 5px;\n  vertical-align: top;\n}\n\n#osncjrvket .gt_row_group_first td {\n  border-top-width: 2px;\n}\n\n#osncjrvket .gt_row_group_first th {\n  border-top-width: 2px;\n}\n\n#osncjrvket .gt_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#osncjrvket .gt_first_summary_row {\n  border-top-style: solid;\n  border-top-color: #D3D3D3;\n}\n\n#osncjrvket .gt_first_summary_row.thick {\n  border-top-width: 2px;\n}\n\n#osncjrvket .gt_last_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#osncjrvket .gt_grand_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#osncjrvket .gt_first_grand_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-style: double;\n  border-top-width: 6px;\n  border-top-color: #D3D3D3;\n}\n\n#osncjrvket .gt_last_grand_summary_row_top {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-bottom-style: double;\n  border-bottom-width: 6px;\n  border-bottom-color: #D3D3D3;\n}\n\n#osncjrvket .gt_striped {\n  background-color: rgba(128, 128, 128, 0.05);\n}\n\n#osncjrvket .gt_table_body {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#osncjrvket .gt_footnotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#osncjrvket .gt_footnote {\n  margin: 0px;\n  font-size: 90%;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#osncjrvket .gt_sourcenotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#osncjrvket .gt_sourcenote {\n  font-size: 90%;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#osncjrvket .gt_left {\n  text-align: left;\n}\n\n#osncjrvket .gt_center {\n  text-align: center;\n}\n\n#osncjrvket .gt_right {\n  text-align: right;\n  font-variant-numeric: tabular-nums;\n}\n\n#osncjrvket .gt_font_normal {\n  font-weight: normal;\n}\n\n#osncjrvket .gt_font_bold {\n  font-weight: bold;\n}\n\n#osncjrvket .gt_font_italic {\n  font-style: italic;\n}\n\n#osncjrvket .gt_super {\n  font-size: 65%;\n}\n\n#osncjrvket .gt_footnote_marks {\n  font-size: 75%;\n  vertical-align: 0.4em;\n  position: initial;\n}\n\n#osncjrvket .gt_asterisk {\n  font-size: 100%;\n  vertical-align: 0;\n}\n\n#osncjrvket .gt_indent_1 {\n  text-indent: 5px;\n}\n\n#osncjrvket .gt_indent_2 {\n  text-indent: 10px;\n}\n\n#osncjrvket .gt_indent_3 {\n  text-indent: 15px;\n}\n\n#osncjrvket .gt_indent_4 {\n  text-indent: 20px;\n}\n\n#osncjrvket .gt_indent_5 {\n  text-indent: 25px;\n}\n</style>\n<table class=\"gt_table\" data-quarto-disable-processing=\"false\" data-quarto-bootstrap=\"false\">\n  <thead>\n    <tr class=\"gt_col_headings gt_spanner_row\">\n      <th class=\"gt_center gt_columns_top_border gt_column_spanner_outer\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id></th>\n      <th class=\"gt_center gt_columns_top_border gt_column_spanner_outer\" rowspan=\"1\" colspan=\"5\" scope=\"colgroup\" id=\"Outcome: Job Attrition\">\n        <span class=\"gt_column_spanner\">Outcome: Job Attrition</span>\n      </th>\n    </tr>\n    <tr class=\"gt_col_headings gt_spanner_row\">\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_left\" rowspan=\"2\" colspan=\"1\" scope=\"col\" id=\" \"> </th>\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_center\" rowspan=\"2\" colspan=\"1\" scope=\"col\" id=\"Original Data\">Original Data</th>\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_center\" rowspan=\"2\" colspan=\"1\" scope=\"col\" id=\"Listwise Deletion\">Listwise Deletion</th>\n      <th class=\"gt_center gt_columns_top_border gt_column_spanner_outer\" rowspan=\"1\" colspan=\"3\" scope=\"colgroup\" id=\"Imputation\">\n        <span class=\"gt_column_spanner\">Imputation</span>\n      </th>\n    </tr>\n    <tr class=\"gt_col_headings\">\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_center\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"Mean\">Mean</th>\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_center\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"Regression\">Regression</th>\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_center\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"Multiple\">Multiple</th>\n    </tr>\n  </thead>\n  <tbody class=\"gt_table_body\">\n    <tr><td headers=\" \" class=\"gt_row gt_left\">(Intercept)</td>\n<td headers=\"Original Data\" class=\"gt_row gt_center\">0.260</td>\n<td headers=\"Listwise Deletion\" class=\"gt_row gt_center\">0.297</td>\n<td headers=\"Mean\" class=\"gt_row gt_center\">0.247</td>\n<td headers=\"Regression\" class=\"gt_row gt_center\">0.261</td>\n<td headers=\"Multiple\" class=\"gt_row gt_center\">0.262</td></tr>\n    <tr><td headers=\" \" class=\"gt_row gt_left\"></td>\n<td headers=\"Original Data\" class=\"gt_row gt_center\">(0.052)</td>\n<td headers=\"Listwise Deletion\" class=\"gt_row gt_center\">(0.082)</td>\n<td headers=\"Mean\" class=\"gt_row gt_center\">(0.047)</td>\n<td headers=\"Regression\" class=\"gt_row gt_center\">(0.056)</td>\n<td headers=\"Multiple\" class=\"gt_row gt_center\">(0.058)</td></tr>\n    <tr><td headers=\" \" class=\"gt_row gt_left\">Monthly Income</td>\n<td headers=\"Original Data\" class=\"gt_row gt_center\">0.574</td>\n<td headers=\"Listwise Deletion\" class=\"gt_row gt_center\">0.387</td>\n<td headers=\"Mean\" class=\"gt_row gt_center\">0.334</td>\n<td headers=\"Regression\" class=\"gt_row gt_center\">0.520</td>\n<td headers=\"Multiple\" class=\"gt_row gt_center\">0.543</td></tr>\n    <tr><td headers=\" \" class=\"gt_row gt_left\"></td>\n<td headers=\"Original Data\" class=\"gt_row gt_center\">(0.161)</td>\n<td headers=\"Listwise Deletion\" class=\"gt_row gt_center\">(0.166)</td>\n<td headers=\"Mean\" class=\"gt_row gt_center\">(0.095)</td>\n<td headers=\"Regression\" class=\"gt_row gt_center\">(0.163)</td>\n<td headers=\"Multiple\" class=\"gt_row gt_center\">(0.174)</td></tr>\n    <tr><td headers=\" \" class=\"gt_row gt_left\">Total Working Years</td>\n<td headers=\"Original Data\" class=\"gt_row gt_center\">0.937</td>\n<td headers=\"Listwise Deletion\" class=\"gt_row gt_center\">0.906</td>\n<td headers=\"Mean\" class=\"gt_row gt_center\">0.931</td>\n<td headers=\"Regression\" class=\"gt_row gt_center\">0.932</td>\n<td headers=\"Multiple\" class=\"gt_row gt_center\">0.933</td></tr>\n    <tr><td headers=\" \" class=\"gt_row gt_left\"></td>\n<td headers=\"Original Data\" class=\"gt_row gt_center\">(0.017)</td>\n<td headers=\"Listwise Deletion\" class=\"gt_row gt_center\">(0.023)</td>\n<td headers=\"Mean\" class=\"gt_row gt_center\">(0.017)</td>\n<td headers=\"Regression\" class=\"gt_row gt_center\">(0.017)</td>\n<td headers=\"Multiple\" class=\"gt_row gt_center\">(0.018)</td></tr>\n    <tr><td headers=\" \" class=\"gt_row gt_left\">Num Companies Worked</td>\n<td headers=\"Original Data\" class=\"gt_row gt_center\">1.107</td>\n<td headers=\"Listwise Deletion\" class=\"gt_row gt_center\">1.097</td>\n<td headers=\"Mean\" class=\"gt_row gt_center\">1.114</td>\n<td headers=\"Regression\" class=\"gt_row gt_center\">1.117</td>\n<td headers=\"Multiple\" class=\"gt_row gt_center\">1.111</td></tr>\n    <tr><td headers=\" \" class=\"gt_row gt_left\" style=\"border-bottom-width: 1px; border-bottom-style: solid; border-bottom-color: #000000;\"></td>\n<td headers=\"Original Data\" class=\"gt_row gt_center\" style=\"border-bottom-width: 1px; border-bottom-style: solid; border-bottom-color: #000000;\">(0.032)</td>\n<td headers=\"Listwise Deletion\" class=\"gt_row gt_center\" style=\"border-bottom-width: 1px; border-bottom-style: solid; border-bottom-color: #000000;\">(0.041)</td>\n<td headers=\"Mean\" class=\"gt_row gt_center\" style=\"border-bottom-width: 1px; border-bottom-style: solid; border-bottom-color: #000000;\">(0.033)</td>\n<td headers=\"Regression\" class=\"gt_row gt_center\" style=\"border-bottom-width: 1px; border-bottom-style: solid; border-bottom-color: #000000;\">(0.033)</td>\n<td headers=\"Multiple\" class=\"gt_row gt_center\" style=\"border-bottom-width: 1px; border-bottom-style: solid; border-bottom-color: #000000;\">(0.033)</td></tr>\n    <tr><td headers=\" \" class=\"gt_row gt_left\">Num.Obs.</td>\n<td headers=\"Original Data\" class=\"gt_row gt_center\">1470</td>\n<td headers=\"Listwise Deletion\" class=\"gt_row gt_center\">989</td>\n<td headers=\"Mean\" class=\"gt_row gt_center\">1470</td>\n<td headers=\"Regression\" class=\"gt_row gt_center\">1470</td>\n<td headers=\"Multiple\" class=\"gt_row gt_center\">1470</td></tr>\n    <tr><td headers=\" \" class=\"gt_row gt_left\">Num.Imp.</td>\n<td headers=\"Original Data\" class=\"gt_row gt_center\"></td>\n<td headers=\"Listwise Deletion\" class=\"gt_row gt_center\"></td>\n<td headers=\"Mean\" class=\"gt_row gt_center\"></td>\n<td headers=\"Regression\" class=\"gt_row gt_center\"></td>\n<td headers=\"Multiple\" class=\"gt_row gt_center\">50</td></tr>\n  </tbody>\n  <tfoot class=\"gt_sourcenotes\">\n    <tr>\n      <td class=\"gt_sourcenote\" colspan=\"6\">Source: IBM Watson Analytics Lab</td>\n    </tr>\n  </tfoot>\n  \n</table>\n</div>\n```\n\n:::\n:::\n\n\n@tbl-regression highlights how impressive multiple imputation can be and why it is often ill-advised to use deletion or single imputation methods. While there are one or two coefficients where the deletion or single imputation methods are close to or even marginally outperform the multiple imputation process, the multiple imputation results are significantly better overall. \n\nIt's not only that the coefficient estimates are generally closer to the estimates on the original data but also that the standard errors account for the greater uncertainty that should be factored in when imputing missing values. This is good. We do not want to be overconfident in conclusions drawn on data that includes imputed values, and multiple imputation guards against this.\n\n## Which Way, Western Man?\n\nSo, what kind of analyst are you going to be? Are you going to impute everything because unbiased estimates are overrated? Or will you delete all your missing data because less data is good, and there couldn't be anything important in those observations anyway? The good news is that these are improvements compared to what you have probably been doing until now. Not dealing with missing values is a methodological choice in and of itself, often a bad one. Sometimes, listwise deletion is defensible, but you should be the one to make that call because you're the one who's on the hook for the consequences! As @newman2014 argue - abstinence is not an option.\n\nOf course, if you want to show everyone what a wise old sage you are, consider multiple imputation instead.\n\nThis blog post only scratches the surface of missing data methods. As tempting as it might be to summarise an entire research field, I'd also have to read about 2,000 Donald Rubin papers before I could even get started on what anyone else has to say on this topic. Various methods, such as likelihood methods and multilevel imputation, haven't been covered here, and I haven't discussed the [evaluation of imputation results](stefvanbuuren.name/fimd/sec-evaluation.html). All of this stuff is important! But this blog post is already longer than I originally planned, and if I'm not careful it'll soon end up being so large that tech companies will start using it as a corpus to train their large language models. Hopefully, this brief introduction will inspire you to continue learning about missing data methods.\n\n## Acknowledgments {.appendix}\n\nMany thanks to Camilo Alvarez (of the great Trivote Discord fame) for his kind but constructive feedback during the development of this series of blog posts. I greatly appreciate anyone who helps me be just a little less stupid.\n\nThe preview image was generated using StabilityAI's [DreamStudio](https://beta.dreamstudio.ai/), using the prompt \"An eerie-looking image of a boy lost in a field, with a sign behind him saying 'Missing Data - Please Return At Earliest Convenience'\".\n\n## Support {.appendix}\n\nIf you enjoyed this blog post and would like to support my work, you can [buy me a coffee or a beer or give me a tip](https://www.buymeacoffee.com/paulj1989) as a thank you. \n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}