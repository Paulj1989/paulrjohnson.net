{
  "hash": "4aef6ab222753eab714f6df9a929f021",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Predicting Bob's Burgers Episode Ratings\nsubtitle: |\n  Proving that machine learning can be effective for even the dumbest of problems.\ndescription-meta: |\n  Building a machine learning model in Python to predict the IMDb user ratings of \n  Bob's Burgers episodes, using episode details and dialogue metrics.\ndate: 2025-02-07\nimage: burgers.webp\nimage-alt: |\n  A red sign outside a restaurant that has the outline of a burger, with \n  Burgers written in place of the patty.\ncategories: [Machine Learning, Hyperparameter Optimisation, Python]\nbibliography: references.bib\n---\n\n::: {#setup .cell execution_count=1}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Setup Code (Click to Expand)\"}\n# import packages\nimport optuna\nimport textwrap\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nfrom great_tables import GT\nfrom plotnine import (\n    aes,\n    element_blank,\n    element_line,\n    element_text,\n    facet_wrap,\n    geom_bar,\n    geom_histogram,\n    geom_hline,\n    geom_point,\n    geom_smooth,\n    ggplot,\n    labs,\n    scale_x_continuous,\n    scale_fill_gradient,\n    theme,\n    theme_minimal,\n    theme_set,\n    theme_update,\n)\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.dummy import DummyRegressor\nfrom sklearn.ensemble import (\n    ExtraTreesRegressor,\n    GradientBoostingRegressor,\n    RandomForestRegressor,\n)\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import make_scorer, root_mean_squared_error\nfrom sklearn.model_selection import cross_val_score, KFold, train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import FunctionTransformer, OneHotEncoder\n\n# import data\nbobs_burgers = (\n    pd.read_csv(\n        \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/\"\n        \"master/data/2024/2024-11-19/episode_metrics.csv\"\n    )\n    .merge(\n        pd.read_csv(\n            \"https://raw.githubusercontent.com/poncest/bobsburgersR/refs/\"\n            \"heads/main/data-raw/IMDb_Wikipedia_Bobs_Burgers_Data_Clean.csv\"\n        ),\n        on=[\"season\", \"episode\"],\n    )\n    .assign(\n        wikipedia_directed_by=lambda df: df[\"wikipedia_directed_by\"]\n        .str.replace(r\"\\bBoohwan\\b(?! Lim)\", \"Boohwan Lim\", regex=True)\n        .str.replace(\"Brian LoSchiavo\", \"Brian Loschiavo\")\n        .str.replace(\"Mathew Long\", \"Matthew Long\")\n    )\n    .assign(\n        wikipedia_written_by=lambda df: df[\"wikipedia_written_by\"]\n        .str.replace(\"Teleplay by : \", \"\")\n        .str.replace(\"Story by : \", \" & \")\n        .str.replace(\"Lizzie Molyneux-Logelin\", \"Lizzie Molyneux\")\n        .str.replace(r\"\\bLizzie\\b(?! Molyneux)\", \"Lizzie Molyneux\", regex=True)\n    )\n    .pipe(lambda df: df.astype({col: \"category\" for col in df.select_dtypes(\"object\")}))\n)\n\n# set plot theme\ntheme_set(theme_minimal(base_size=11, base_family=\"Poppins\"))\ntheme_update(\n    panel_grid_major=element_line(color=\"#e5e5e5\", size=1),\n    panel_grid_minor=element_blank(),\n    panel_spacing_x=0.01,\n    panel_spacing_y=0.01,\n    axis_title_x=element_text(color=\"#4d4d4d\", margin={\"t\": 5}, size=11),\n    axis_title_y=element_text(color=\"#4d4d4d\", margin={\"r\": 5}, size=11),\n    axis_text=element_text(color=\"#7f7f7f\", size=10),\n    axis_text_x=element_text(angle=30, vjust=1, hjust=0.75),\n    axis_ticks=element_line(color=\"#e5e5e5\", size=0.4),\n    axis_ticks_length=1,\n    legend_position=\"top\",\n    legend_title=element_blank(),\n    legend_text=element_text(\n        size=11, va=\"center\", margin={\"t\": 5, \"r\": 5, \"b\": 0, \"l\": 5}\n    ),\n    legend_box_margin=5,\n    legend_key_width=20,\n    legend_key_spacing=10,\n    plot_title=element_text(\n        ha=\"left\", color=\"black\", family=\"Lora\", size=15, margin={\"t\": 5, \"b\": 5}\n    ),\n    plot_subtitle=element_text(\n        ha=\"left\",\n        color=\"#4d4d4d\",\n        family=\"Lora\",\n        lineheight=1.2,\n        size=11.5,\n        margin={\"t\": 5, \"r\": 0, \"b\": 10, \"l\": 0},\n    ),\n    plot_caption=element_text(\n        color=\"#7f7f7f\", size=9, ha=\"right\", margin={\"t\": 10, \"r\": 0, \"b\": 0, \"l\": 0}\n    ),\n    strip_text=element_text(size=11, margin={\"t\": 0, \"r\": 0, \"b\": 10, \"l\": 0}),\n)\n\n# suppress all optuna logging outputs\noptuna.logging.set_verbosity(optuna.logging.CRITICAL)\n# to switch back - optuna.logging.set_verbosity(optuna.logging.INFO)\n```\n:::\n\n\nBiologists have been wrestling with the challenge of predicting the structure of proteins for over 50 years. Doing so would enable scientists to better understand diseases and accelerate the development of medications [@heaven2022]. Experimental methods for predicting protein structures can be arduous, proving inadequate for predicting the structure of the over 200 million proteins discovered in nature [@service2020]. On the other hand, the performance of computational methods fell short of the experimental. That was until DeepMind's [AlphaFold2](https://deepmind.google/technologies/alphafold/) was released at CASP14 [@jumper2021]. DeepMind combined a vast repository of training data from the [Protein Data Bank](https://www.wwpdb.org) with innovations in deep learning and immense computational resources, achieving landmark performance and solving one of science's grand challenges.\n\nAlphaFold is a testament to the potential value created by combining massive volumes of data and computational power (when incentives align and intentions are good). The right data can help us solve all kinds of problems, from predicting protein structures to less grand but no less important issues, such as predicting the IMDb user ratings of Bob's Burgers episodes. \n\nWhile the big brains at the world's best universities are busy trying to change the world, I'm creating models no one needs or asks for. I will make a machine learning model that predicts IMDb user ratings for Bob's Burgers episodes, using features that summarise the episode (episode and season number, date aired, writer, director, etc.) and metrics that describe the dialogue in the episode itself, seeing how far I can stretch the limited sample size.\n\nI don't know what the TV equivalent is to \"football isn't played on a spreadsheet,\" but by the end of this post, I hope everyone is so mad they are telling me to \"watch the ~~games~~ TV.\"\n\n## Data Exploration\n\nThe Bob's Burgers data comes from Week 47 of 2024's [TidyTuesday](https://github.com/rfordatascience/tidytuesday/tree/main/data/2024/2024-11-19). Most of the data is originally from [Steven Ponce](https://stevenponce.netlify.app)'s [`bobsburgersR`](https://github.com/poncest/bobsburgersR) R package, including many self-explanatory features that describe episode details. But the TidyTuesday data also includes dialogue metrics that are not as clear. @tbl-dialogue-metrics below defines the six dialogue metrics in the dataset.\n\n::: {#tbl-dialogue-metrics}\n\n::: {#cell-dialogue-metrics .cell execution_count=2}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Table Code (Click to Expand)\"}\n(\n    pd.DataFrame(\n        {\n            \"Features\": [\n                \"dialogue_density\",\n                \"avg_length\",\n                \"sentiment_variance\",\n                \"unique_words\",\n                \"question_ratio\",\n                \"exclamation_ratio\",\n            ],\n            \"Description\": [\n                \"The number of non-blank lines in this episode.\",\n                \"The average number of characters (technically codepoints) per line \"\n                \"of dialogue.\",\n                \"The variance in the numeric AFINN sentiment of words in this episode.\",\n                \"The number of unique lowercase words in this episode.\",\n                \"The proportion of lines of dialogue that contain at least one \"\n                \"question mark ('?').\",\n                \"The proportion of lines of dialogue that contain at least one \"\n                \"exclamation point ('!').\",\n            ],\n        }\n    ).pipe(\n        lambda df: (\n            GT(df).tab_source_note(source_note=\"Data: {bobsburgersR} (via TidyTuesday)\")\n        )\n    )\n)\n```\n\n::: {#dialogue-metrics .cell-output .cell-output-display execution_count=2}\n```{=html}\n<div id=\"wdcrjmdqbk\" style=\"padding-left:0px;padding-right:0px;padding-top:10px;padding-bottom:10px;overflow-x:auto;overflow-y:auto;width:auto;height:auto;\">\n<style>\n#wdcrjmdqbk table {\n          font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;\n          -webkit-font-smoothing: antialiased;\n          -moz-osx-font-smoothing: grayscale;\n        }\n\n#wdcrjmdqbk thead, tbody, tfoot, tr, td, th { border-style: none; }\n tr { background-color: transparent; }\n#wdcrjmdqbk p { margin: 0; padding: 0; }\n #wdcrjmdqbk .gt_table { display: table; border-collapse: collapse; line-height: normal; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; }\n #wdcrjmdqbk .gt_caption { padding-top: 4px; padding-bottom: 4px; }\n #wdcrjmdqbk .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; }\n #wdcrjmdqbk .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 3px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; }\n #wdcrjmdqbk .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; }\n #wdcrjmdqbk .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; }\n #wdcrjmdqbk .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; }\n #wdcrjmdqbk .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; }\n #wdcrjmdqbk .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; }\n #wdcrjmdqbk .gt_column_spanner_outer:first-child { padding-left: 0; }\n #wdcrjmdqbk .gt_column_spanner_outer:last-child { padding-right: 0; }\n #wdcrjmdqbk .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; }\n #wdcrjmdqbk .gt_spanner_row { border-bottom-style: hidden; }\n #wdcrjmdqbk .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; }\n #wdcrjmdqbk .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; }\n #wdcrjmdqbk .gt_from_md> :first-child { margin-top: 0; }\n #wdcrjmdqbk .gt_from_md> :last-child { margin-bottom: 0; }\n #wdcrjmdqbk .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; }\n #wdcrjmdqbk .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; }\n #wdcrjmdqbk .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; }\n #wdcrjmdqbk .gt_row_group_first td { border-top-width: 2px; }\n #wdcrjmdqbk .gt_row_group_first th { border-top-width: 2px; }\n #wdcrjmdqbk .gt_striped { background-color: rgba(128,128,128,0.05); }\n #wdcrjmdqbk .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; }\n #wdcrjmdqbk .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; }\n #wdcrjmdqbk .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; text-align: left; }\n #wdcrjmdqbk .gt_left { text-align: left; }\n #wdcrjmdqbk .gt_center { text-align: center; }\n #wdcrjmdqbk .gt_right { text-align: right; font-variant-numeric: tabular-nums; }\n #wdcrjmdqbk .gt_font_normal { font-weight: normal; }\n #wdcrjmdqbk .gt_font_bold { font-weight: bold; }\n #wdcrjmdqbk .gt_font_italic { font-style: italic; }\n #wdcrjmdqbk .gt_super { font-size: 65%; }\n #wdcrjmdqbk .gt_footnote_marks { font-size: 75%; vertical-align: 0.4em; position: initial; }\n #wdcrjmdqbk .gt_asterisk { font-size: 100%; vertical-align: 0; }\n \n</style>\n<table class=\"gt_table\" data-quarto-disable-processing=\"false\" data-quarto-bootstrap=\"false\">\n<thead>\n\n<tr class=\"gt_col_headings\">\n  <th class=\"gt_col_heading gt_columns_bottom_border gt_left\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"Features\">Features</th>\n  <th class=\"gt_col_heading gt_columns_bottom_border gt_left\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"Description\">Description</th>\n</tr>\n</thead>\n<tbody class=\"gt_table_body\">\n  <tr>\n    <td class=\"gt_row gt_left\">dialogue_density</td>\n    <td class=\"gt_row gt_left\">The number of non-blank lines in this episode.</td>\n  </tr>\n  <tr>\n    <td class=\"gt_row gt_left\">avg_length</td>\n    <td class=\"gt_row gt_left\">The average number of characters (technically codepoints) per line of dialogue.</td>\n  </tr>\n  <tr>\n    <td class=\"gt_row gt_left\">sentiment_variance</td>\n    <td class=\"gt_row gt_left\">The variance in the numeric AFINN sentiment of words in this episode.</td>\n  </tr>\n  <tr>\n    <td class=\"gt_row gt_left\">unique_words</td>\n    <td class=\"gt_row gt_left\">The number of unique lowercase words in this episode.</td>\n  </tr>\n  <tr>\n    <td class=\"gt_row gt_left\">question_ratio</td>\n    <td class=\"gt_row gt_left\">The proportion of lines of dialogue that contain at least one question mark ('?').</td>\n  </tr>\n  <tr>\n    <td class=\"gt_row gt_left\">exclamation_ratio</td>\n    <td class=\"gt_row gt_left\">The proportion of lines of dialogue that contain at least one exclamation point ('!').</td>\n  </tr>\n</tbody>\n  <tfoot class=\"gt_sourcenotes\">\n  \n  <tr>\n    <td class=\"gt_sourcenote\" colspan=\"2\">Data: {bobsburgersR} (via TidyTuesday)</td>\n  </tr>\n\n</tfoot>\n\n</table>\n\n</div>\n        \n```\n:::\n:::\n\n\nBob's Burgers Episode Dialogue Metrics\n\n:::\n\nThere are further details about the data in the TidyTuesday repository, including the following quote:\n\n> We would like to emphasise that you should not draw conclusions about causation in the data. There are various moderating variables that affect all data, many of which might not have been captured in these datasets. As such, our suggestion is to use the data provided to practice your data tidying and plotting techniques, and to consider for yourself what nuances might underlie these relationships.\n\nThey are not wrong! If I was doing this **properly**, I'd spend more time trying to understand the data, including extracting more information from the dialogue. I'd also spend some time thinking about what moderating variables they are referring to and including those where possible. While the results might not be groundbreaking, I'm sure the model would be more precise. Instead, I'm just having fun. I'm a silly goose.\n\nI should still do some exploratory analysis first. I'm not a complete monster. Below, we plot the outcome distribution using a colour gradient scale to represent seasons. IMDb ratings are approximately normally distributed, with most values between 7 and 8.\n\n::: {#cell-season-ratings .cell execution_count=3}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Plot Code (Click to Expand)\"}\n(\n    ggplot(bobs_burgers, aes(\"rating\", fill=\"factor(season)\"))\n    + geom_bar()\n    + geom_hline(yintercept=0, colour=\"#343a40\")\n    + labs(\n        title=\"Distribution of Bob's Burgers Episode Ratings by Season\",\n        subtitle=(\n            textwrap.fill(\n                \"The distribution of IMDb user ratings for Bob's Burgers episodes, \"\n                \"with a gradient scale representing season (blue = earlier seasons; \"\n                \"grey = later seasons). The earlier seasons of the show are generally \"\n                \"favoured.\",\n                width=92,\n            )\n        ),\n        x=\"IMDb Rating\",\n        y=\"\",\n        caption=\"Visualisation: Paul Johnson | Data: {bobsburgersR} (via TidyTuesday)\",\n    )\n    + scale_fill_gradient(low=\"#026E99\", high=\"#D2D2D2\", guide=None)\n)\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/season-ratings-output-1.png){#season-ratings width=672 height=480 fig-alt='A bar plot visualising the distribution of Bob\\'s Burgers IMDb user ratings, with a gradient scale distinguishing seasons. The plot demonstrates that earlier seasons tend to have higher ratings, but there are several large outliers in later seasons that have ratings of nine or higher.'}\n:::\n:::\n\n\nThe earlier seasons (blue) are generally higher-rated, though some of the highest-rated episodes come from the later seasons (grey). Five episodes have ratings of 9+: two in season 6, two in season 13, and one in season 14. The highest-rated episode, _The Plight Before Christmas_ (S13E10), gets a 9.6.\n\nWe can also look at the correlation between all the numeric features in the dataset and the target, IMDb ratings, in @tbl-correlations.\n\n::: {#tbl-correlations}\n\n::: {#cell-correlations .cell execution_count=4}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Table Code (Click to Expand)\"}\n(\n    bobs_burgers.select_dtypes(include=\"number\")\n    .drop(columns=\"rating\")\n    .corrwith(bobs_burgers[\"rating\"])\n    .sort_values(ascending=False)\n    .reset_index()\n    .rename(columns={0: \"Correlation\", \"index\": \"Feature\"})\n    .pipe(\n        lambda df: GT(df, rowname_col=\"Feature\")\n        .fmt_number(columns=\"Correlation\", decimals=2)\n        .cols_align(columns=\"Correlation\", align=\"center\")\n        .tab_source_note(source_note=\"Data: {bobsburgersR} (via TidyTuesday)\")\n        .tab_options(table_width=\"100%\")\n    )\n)\n```\n\n::: {#correlations .cell-output .cell-output-display execution_count=4}\n```{=html}\n<div id=\"ejyuegqrgf\" style=\"padding-left:0px;padding-right:0px;padding-top:10px;padding-bottom:10px;overflow-x:auto;overflow-y:auto;width:auto;height:auto;\">\n<style>\n#ejyuegqrgf table {\n          font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;\n          -webkit-font-smoothing: antialiased;\n          -moz-osx-font-smoothing: grayscale;\n        }\n\n#ejyuegqrgf thead, tbody, tfoot, tr, td, th { border-style: none; }\n tr { background-color: transparent; }\n#ejyuegqrgf p { margin: 0; padding: 0; }\n #ejyuegqrgf .gt_table { display: table; border-collapse: collapse; line-height: normal; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: 100%; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; }\n #ejyuegqrgf .gt_caption { padding-top: 4px; padding-bottom: 4px; }\n #ejyuegqrgf .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; }\n #ejyuegqrgf .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 3px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; }\n #ejyuegqrgf .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; }\n #ejyuegqrgf .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; }\n #ejyuegqrgf .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; }\n #ejyuegqrgf .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; }\n #ejyuegqrgf .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; }\n #ejyuegqrgf .gt_column_spanner_outer:first-child { padding-left: 0; }\n #ejyuegqrgf .gt_column_spanner_outer:last-child { padding-right: 0; }\n #ejyuegqrgf .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; }\n #ejyuegqrgf .gt_spanner_row { border-bottom-style: hidden; }\n #ejyuegqrgf .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; }\n #ejyuegqrgf .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; }\n #ejyuegqrgf .gt_from_md> :first-child { margin-top: 0; }\n #ejyuegqrgf .gt_from_md> :last-child { margin-bottom: 0; }\n #ejyuegqrgf .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; }\n #ejyuegqrgf .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; }\n #ejyuegqrgf .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; }\n #ejyuegqrgf .gt_row_group_first td { border-top-width: 2px; }\n #ejyuegqrgf .gt_row_group_first th { border-top-width: 2px; }\n #ejyuegqrgf .gt_striped { background-color: rgba(128,128,128,0.05); }\n #ejyuegqrgf .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; }\n #ejyuegqrgf .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; }\n #ejyuegqrgf .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; text-align: left; }\n #ejyuegqrgf .gt_left { text-align: left; }\n #ejyuegqrgf .gt_center { text-align: center; }\n #ejyuegqrgf .gt_right { text-align: right; font-variant-numeric: tabular-nums; }\n #ejyuegqrgf .gt_font_normal { font-weight: normal; }\n #ejyuegqrgf .gt_font_bold { font-weight: bold; }\n #ejyuegqrgf .gt_font_italic { font-style: italic; }\n #ejyuegqrgf .gt_super { font-size: 65%; }\n #ejyuegqrgf .gt_footnote_marks { font-size: 75%; vertical-align: 0.4em; position: initial; }\n #ejyuegqrgf .gt_asterisk { font-size: 100%; vertical-align: 0; }\n \n</style>\n<table class=\"gt_table\" data-quarto-disable-processing=\"false\" data-quarto-bootstrap=\"false\">\n<thead>\n\n<tr class=\"gt_col_headings\">\n  <th class=\"gt_col_heading gt_columns_bottom_border gt_left\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"\"></th>\n  <th class=\"gt_col_heading gt_columns_bottom_border gt_center\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"Correlation\">Correlation</th>\n</tr>\n</thead>\n<tbody class=\"gt_table_body\">\n  <tr>\n    <th class=\"gt_row gt_left gt_stub\">wikipedia_viewers</th>\n    <td class=\"gt_row gt_center\">0.31</td>\n  </tr>\n  <tr>\n    <th class=\"gt_row gt_left gt_stub\">exclamation_ratio</th>\n    <td class=\"gt_row gt_center\">0.28</td>\n  </tr>\n  <tr>\n    <th class=\"gt_row gt_left gt_stub\">dialogue_density</th>\n    <td class=\"gt_row gt_center\">0.17</td>\n  </tr>\n  <tr>\n    <th class=\"gt_row gt_left gt_stub\">question_ratio</th>\n    <td class=\"gt_row gt_center\">0.11</td>\n  </tr>\n  <tr>\n    <th class=\"gt_row gt_left gt_stub\">sentiment_variance</th>\n    <td class=\"gt_row gt_center\">0.09</td>\n  </tr>\n  <tr>\n    <th class=\"gt_row gt_left gt_stub\">avg_length</th>\n    <td class=\"gt_row gt_center\">0.07</td>\n  </tr>\n  <tr>\n    <th class=\"gt_row gt_left gt_stub\">episode</th>\n    <td class=\"gt_row gt_center\">−0.06</td>\n  </tr>\n  <tr>\n    <th class=\"gt_row gt_left gt_stub\">unique_words</th>\n    <td class=\"gt_row gt_center\">−0.15</td>\n  </tr>\n  <tr>\n    <th class=\"gt_row gt_left gt_stub\">season</th>\n    <td class=\"gt_row gt_center\">−0.39</td>\n  </tr>\n  <tr>\n    <th class=\"gt_row gt_left gt_stub\">episode_overall</th>\n    <td class=\"gt_row gt_center\">−0.39</td>\n  </tr>\n  <tr>\n    <th class=\"gt_row gt_left gt_stub\">year</th>\n    <td class=\"gt_row gt_center\">−0.40</td>\n  </tr>\n</tbody>\n  <tfoot class=\"gt_sourcenotes\">\n  \n  <tr>\n    <td class=\"gt_sourcenote\" colspan=\"2\">Data: {bobsburgersR} (via TidyTuesday)</td>\n  </tr>\n\n</tfoot>\n\n</table>\n\n</div>\n        \n```\n:::\n:::\n\n\nCorrelations Between Numeric Features & IMDb User Ratings\n\n:::\n\nSeveral features, including a couple dialogue metrics, appear to be making minimal contributions to the variance in episode ratings. However, ratings are moderately correlated with viewers, exclamation ratio, season, the overall episode number, and the year the episode was aired. \n\nThe remarkably similar correlations between ratings and the season, overall episode, and year suggest that those three features capture the same thing—time. The plot below visualises how ratings have changed over time. \n\n::: {#cell-episode-ratings .cell execution_count=5}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Plot Code (Click to Expand)\"}\n(\n    ggplot(bobs_burgers, aes(x=\"episode_overall\", y=\"rating\"))\n    + geom_point(size=2, stroke=0.5, fill=\"white\", color=\"#343a40\", shape=\"o\")\n    + geom_smooth(method=\"lm\", size=1, se=False, color=\"#026E99\")\n    + scale_x_continuous(breaks=[0, 50, 100, 150, 200, 250])\n    + labs(\n        title=\"Bob's Burgers Episode Ratings Over Time\",\n        subtitle=(\n            textwrap.fill(\n                \"Comparing how Bob's Burgers episode ratings have changed over time. \"\n                \"There has been a steady decline in ratings, with no episode receiving \"\n                \"an average rating below seven until season 10.\",\n                width=80,\n            )\n        ),\n        x=\"Episode\",\n        y=\"IMDb Rating\",\n        caption=\"Visualisation: Paul Johnson | Data: {bobsburgersR} (via TidyTuesday)\",\n    ) \n)\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/episode-ratings-output-1.png){#episode-ratings width=672 height=480 fig-alt='A regression plot visualising the relationship between Bob\\'s Burgers IMDb user ratings and total episode number. While the average rating is just under 8 when the show first begins, it gradually declines over 250+ episodes, to around 7.5.'}\n:::\n:::\n\n\nWhile episode ratings are noisy, there is a visible decline over time. The average ratings for the first eight seasons are between 7.7 and 8, dropping in season nine (seen around 150 episodes in the above plot) and hovering around 7.5 for the remaining seasons.\n\nAt the other end of @tbl-correlations, a moderate positive correlation exists between ratings and viewership, which is visualised below.\n\n::: {#cell-viewers-ratings .cell execution_count=6}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Plot Code (Click to Expand)\"}\n(\n    ggplot(bobs_burgers, aes(x=\"wikipedia_viewers\", y=\"rating\"))\n    + geom_point(size=2, stroke=0.5, fill=\"white\", color=\"#343a40\", shape=\"o\")\n    + geom_smooth(method=\"lm\", size=1, se=False, color=\"#026E99\")\n    + labs(\n        title=\"Bob's Burgers Episode Ratings by Viewers\",\n        subtitle=(\n            textwrap.fill(\n                \"Comparing the association between IMDb user ratings of Bob's Burgers \"\n                \"episodes and US viewers when first aired. While it is noisy, there \"\n                \"does appear to be a positive correlation between viewing figures and \"\n                \"the episode rating.\",\n                width=91,\n            )\n        ),\n        x=\"Viewers (Millions)\",\n        y=\"IMDb Rating\",\n        caption=\"Visualisation: Paul Johnson | Data: {bobsburgersR} (via TidyTuesday)\",\n    )\n)\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/viewers-ratings-output-1.png){#viewers-ratings width=672 height=480 fig-alt='A regression plot visualising the relationship between Bob\\'s Burgers IMDb user ratings and total US viewers when the episode was first aired. There appears to be a moderate increase in ratings as viewership increases, but there are also several outliers that distort the plot, particularly one episode with more than nine million viewers.'}\n:::\n:::\n\n\nOne almighty outlier pushes the x-axis out a lot further: S1E1, watched by over nine million people, almost three million more viewers than any other episode. It turns out this was the first episode of Bob's Burgers, and I think this hints at the likely explanation for the strong correlation between viewers and ratings[^Viewers]. The viewership has been steadily declining since the first season. The causal mechanism may go in the other direction (the quality of episodes declining and leading to a decline in the audience), but it's just as likely the correlation is just capturing time effects.\n\nIs everything a function of time? Maybe the overall episode number is the only feature we need? @tbl-correlations might not have given us much hope in the predictive power of the dialogue metrics, but they might be our only hope if we want to include any features that are not just a variety of ways of measuring the effect of time.\n\nThe dialogue metrics are intended to describe the episode itself. The assumption is that these metrics will capture characteristics that define the episode and offer some signal about the episode quality (and, therefore, ratings). Below, each dialogue metric is plotted against IMDb ratings.\n\n::: {#cell-metric-ratings .cell execution_count=7}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Plot Code (Click to Expand)\"}\n(\n    bobs_burgers.melt(\n        id_vars=[\"rating\"],\n        value_vars=[\n            \"dialogue_density\",\n            \"avg_length\",\n            \"sentiment_variance\",\n            \"unique_words\",\n            \"question_ratio\",\n            \"exclamation_ratio\",\n        ],\n        var_name=\"label\",\n        value_name=\"value\",\n    )\n    .assign(\n        label=lambda df: df[\"label\"]\n        .str.replace(\"_\", \" \")\n        .str.title()\n        .str.replace(\"Avg\", \"Average\", case=False)\n    )\n    .pipe(\n        lambda df: ggplot(df, aes(x=\"value\", y=\"rating\", group=1))\n        + geom_point(\n            size=2, stroke=0.5, alpha=0.8, fill=\"white\", color=\"#343a40\", shape=\"o\"\n        )\n        + geom_smooth(method=\"lm\", size=1, se=False, color=\"#026E99\")\n        + facet_wrap(\"~label\", scales=\"free_x\", nrow=3)\n        + labs(\n            title=\"Bob's Burgers Episode Ratings by Episode Dialogue Metrics\",\n            subtitle=(\n                textwrap.fill(\n                    \"Comparing the association between Bob's Burgers IMDb user ratings \"\n                    \"by the dialogue metrics for each episode. While all six metrics \"\n                    \"are noisy, the exclamation ratio and the number of unique words do \"\n                    \"appear to have correlations with the episode's rating.\",\n                    width=88,\n                )\n            ),\n            x=\"\",\n            y=\"IMDb Rating\",\n            caption=(\n                \"Visualisation: Paul Johnson | \"\n                \"Data: {bobsburgersR} (via TidyTuesday)\"\n            ),\n        )\n        + theme(\n            figure_size=(7, 9),\n            panel_spacing_x=0.02,\n            panel_spacing_y=0.04,\n            plot_subtitle=element_text(\n                margin={\"t\": 5, \"r\": 0, \"b\": 25, \"l\": 0},\n            ),\n        )\n    )\n)\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/metric-ratings-output-1.png){#metric-ratings width=672 height=864 fig-alt='A faceted regression plot visualising the relationship between Bob\\'s Burgers IMDb user ratings and each of the six dialogue metrics. While most of the metrics appear to have minimal effect on ratings, increases in the number of unique words in an episode is associated with a decline in ratings, while the larger exclamation ratio leads to higher ratings.'}\n:::\n:::\n\n\nThere appears to be some signal here, but it's mostly noise. Metrics like dialogue density appear to offer nothing, but exclamation ratio and unique words have stronger associations with ratings. I can strain to convince myself there are some interesting patterns in the question ratio and sentiment variance plots, but that may be explained by motivated reasoning more than any meaningful signal in these features.\n\nFinally, in my desperation, I turn to the categorical features. Specifically, the episode directors and writers could contain predictive value[^Cats]. @tbl-director-ratings & @tbl-writer-ratings show the total episodes and average rating for each director and writer credited on at least five Bob's Burgers episodes.\n\n::: {.panel-tabset}\n\n### Director Ratings\n\n:::: {#tbl-director-ratings}\n\n::: {#cell-director-ratings .cell execution_count=8}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Table Code (Click to Expand)\"}\n(\n    bobs_burgers.assign(\n        wikipedia_directed_by=lambda df: (\n            df[\"wikipedia_directed_by\"].str.split(\" & \")\n        )\n    )\n    .explode(\"wikipedia_directed_by\")\n    .groupby(\"wikipedia_directed_by\")\n    .agg(\n        episodes=(\"wikipedia_directed_by\", \"size\"), \n        average_rating=(\"rating\", \"mean\")\n        )\n    .query(\"episodes >= 5\")\n    .sort_values(by=\"average_rating\", ascending=False)\n    .reset_index()\n    .pipe(\n        lambda df: GT(df, rowname_col=\"wikipedia_directed_by\")\n        .fmt_number(columns=\"average_rating\", decimals=2)\n        .cols_label(\n            episodes=\"Total Episodes Directed\", \n            average_rating=\"Average Episode Rating\"\n        )\n        .cols_align(columns=[\"episodes\", \"average_rating\"], align=\"center\")\n        .tab_source_note(source_note=\"Data: {bobsburgersR} (via TidyTuesday)\")\n        .tab_options(table_width=\"100%\")\n    )\n)\n```\n\n::: {#director-ratings .cell-output .cell-output-display execution_count=8}\n```{=html}\n<div id=\"dotscgrwos\" style=\"padding-left:0px;padding-right:0px;padding-top:10px;padding-bottom:10px;overflow-x:auto;overflow-y:auto;width:auto;height:auto;\">\n<style>\n#dotscgrwos table {\n          font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;\n          -webkit-font-smoothing: antialiased;\n          -moz-osx-font-smoothing: grayscale;\n        }\n\n#dotscgrwos thead, tbody, tfoot, tr, td, th { border-style: none; }\n tr { background-color: transparent; }\n#dotscgrwos p { margin: 0; padding: 0; }\n #dotscgrwos .gt_table { display: table; border-collapse: collapse; line-height: normal; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: 100%; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; }\n #dotscgrwos .gt_caption { padding-top: 4px; padding-bottom: 4px; }\n #dotscgrwos .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; }\n #dotscgrwos .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 3px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; }\n #dotscgrwos .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; }\n #dotscgrwos .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; }\n #dotscgrwos .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; }\n #dotscgrwos .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; }\n #dotscgrwos .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; }\n #dotscgrwos .gt_column_spanner_outer:first-child { padding-left: 0; }\n #dotscgrwos .gt_column_spanner_outer:last-child { padding-right: 0; }\n #dotscgrwos .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; }\n #dotscgrwos .gt_spanner_row { border-bottom-style: hidden; }\n #dotscgrwos .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; }\n #dotscgrwos .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; }\n #dotscgrwos .gt_from_md> :first-child { margin-top: 0; }\n #dotscgrwos .gt_from_md> :last-child { margin-bottom: 0; }\n #dotscgrwos .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; }\n #dotscgrwos .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; }\n #dotscgrwos .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; }\n #dotscgrwos .gt_row_group_first td { border-top-width: 2px; }\n #dotscgrwos .gt_row_group_first th { border-top-width: 2px; }\n #dotscgrwos .gt_striped { background-color: rgba(128,128,128,0.05); }\n #dotscgrwos .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; }\n #dotscgrwos .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; }\n #dotscgrwos .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; text-align: left; }\n #dotscgrwos .gt_left { text-align: left; }\n #dotscgrwos .gt_center { text-align: center; }\n #dotscgrwos .gt_right { text-align: right; font-variant-numeric: tabular-nums; }\n #dotscgrwos .gt_font_normal { font-weight: normal; }\n #dotscgrwos .gt_font_bold { font-weight: bold; }\n #dotscgrwos .gt_font_italic { font-style: italic; }\n #dotscgrwos .gt_super { font-size: 65%; }\n #dotscgrwos .gt_footnote_marks { font-size: 75%; vertical-align: 0.4em; position: initial; }\n #dotscgrwos .gt_asterisk { font-size: 100%; vertical-align: 0; }\n \n</style>\n<table class=\"gt_table\" data-quarto-disable-processing=\"false\" data-quarto-bootstrap=\"false\">\n<thead>\n\n<tr class=\"gt_col_headings\">\n  <th class=\"gt_col_heading gt_columns_bottom_border gt_left\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"\"></th>\n  <th class=\"gt_col_heading gt_columns_bottom_border gt_center\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"Total Episodes Directed\">Total Episodes Directed</th>\n  <th class=\"gt_col_heading gt_columns_bottom_border gt_center\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"Average Episode Rating\">Average Episode Rating</th>\n</tr>\n</thead>\n<tbody class=\"gt_table_body\">\n  <tr>\n    <th class=\"gt_row gt_left gt_stub\">Jennifer Coyle</th>\n    <td class=\"gt_row gt_center\">19</td>\n    <td class=\"gt_row gt_center\">8.07</td>\n  </tr>\n  <tr>\n    <th class=\"gt_row gt_left gt_stub\">Kyounghee Lim</th>\n    <td class=\"gt_row gt_center\">15</td>\n    <td class=\"gt_row gt_center\">8.00</td>\n  </tr>\n  <tr>\n    <th class=\"gt_row gt_left gt_stub\">Boohwan Lim</th>\n    <td class=\"gt_row gt_center\">15</td>\n    <td class=\"gt_row gt_center\">7.95</td>\n  </tr>\n  <tr>\n    <th class=\"gt_row gt_left gt_stub\">Bernard Derriman</th>\n    <td class=\"gt_row gt_center\">9</td>\n    <td class=\"gt_row gt_center\">7.89</td>\n  </tr>\n  <tr>\n    <th class=\"gt_row gt_left gt_stub\">Mauricio Pardo</th>\n    <td class=\"gt_row gt_center\">6</td>\n    <td class=\"gt_row gt_center\">7.87</td>\n  </tr>\n  <tr>\n    <th class=\"gt_row gt_left gt_stub\">Anthony Chun</th>\n    <td class=\"gt_row gt_center\">8</td>\n    <td class=\"gt_row gt_center\">7.86</td>\n  </tr>\n  <tr>\n    <th class=\"gt_row gt_left gt_stub\">Tyree Dillihay</th>\n    <td class=\"gt_row gt_center\">32</td>\n    <td class=\"gt_row gt_center\">7.86</td>\n  </tr>\n  <tr>\n    <th class=\"gt_row gt_left gt_stub\">Don MacKinnon</th>\n    <td class=\"gt_row gt_center\">12</td>\n    <td class=\"gt_row gt_center\">7.85</td>\n  </tr>\n  <tr>\n    <th class=\"gt_row gt_left gt_stub\">Wes Archer</th>\n    <td class=\"gt_row gt_center\">7</td>\n    <td class=\"gt_row gt_center\">7.77</td>\n  </tr>\n  <tr>\n    <th class=\"gt_row gt_left gt_stub\">Brian Loschiavo</th>\n    <td class=\"gt_row gt_center\">23</td>\n    <td class=\"gt_row gt_center\">7.76</td>\n  </tr>\n  <tr>\n    <th class=\"gt_row gt_left gt_stub\">Ian Hamilton</th>\n    <td class=\"gt_row gt_center\">11</td>\n    <td class=\"gt_row gt_center\">7.65</td>\n  </tr>\n  <tr>\n    <th class=\"gt_row gt_left gt_stub\">Chris Song</th>\n    <td class=\"gt_row gt_center\">47</td>\n    <td class=\"gt_row gt_center\">7.60</td>\n  </tr>\n  <tr>\n    <th class=\"gt_row gt_left gt_stub\">Matthew Long</th>\n    <td class=\"gt_row gt_center\">18</td>\n    <td class=\"gt_row gt_center\">7.53</td>\n  </tr>\n  <tr>\n    <th class=\"gt_row gt_left gt_stub\">Ryan Mattos</th>\n    <td class=\"gt_row gt_center\">26</td>\n    <td class=\"gt_row gt_center\">7.49</td>\n  </tr>\n  <tr>\n    <th class=\"gt_row gt_left gt_stub\">Tom Riggin</th>\n    <td class=\"gt_row gt_center\">19</td>\n    <td class=\"gt_row gt_center\">7.36</td>\n  </tr>\n  <tr>\n    <th class=\"gt_row gt_left gt_stub\">Simon Chong</th>\n    <td class=\"gt_row gt_center\">9</td>\n    <td class=\"gt_row gt_center\">7.31</td>\n  </tr>\n</tbody>\n  <tfoot class=\"gt_sourcenotes\">\n  \n  <tr>\n    <td class=\"gt_sourcenote\" colspan=\"3\">Data: {bobsburgersR} (via TidyTuesday)</td>\n  </tr>\n\n</tfoot>\n\n</table>\n\n</div>\n        \n```\n:::\n:::\n\n\nIMDb User Ratings by Director\n\n::::\n\n### Writer Ratings\n\n:::: {#tbl-writer-ratings}\n\n::: {#cell-writer-ratings .cell execution_count=9}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Table Code (Click to Expand)\"}\n(\n    bobs_burgers.assign(\n        wikipedia_written_by=lambda df: (\n            df[\"wikipedia_written_by\"].str.split(\" & \")\n        )\n    )\n    .explode(\"wikipedia_written_by\")\n    .groupby(\"wikipedia_written_by\")\n    .agg(\n        episodes=(\"wikipedia_written_by\", \"size\"), \n        average_rating=(\"rating\", \"mean\")\n        )\n    .query(\"episodes >= 5\")\n    .sort_values(by=\"average_rating\", ascending=False)\n    .reset_index()\n    .pipe(\n        lambda df: GT(df, rowname_col=\"wikipedia_written_by\")\n        .fmt_number(columns=\"average_rating\", decimals=2)\n        .cols_label(\n            episodes=\"Total Episodes Written\", \n            average_rating=\"Average Episode Rating\"\n        )\n        .cols_align(columns=[\"episodes\", \"average_rating\"], align=\"center\")\n        .tab_source_note(source_note=\"Data: {bobsburgersR} (via TidyTuesday)\")\n        .tab_options(table_width=\"100%\")\n    )\n)\n```\n\n::: {#writer-ratings .cell-output .cell-output-display execution_count=9}\n```{=html}\n<div id=\"nowpyzpdqt\" style=\"padding-left:0px;padding-right:0px;padding-top:10px;padding-bottom:10px;overflow-x:auto;overflow-y:auto;width:auto;height:auto;\">\n<style>\n#nowpyzpdqt table {\n          font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;\n          -webkit-font-smoothing: antialiased;\n          -moz-osx-font-smoothing: grayscale;\n        }\n\n#nowpyzpdqt thead, tbody, tfoot, tr, td, th { border-style: none; }\n tr { background-color: transparent; }\n#nowpyzpdqt p { margin: 0; padding: 0; }\n #nowpyzpdqt .gt_table { display: table; border-collapse: collapse; line-height: normal; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: 100%; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; }\n #nowpyzpdqt .gt_caption { padding-top: 4px; padding-bottom: 4px; }\n #nowpyzpdqt .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; }\n #nowpyzpdqt .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 3px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; }\n #nowpyzpdqt .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; }\n #nowpyzpdqt .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; }\n #nowpyzpdqt .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; }\n #nowpyzpdqt .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; }\n #nowpyzpdqt .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; }\n #nowpyzpdqt .gt_column_spanner_outer:first-child { padding-left: 0; }\n #nowpyzpdqt .gt_column_spanner_outer:last-child { padding-right: 0; }\n #nowpyzpdqt .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; }\n #nowpyzpdqt .gt_spanner_row { border-bottom-style: hidden; }\n #nowpyzpdqt .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; }\n #nowpyzpdqt .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; }\n #nowpyzpdqt .gt_from_md> :first-child { margin-top: 0; }\n #nowpyzpdqt .gt_from_md> :last-child { margin-bottom: 0; }\n #nowpyzpdqt .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; }\n #nowpyzpdqt .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; }\n #nowpyzpdqt .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; }\n #nowpyzpdqt .gt_row_group_first td { border-top-width: 2px; }\n #nowpyzpdqt .gt_row_group_first th { border-top-width: 2px; }\n #nowpyzpdqt .gt_striped { background-color: rgba(128,128,128,0.05); }\n #nowpyzpdqt .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; }\n #nowpyzpdqt .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; }\n #nowpyzpdqt .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; text-align: left; }\n #nowpyzpdqt .gt_left { text-align: left; }\n #nowpyzpdqt .gt_center { text-align: center; }\n #nowpyzpdqt .gt_right { text-align: right; font-variant-numeric: tabular-nums; }\n #nowpyzpdqt .gt_font_normal { font-weight: normal; }\n #nowpyzpdqt .gt_font_bold { font-weight: bold; }\n #nowpyzpdqt .gt_font_italic { font-style: italic; }\n #nowpyzpdqt .gt_super { font-size: 65%; }\n #nowpyzpdqt .gt_footnote_marks { font-size: 75%; vertical-align: 0.4em; position: initial; }\n #nowpyzpdqt .gt_asterisk { font-size: 100%; vertical-align: 0; }\n \n</style>\n<table class=\"gt_table\" data-quarto-disable-processing=\"false\" data-quarto-bootstrap=\"false\">\n<thead>\n\n<tr class=\"gt_col_headings\">\n  <th class=\"gt_col_heading gt_columns_bottom_border gt_left\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"\"></th>\n  <th class=\"gt_col_heading gt_columns_bottom_border gt_center\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"Total Episodes Written\">Total Episodes Written</th>\n  <th class=\"gt_col_heading gt_columns_bottom_border gt_center\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"Average Episode Rating\">Average Episode Rating</th>\n</tr>\n</thead>\n<tbody class=\"gt_table_body\">\n  <tr>\n    <th class=\"gt_row gt_left gt_stub\">Loren Bouchard</th>\n    <td class=\"gt_row gt_center\">7</td>\n    <td class=\"gt_row gt_center\">8.30</td>\n  </tr>\n  <tr>\n    <th class=\"gt_row gt_left gt_stub\">Lizzie Molyneux</th>\n    <td class=\"gt_row gt_center\">29</td>\n    <td class=\"gt_row gt_center\">7.87</td>\n  </tr>\n  <tr>\n    <th class=\"gt_row gt_left gt_stub\">Wendy Molyneux</th>\n    <td class=\"gt_row gt_center\">29</td>\n    <td class=\"gt_row gt_center\">7.87</td>\n  </tr>\n  <tr>\n    <th class=\"gt_row gt_left gt_stub\">Steven Davis</th>\n    <td class=\"gt_row gt_center\">28</td>\n    <td class=\"gt_row gt_center\">7.86</td>\n  </tr>\n  <tr>\n    <th class=\"gt_row gt_left gt_stub\">Kelvin Yu</th>\n    <td class=\"gt_row gt_center\">25</td>\n    <td class=\"gt_row gt_center\">7.86</td>\n  </tr>\n  <tr>\n    <th class=\"gt_row gt_left gt_stub\">Greg Thompson</th>\n    <td class=\"gt_row gt_center\">21</td>\n    <td class=\"gt_row gt_center\">7.80</td>\n  </tr>\n  <tr>\n    <th class=\"gt_row gt_left gt_stub\">Dan Fybel</th>\n    <td class=\"gt_row gt_center\">27</td>\n    <td class=\"gt_row gt_center\">7.73</td>\n  </tr>\n  <tr>\n    <th class=\"gt_row gt_left gt_stub\">Nora Smith</th>\n    <td class=\"gt_row gt_center\">18</td>\n    <td class=\"gt_row gt_center\">7.73</td>\n  </tr>\n  <tr>\n    <th class=\"gt_row gt_left gt_stub\">Jon Schroeder</th>\n    <td class=\"gt_row gt_center\">25</td>\n    <td class=\"gt_row gt_center\">7.66</td>\n  </tr>\n  <tr>\n    <th class=\"gt_row gt_left gt_stub\">Rich Rinaldi</th>\n    <td class=\"gt_row gt_center\">28</td>\n    <td class=\"gt_row gt_center\">7.63</td>\n  </tr>\n  <tr>\n    <th class=\"gt_row gt_left gt_stub\">Scott Jacobson</th>\n    <td class=\"gt_row gt_center\">26</td>\n    <td class=\"gt_row gt_center\">7.60</td>\n  </tr>\n  <tr>\n    <th class=\"gt_row gt_left gt_stub\">Holly Schlesinger</th>\n    <td class=\"gt_row gt_center\">28</td>\n    <td class=\"gt_row gt_center\">7.58</td>\n  </tr>\n  <tr>\n    <th class=\"gt_row gt_left gt_stub\">Katie Crown</th>\n    <td class=\"gt_row gt_center\">11</td>\n    <td class=\"gt_row gt_center\">7.28</td>\n  </tr>\n</tbody>\n  <tfoot class=\"gt_sourcenotes\">\n  \n  <tr>\n    <td class=\"gt_sourcenote\" colspan=\"3\">Data: {bobsburgersR} (via TidyTuesday)</td>\n  </tr>\n\n</tfoot>\n\n</table>\n\n</div>\n        \n```\n:::\n:::\n\n\nIMDb User Ratings by Writer\n\n::::\n\n:::\n\nThere is plenty of variance in the outcomes across directors and writers. It is reasonable to assume that these should impact the ratings and improve model performance. However, including directors and writers in the model will require inducing considerable sparsity in the data (assuming that one-hot encoding is used).\n\n[^Viewers]:\n\n    My initial reaction to viewership's strong correlation with ratings was that this made \n    sense, but as soon as I tried to explain why, I realised, I had the direction of causality \n    mixed up. \n\n[^Cats]:\n\n    The other categorical features are the episode title and synopsis, which might include some \n    helpful signal but would require some effort to extract. The date the episode aired is also \n    categorical, but only because I haven't bothered to convert it (having done some digging and \n    found that there doesn't appear to be any seasonality or anything to work with).\n\n## Data Preparation\n\nA brief exploration of the data has not given me confidence that it is sufficient for developing a cromulent predictive model (maybe the TidyTuesday folks were on to something). But I'm committed to the bit. I've never heard of sunken costs. \n\nI will carry out some basic preprocessing but won't do any feature engineering or add additional data from elsewhere. I will keep all the dialogue metrics in the model because meaningful interactions between them could add some value[^Interactions]. However, I will drop the season-episode number and year features because they offer minimal additional value over the season and overall episode number features. Finally, I'm following my instincts and dropping the viewership feature. I think it is just serving as another proxy for time and isn't needed.\n\nThere is a lot more that could be done here. Never underestimate how many different feature engineering methods exist[^Features], nor the potential performance gains they can achieve. I think a lot could be gained from the transcripts of each episode's dialogue, but this is outside the scope of this post (because it's hard and I'm lazy).\n\n[^Interactions]:\n\n    If I were spending more time on this, I would explore any interactions, but I think we can \n    get away with chucking all the potentially valuable features into the mixer here and saving \n    some time.\n\n[^Features]:\n\n    I'd highly recommend Emil Hvitfeldt's [_Feature Engineering A-Z_](https://feaz-book.com/), a \n    comprehensive resource on the endless possibilities of feature engineering.\n\n### Train/Test Split\n\nI should have split the data into [training and test sets](https://mlu-explain.github.io/train-test-validation/) before doing the exploratory work, but with only 271 observations, I don't think we have enough data to justify splitting it up beforehand. \n\n::: {#train-test-split .cell execution_count=10}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Data Code (Click to Expand)\"}\nX_train, X_test, y_train, y_test = train_test_split(\n    bobs_burgers.drop([\"rating\"], axis=1),\n    bobs_burgers[\"rating\"],\n    test_size=0.3,\n    random_state=42,\n)\n```\n:::\n\n\nThe data has been split 70/30 into training and testing. I also decided there wasn't enough to include a validation set.\n\n### Data Preprocessing\n\nThe model pipeline includes several preprocessing steps. I have split the features into categorical and numeric, and missing values are imputed using slightly different methods depending on the data type. \n\n::: {#explode-encode-class .cell execution_count=11}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Class Code (Click to Expand)\"}\nclass ExplodeAndEncode(TransformerMixin, BaseEstimator):\n    def __init__(self, min_frequency=3):\n        # initialise transformer with min frequency for encoding\n        self.min_frequency = min_frequency\n        self.ohe_directors = OneHotEncoder(\n            handle_unknown=\"ignore\",\n            sparse_output=False,\n            min_frequency=self.min_frequency,\n        )\n        self.ohe_writers = OneHotEncoder(\n            handle_unknown=\"ignore\",\n            sparse_output=False,\n            min_frequency=self.min_frequency,\n        )\n\n    def fit(self, X, y=None):\n        # check input type\n        if not isinstance(X, pd.DataFrame):\n            raise ValueError(\"Input must be a DataFrame.\")\n\n        # rename columns for consistency\n        X = X.rename(\n            columns={\n                \"wikipedia_directed_by\": \"director\",\n                \"wikipedia_written_by\": \"writer\",\n            }\n        )\n\n        # fit one-hot encoders on expanded and encoded data\n        self.ohe_directors.fit(self._expand_and_encode(X[\"director\"]))\n        self.ohe_writers.fit(self._expand_and_encode(X[\"writer\"]))\n\n        return self\n\n    def transform(self, X, y=None):\n        # check input type\n        if not isinstance(X, pd.DataFrame):\n            raise ValueError(\"Input must be a DataFrame.\")\n\n        # rename columns for consistency\n        X = X.rename(\n            columns={\n                \"wikipedia_directed_by\": \"director\",\n                \"wikipedia_written_by\": \"writer\",\n            }\n        )\n\n        # expand and encode director and writer columns\n        directors_expanded = self._expand_and_encode(X[\"director\"])\n        writers_expanded = self._expand_and_encode(X[\"writer\"])\n\n        # apply one-hot encoding\n        transformed_directors = self.ohe_directors.transform(directors_expanded)\n        transformed_writers = self.ohe_writers.transform(writers_expanded)\n\n        # create dataframes with meaningful column names\n        directors_df = (\n            pd.DataFrame(\n                transformed_directors,\n                index=directors_expanded.index,\n                columns=[\n                    f\"director_{col}\"\n                    for col in self.ohe_directors.get_feature_names_out()\n                ],\n            )\n            .groupby(level=0)\n            .sum()  # aggregate back to original index level\n        )\n\n        writers_df = (\n            pd.DataFrame(\n                transformed_writers,\n                index=writers_expanded.index,\n                columns=[\n                    f\"writer_{col}\" for col in self.ohe_writers.get_feature_names_out()\n                ],\n            )\n            .groupby(level=0)\n            .sum()  # aggregate back to original index level\n        )\n\n        # merge transformed features and fill missing values with zero\n        transformed_df = directors_df.join(writers_df, how=\"outer\").fillna(0)\n\n        return transformed_df.values  # return as numpy array\n\n    def _expand_and_encode(self, series):\n        # ensure series is categorical\n        if not isinstance(series.dtype, pd.CategoricalDtype):\n            series = series.astype(\"category\")\n\n        # add 'unknown' category if missing\n        if \"Unknown\" not in series.cat.categories:\n            series = series.cat.add_categories([\"Unknown\"])\n\n        # fill missing values with 'unknown'\n        series = series.fillna(\"Unknown\")\n\n        # split multi-value strings into separate rows\n        exploded = series.str.split(\" & \").explode()\n\n        # standardise formatting (lowercase, replace spaces with underscores)\n        exploded = exploded.str.lower().str.replace(\" \", \"_\")\n\n        return exploded.to_frame()  # return as a dataframe\n```\n:::\n\n\nWith the help of ChatGPT[^Help], I created a custom transformer class that explodes and formats the categorical columns—episode writers and directors—before one-hot encoding each of the individual writers/directors per episode.\n\n::: {#preprocessing .cell execution_count=12}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Data Code (Click to Expand)\"}\ncategorical_features = [\"wikipedia_written_by\", \"wikipedia_directed_by\"]\nnumeric_features = [\n    \"season\",\n    \"episode_overall\",\n    \"dialogue_density\",\n    \"unique_words\",\n    \"question_ratio\",\n    \"exclamation_ratio\",\n    \"avg_length\",\n    \"sentiment_variance\",\n]\n\ncategorical_transformer = Pipeline(\n    steps=[(\"explode_encode\", ExplodeAndEncode(min_frequency=5))]\n)\n\nnumeric_transformer = Pipeline(steps=[(\"imputer\", IterativeImputer())])\n\ncol_transformer = ColumnTransformer(\n    transformers=[\n        (\"num\", numeric_transformer, numeric_features),\n        (\"cat\", categorical_transformer, categorical_features),\n    ]\n)\n\npreprocessor = Pipeline(steps=[(\"col_transformer\", col_transformer)])\n```\n:::\n\n\nTo prevent the newly created binary features from markedly expanding the data's dimensionality and sparsity, I have restricted the one-hot encoding to those credited with a minimum of five episodes.\n\n[^Help]:\n\n    I haven't had much experience building custom transformers like this, so I needed a helping \n    hand.\n\n## Model Training\n\nI'll start with a baseline model and then a relatively simple model to compare against. This is always a good starting point, especially when there's a reasonable chance your model will be garbage. You need to make sure you are at **least** able to build a model that performs better than taking the most basic baseline predictions.\n\nThe baseline model predicts that all test set values will equal the mean IMDb ratings in the training set. \n\n::: {#baseline-model .cell execution_count=13}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Model Code (Click to Expand)\"}\nbaseline_model = DummyRegressor(strategy=\"mean\")\n\nbaseline_model.fit(X_train, y_train)\n\nbaseline_preds = baseline_model.predict(X_test)\n\nbaseline_rmse = root_mean_squared_error(y_test, baseline_preds)\n```\n:::\n\n\nThe RMSE for the baseline model is **0\\.430**. This gives us a target to try and beat. If we can't do better than this, we should pack it in and not spend a moment more on this nonsense. Remember to blame the tools, though. It's certainly not my fault.\n\nI have also built a linear regression to compare against the baseline and help evaluate whether a more complex model is worth the effort. Linear regressions are straightforward. Despite this, they are capable of remarkable performance and typically generalise very well.\n\n::: {#linear-regression .cell execution_count=14}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Model Code (Click to Expand)\"}\npipeline = Pipeline(\n    steps=[(\"preprocessor\", preprocessor), (\"regressor\", LinearRegression())]\n)\n\npipeline.fit(X_train, y_train)\n\nlm_preds = pipeline.predict(X_test)\n\nlm_rmse = root_mean_squared_error(y_test, lm_preds)\n```\n:::\n\n\nThe linear regression returns a score of **0\\.415**, which beats the baseline. We can use both these models as a good starting point, but the linear regression performance is the new target to beat.\n\n### Model Selection\n\nI decided to stick with algorithms provided by scikit-learn, choosing [random forests](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html) [@breiman2001], [extremely randomised trees](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesRegressor.html) (Extra Trees in scikit-learn) [@geurts2006], and [gradient boosting](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html) [@friedman2001].\n\nRandom forests[^RF] and extra trees are bagging ensembles combining multiple decision trees by averaging their predictions. The key difference between the two is that random forests use bootstrapped samples of the data for each tree instead of the entire sample (though you can choose to use this feature in scikit-learn's extra trees implementation, too), and extra trees split tree nodes randomly, to induce more variance between trees. Gradient boosting[^GB] combines an ensemble of weak models (typically decision trees) in a sequential, additive fashion. The sequential process, called boosting, allows each model to learn from all that precede it. \n\nI am testing the performance of these algorithms by tuning a couple key hyperparameters for each, split over five folds using [k-fold cross-validation](https://mlu-explain.github.io/cross-validation/). The goal is that this should identify the algorithm that gives us the best opportunity for maximising performance. I am using [Optuna](https://optuna.org) for [hyperparameter optimisation](https://maelfabien.github.io/machinelearning/Explorium4/) (here and throughout the rest of this post)[^Optuna]. Optuna's default optimisation strategy is Bayesian optimisation using [tree-structured parzen estimators](https://anna-ssi.github.io/blog/tpe/), and I've always had good luck with it.\n\n::: {#model-selection .cell cache='true' execution_count=15}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Model Code (Click to Expand)\"}\nrmse_scorer = make_scorer(root_mean_squared_error, greater_is_better=False)\n\n\ndef objective(trial):\n    regressor_name = trial.suggest_categorical(\n        \"regressor\", [\"RandomForest\", \"ExtraTrees\", \"GradientBoosting\"]\n    )\n\n    if regressor_name == \"RandomForest\":\n        n_estimators = trial.suggest_int(\"n_estimators\", 100, 1000, step=100)\n        max_features = trial.suggest_float(\"max_features\", 0.1, 1.0, step=0.1)\n        model = RandomForestRegressor(\n            n_estimators=n_estimators, max_features=max_features\n        )\n\n    elif regressor_name == \"ExtraTrees\":\n        n_estimators = trial.suggest_int(\"n_estimators\", 100, 1000, step=100)\n        max_features = trial.suggest_float(\"max_features\", 0.1, 1.0, step=0.1)\n        model = ExtraTreesRegressor(\n            n_estimators=n_estimators, max_features=max_features\n        )\n\n    else:\n        n_estimators = trial.suggest_int(\"n_estimators\", 100, 1000, step=100)\n        learning_rate = trial.suggest_float(\"learning_rate\", 0.01, 0.3, step=0.01)\n        model = GradientBoostingRegressor(\n            n_estimators=n_estimators, learning_rate=learning_rate\n        )\n\n    pipeline = Pipeline([(\"preprocesser\", preprocessor), (\"regressor\", model)])\n\n    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n    scores = cross_val_score(\n        pipeline, X_train, np.ravel(y_train), cv=kf, scoring=rmse_scorer\n    )\n\n    return -scores.mean()\n\n\nselect_model = optuna.create_study(direction=\"minimize\")\nselect_model.optimize(objective, n_trials=100)\n```\n:::\n\n\nI'm not especially concerned with model performance during this step; it's just a comparison between algorithms. The study suggests **RandomForest** is the best of the bunch. \n\n[^RF]:\n\n    For an intuition into random forests (and, by extension, extra trees, too), I'd recommend \n    [MLU-Explain](https://mlu-explain.github.io/random-forest/)'s explainer.\n\n[^GB]:\n\n    For an intuition into gradient boosting (and all its variants), I'd recommend Terence Parr \n    & Jeremy Howard's [_How to Explain Gradient Boosting_](https://explained.ai/gradient-boosting/)\n\n[^Optuna]:\n\n    There are other hyperparameter optimisation libraries, such as HyerOpt, and scikit-learn, \n    which offers tuning functionality of its own, but Optuna is the most \n    [feature-rich](https://neptune.ai/blog/optuna-vs-hyperopt). Perhaps the focus should be on \n    [methods, not libraries](https://trainindata.substack.com/p/choose-the-library-based-on-the-method), \n    but in my experience, Optuna's methods also provide excellent performance. \n\n### Hyperparameter Tuning\n\nNow, we can get knee-deep in filth, searching for the best hyperparameter values for optimising our random forest. The setup is more or less the same as in the previous section; here, we are digging deeper into one algorithm and beefing up the total number of trials in the study.\n\nThe study below includes several more hyperparameters. The search space for the hyperparameters is relatively modest. I have tried to keep the model simple to reflect the data, and I have iteratively refined the study to narrow the hyperparameter values and speed up the tuning. Finally, I've also tried to set some constraints to help the model avoid overfitting.\n\n::: {#random-forest .cell cache='true' execution_count=16}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Model Code (Click to Expand)\"}\ndef objective(trial):\n\n    rf_params = {\n        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 250),\n        \"max_features\": trial.suggest_float(\"max_features\", 0.2, 0.5),\n        \"max_samples\": trial.suggest_float(\"max_samples\", 0.7, 1.0),\n        \"max_depth\": trial.suggest_int(\"max_depth\", 10, 15),\n        \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 20, 25),\n        \"criterion\": trial.suggest_categorical(\n            \"criterion\", [\"poisson\", \"friedman_mse\", \"squared_error\"]\n        ),\n    }\n\n    model = RandomForestRegressor(**rf_params, random_state=42)\n\n    pipeline = Pipeline([(\"preprocessor\", preprocessor), (\"regressor\", model)])\n\n    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n    scores = cross_val_score(\n        pipeline, X_train, np.ravel(y_train), cv=kf, scoring=rmse_scorer\n    )\n\n    return -scores.mean()\n\n\ntuned_rf = optuna.create_study(direction=\"minimize\")\ntuned_rf.optimize(objective, n_trials=500)\n```\n:::\n\n\nThe tuned random forest produces an RMSE of **0\\.395**. Comparisons with the baselines don't work because they are fit to the test data, while this is the performance of the best trial on the training data[^Validation]. The hope is that how the tuning is set up will make the model's performance on the training data generalisable. Given the small sample we're working with, there could be plenty of variance, so while our model shows promise, we can't draw conclusions yet.\n\n@tbl-hyperparameters below shows the tuned hyperparameter values.\n\n::: {#tbl-hyperparameters}\n\n::: {#cell-tuned-hyperparameters .cell execution_count=17}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Table Code (Click to Expand)\"}\n(\n    GT(\n        pd.DataFrame(\n            tuned_rf.best_trial.params.items(), columns=[\"Hyperparameter\", \"Value\"]\n        ).assign(\n            Value=lambda df: df[\"Value\"].apply(\n                lambda x: round(x, 2) if isinstance(x, (int, float)) else x\n            )\n        )\n    )\n    .cols_align(columns=\"Value\", align=\"center\")\n    .tab_source_note(source_note=\"Data: {bobsburgersR} (via TidyTuesday)\")\n    .tab_options(table_width=\"100%\")\n)\n```\n\n::: {#tuned-hyperparameters .cell-output .cell-output-display execution_count=17}\n```{=html}\n<div id=\"gfmeokwuno\" style=\"padding-left:0px;padding-right:0px;padding-top:10px;padding-bottom:10px;overflow-x:auto;overflow-y:auto;width:auto;height:auto;\">\n<style>\n#gfmeokwuno table {\n          font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;\n          -webkit-font-smoothing: antialiased;\n          -moz-osx-font-smoothing: grayscale;\n        }\n\n#gfmeokwuno thead, tbody, tfoot, tr, td, th { border-style: none; }\n tr { background-color: transparent; }\n#gfmeokwuno p { margin: 0; padding: 0; }\n #gfmeokwuno .gt_table { display: table; border-collapse: collapse; line-height: normal; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: 100%; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; }\n #gfmeokwuno .gt_caption { padding-top: 4px; padding-bottom: 4px; }\n #gfmeokwuno .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; }\n #gfmeokwuno .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 3px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; }\n #gfmeokwuno .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; }\n #gfmeokwuno .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; }\n #gfmeokwuno .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; }\n #gfmeokwuno .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; }\n #gfmeokwuno .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; }\n #gfmeokwuno .gt_column_spanner_outer:first-child { padding-left: 0; }\n #gfmeokwuno .gt_column_spanner_outer:last-child { padding-right: 0; }\n #gfmeokwuno .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; }\n #gfmeokwuno .gt_spanner_row { border-bottom-style: hidden; }\n #gfmeokwuno .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; }\n #gfmeokwuno .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; }\n #gfmeokwuno .gt_from_md> :first-child { margin-top: 0; }\n #gfmeokwuno .gt_from_md> :last-child { margin-bottom: 0; }\n #gfmeokwuno .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; }\n #gfmeokwuno .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; }\n #gfmeokwuno .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; }\n #gfmeokwuno .gt_row_group_first td { border-top-width: 2px; }\n #gfmeokwuno .gt_row_group_first th { border-top-width: 2px; }\n #gfmeokwuno .gt_striped { background-color: rgba(128,128,128,0.05); }\n #gfmeokwuno .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; }\n #gfmeokwuno .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; }\n #gfmeokwuno .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; text-align: left; }\n #gfmeokwuno .gt_left { text-align: left; }\n #gfmeokwuno .gt_center { text-align: center; }\n #gfmeokwuno .gt_right { text-align: right; font-variant-numeric: tabular-nums; }\n #gfmeokwuno .gt_font_normal { font-weight: normal; }\n #gfmeokwuno .gt_font_bold { font-weight: bold; }\n #gfmeokwuno .gt_font_italic { font-style: italic; }\n #gfmeokwuno .gt_super { font-size: 65%; }\n #gfmeokwuno .gt_footnote_marks { font-size: 75%; vertical-align: 0.4em; position: initial; }\n #gfmeokwuno .gt_asterisk { font-size: 100%; vertical-align: 0; }\n \n</style>\n<table class=\"gt_table\" data-quarto-disable-processing=\"false\" data-quarto-bootstrap=\"false\">\n<thead>\n\n<tr class=\"gt_col_headings\">\n  <th class=\"gt_col_heading gt_columns_bottom_border gt_left\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"Hyperparameter\">Hyperparameter</th>\n  <th class=\"gt_col_heading gt_columns_bottom_border gt_center\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"Value\">Value</th>\n</tr>\n</thead>\n<tbody class=\"gt_table_body\">\n  <tr>\n    <td class=\"gt_row gt_left\">n_estimators</td>\n    <td class=\"gt_row gt_center\">118</td>\n  </tr>\n  <tr>\n    <td class=\"gt_row gt_left\">max_features</td>\n    <td class=\"gt_row gt_center\">0.34</td>\n  </tr>\n  <tr>\n    <td class=\"gt_row gt_left\">max_samples</td>\n    <td class=\"gt_row gt_center\">0.72</td>\n  </tr>\n  <tr>\n    <td class=\"gt_row gt_left\">max_depth</td>\n    <td class=\"gt_row gt_center\">14</td>\n  </tr>\n  <tr>\n    <td class=\"gt_row gt_left\">min_samples_split</td>\n    <td class=\"gt_row gt_center\">25</td>\n  </tr>\n  <tr>\n    <td class=\"gt_row gt_left\">criterion</td>\n    <td class=\"gt_row gt_center\">friedman_mse</td>\n  </tr>\n</tbody>\n  <tfoot class=\"gt_sourcenotes\">\n  \n  <tr>\n    <td class=\"gt_sourcenote\" colspan=\"2\">Data: {bobsburgersR} (via TidyTuesday)</td>\n  </tr>\n\n</tfoot>\n\n</table>\n\n</div>\n        \n```\n:::\n:::\n\n\nOptimised Random Forest Hyperparameters\n\n:::\n\nThe hyperparameters are mostly tailored to optimise the training data, with some thought towards what is appropriate given the sample size. I have constrained the maximum features and samples to make the model more generalisable.\n\n[^Validation]: \n\n    A validation set or a more involved process like nested cross-validation would add some value. \n    I don't think we have enough data for a validation set, and I decided a fully nested pipeline \n    was overkill, but I'm sure it would be beneficial.\n\n## Model Evaluation\n\nHaving tuned the hyperparameters, we should hopefully have a model that gives us the best performance on our test set.\n\n::: {#final-model .cell cache='true' execution_count=18}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Model Code (Click to Expand)\"}\nbest_params = tuned_rf.best_params\n\nbest_rf = RandomForestRegressor(**best_params, random_state=42)\n\npipeline = Pipeline([(\"preprocessor\", preprocessor), (\"regressor\", best_rf)])\n\npipeline.fit(X_train, np.ravel(y_train))\n\nrf_preds = pipeline.predict(X_test)\n\nrf_rmse = root_mean_squared_error(np.ravel(y_test), rf_preds)\n```\n:::\n\n\nThe model improves on the baseline (**0\\.430**) and the linear regression (**0\\.415**) to achieve an RMSE of **0\\.367**. That is a ~15% improvement on the baseline and ~11% on the linear model[^LM]. This is not nothing! \n\n[^LM]: \n\n    Some simple preprocessing steps could improve the linear regression, potentially making it \n    competitive with our final model. If I intended to put this into production, I'd be tempted \n    to run the linear regression alongside the random forest to see if a simpler model would \n    ultimately come out on top.\n\n### Visualising Model Error\n\n::: {#test-errors .cell execution_count=19}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Data Code (Click to Expand)\"}\ntest_errors = (\n    pd.DataFrame(rf_preds.T, columns=[\"prediction\"])\n    .assign(actual=y_test.values)\n    .assign(error=lambda df: df[\"prediction\"] - df[\"actual\"])\n    .assign(abs_error=lambda df: abs(df[\"error\"]))\n    .join(X_test.reset_index(drop=True))\n)\n```\n:::\n\n\nOur final model beats both baselines, but I'm sure there remains room for improvement. If we wanted to iteratively fine-tune the model to squeeze out some extra drops of joy, the best place to start would be to understand what the model is getting wrong. There are a few ways to go about this. I will gather the predicted ratings, actual ratings, and prediction error (raw and absolute values) and visualise the model's performance to see what jumps out.\n\nThe distribution of prediction error, plotted below, serves as a quick check that there is no obvious indication that the model is misbehaving. If there is some wild skew in errors or several substantial outliers, this might be cause for concern, but the errors appear to be distributed approximately normally.\n\n::: {#cell-error-dist .cell execution_count=20}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Plot Code (Click to Expand)\"}\n(\n    ggplot(test_errors, aes(\"error\"))\n    + geom_histogram(binwidth=0.1, colour=\"#343a40\")\n    + geom_hline(yintercept=0, colour=\"#343a40\")\n    + labs(\n        title=\"Distribution of Bob's Burgers Episode Ratings Prediction Error\",\n        subtitle=(\n            textwrap.fill(\n                \"The distribution of prediction errors for predicted IMDb user ratings \"\n                \"for each episode of Bob's Burgers in the test dataset. Errors peak at \"\n                \"just above zero but the negative errors have a longer tail.\",\n                width=90,\n            )\n        ),\n        x=\"Prediction Error\",\n        y=\"\",\n        caption=\"Visualisation: Paul Johnson | Data: {bobsburgersR} (via TidyTuesday)\",\n    )\n)\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/error-dist-output-1.png){#error-dist width=672 height=480 fig-alt='A histogram visualising the distribution of prediction errors for the random forest predicting Bob\\'s Burgers IMDb user ratings. While the majority of errors appear to be positive and the distribution peaks just above zero, the negative errors have a longer tail.'}\n:::\n:::\n\n\nWith less than 100 observations in the test dataset, it requires a bit of squinting to make some assumptions about the distribution we'd see with a larger sample. Still, nothing concerning jumps out at a glance. The distribution peaks slightly above zero, but the negative errors have a longer tail. There appears to be one outlier that the model struggled with, underestimating the rating by more than one (which is quite a lot when considering the ratings are out of 10).\n\nWe can also plot the predicted and actual ratings below to compare the model's misses more directly. \n\n::: {#cell-predicted-vs-actual .cell cache='true' execution_count=21}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Plot Code (Click to Expand)\"}\n(\n    ggplot(test_errors, aes(x=\"prediction\", y=\"actual\"))\n    + geom_point(size=2, stroke=0.5, fill=\"white\", color=\"#343a40\", shape=\"o\")\n    + geom_smooth(method=\"lm\", size=1, se=False, color=\"#026E99\")\n    + labs(\n        title=\"Predicted vs Actual Bob's Burgers Episode Ratings\",\n        subtitle=(\n            textwrap.fill(\n                \"Comparing predicted Bob's Burgers ratings and their actual IMDb user \"\n                \"ratings for each episode in the test dataset. On average, predictions \"\n                \"appear to slightly overrate episodes.\",\n                width=90,\n            )\n        ),\n        x=\"Predicted Rating\",\n        y=\"IMDb Rating\",\n        caption=\"Visualisation: Paul Johnson | Data: {bobsburgersR} (via TidyTuesday)\",\n    )\n)\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/predicted-vs-actual-output-1.png){#predicted-vs-actual width=672 height=480 fig-alt='A regression plot visualising the relationship between predicted and actual ratings for the random forest model predicting Bob\\'s Burgers IMDb user ratings. On average, pedictions seem to overrate episode ratings.'}\n:::\n:::\n\n\nVisualising predictions against actual ratings shows that the model slightly overrates episodes. While there's plenty of noise, nothing jumps out as an obvious problem. Besides a single pesky outlier, nothing in these first two plots points to areas to try and squeeze out additional performance improvements.\n\nFinally, we can visualise the relationship between prediction error and actual ratings. We are interested in the magnitude of the error, not the direction, so I've used absolute prediction error. This highlights the target values the model struggled to predict, and the trend in the error distribution is immediately apparent.\n\n::: {#cell-absolute-error .cell execution_count=22}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Plot Code (Click to Expand)\"}\n(\n    ggplot(test_errors, aes(x=\"actual\", y=\"abs_error\"))\n    + geom_point(size=2, stroke=0.5, fill=\"white\", color=\"#343a40\", shape=\"o\")\n    + geom_smooth(size=1, se=False, color=\"#026E99\")\n    + labs(\n        title=\"Absolute Prediction Error by Bob's Burgers Episode Ratings\",\n        subtitle=(\n            textwrap.fill(\n                \"Comparing absolute prediction error of predicted Bob's Burgers' \"\n                \"episode ratings and actual IMDb user ratings. Errors are larger \"\n                \"at the extreme ends of the ratings.\",\n                width=85,\n            )\n        ),\n        x=\"IMDb Rating\",\n        y=\"Absolute Predcition Error\",\n        caption=\"Visualisation: Paul Johnson | Data: {bobsburgersR} (via TidyTuesday)\",\n    )\n)\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/absolute-error-output-1.png){#absolute-error width=672 height=480 fig-alt='A regression plot visualising the relationship between IMDb ratings and the absolute prediction error for the random forest model predicting Bob\\'s Burgers ratings. While the model performs relatively well, it struggles at the extreme ends of the ratings.'}\n:::\n:::\n\n\nThe model struggles with values at the extreme ends of the target distribution. Most observations in the dataset fall between 7 and 8, and the mean absolute error for episodes in this range in the test set is ~0.2. In fact, the trough in the prediction errors is between 7.5 and 8 and rises quickly once the rating value moves in either direction. \n\nThe observation with a rating of over 9 is driving a big spike. The episode responsible is —_The Haunting_ (S6E3). I suspect an indicator that an episode is a \"special\" of some description (episodes relevant to public holidays, like Christmas and Halloween, extended episodes, or any one-off episodes) would minimise the issues the model seems to be having with very high ratings.\n\nTo me, the issue appears to be the sample size. The model handles the well-populated target value ranges but struggles once it gets into the ranges with tiny sample sizes. We just need more episodes.\n\n## Final Thoughts\n\nWas this a good use of my time? No, but did we learn something? Also no. Perhaps there's a moral to this story? Still, no. There is no big reveal. I'm just this stupid. My sincere apologies.\n\nThis is the product of my playing around with a fun dataset seeing what I can produce. A more concerted effort to predict Bob's Burgers episode ratings would involve more exploratory work, including bringing in additional data, particularly the episode dialogue data. Dimensionality reduction methods, such as [principal component analysis](https://www.youtube.com/watch?v=FgakZw6K1QQ) (PCA), would have added much value. The process of splitting out and one-hot encoding the writers and directors induced a bunch of sparsity that is probably hampering the model. Using PCA would simplify the data, and given the high chance that many features are not moving the needle that much, I suspect it would do so without losing much predictive value.\n\nBeyond the gains that could be achieved with the data, I think it's likely that other gradient boosting algorithms would probably beat out the random forest we've used here. I chose to limit myself to the models that scikit-learn provides for brevity (and for a bit of a change of pace), but XGBoost, LightGBM, and CatBoost are incredibly popular for a reason. They regularly beat their competition when dealing with structured data.\n\nDespite being limited in scope, I hope this post's limitations are also illustrative. If not, then at least I had fun. I guess we can't turn TV shows into spreadsheets just yet.\n\n## Acknowledgments {.appendix}\n\nPreview image by [S. Tsuchiya](https://unsplash.com/@s_tsuchiya) on [Unsplash](https://unsplash.com/photos/red-and-white-round-signage-P48JzJc-GhQ).\n\n## Support {.appendix}\n\nIf you enjoyed this blog post and would like to support my work, you can [buy me a coffee or a beer or give me a tip](https://www.buymeacoffee.com/paulj1989) as a thank you. \n\n",
    "supporting": [
      "index_files/figure-html"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}